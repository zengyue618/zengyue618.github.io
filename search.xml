<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>支持向量机——从方程到代码</title>
      <link href="/2022/06/07/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E2%80%94%E2%80%94%E4%BB%8E%E6%96%B9%E7%A8%8B%E5%88%B0%E4%BB%A3%E7%A0%81/"/>
      <url>/2022/06/07/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E2%80%94%E2%80%94%E4%BB%8E%E6%96%B9%E7%A8%8B%E5%88%B0%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h2 id="写在前面："><a href="#写在前面：" class="headerlink" title="写在前面："></a>写在前面：</h2><p>  最近《机器学习》课程即将迎来期末考，就想借此机会把机器学习相关的知识系统梳理一遍。具体想法是将学过的算法自己推导一遍，然后将数学结论翻译为代码。</p><p>  在此过程中，借鉴<strong>scikit-learn</strong>的api设计，仿照其形式实现<strong>mykit-learn</strong>。全程只用<strong>numpy</strong>以及python自带的数据结构完成所有内容（在支持向量机部分还会用到<strong>cvxopt</strong>，用以求解二次优化问题）。</p><p>  mykit-learn现在的代码结构分为：decomposition降维、cluster聚类、classification分类、Regression回归以及public公共。</p><p><img src="https://s1.328888.xyz/2022/06/07/zTITT.png" alt="zTITT.png"></p><p>  目前，降维部分有MDS、PCA以及KernelPCA；聚类部分有KMeans以及KMeans++；分类有Softmax回归、逻辑回归、决策树、K近邻以及SVC（支持向量机分类）；回归有线性回归。public部分则是存放一些公用模块的地方，例如核函数。（目前在PCA和SVC中运用，共有五种）</p><p>  接下来将本在博客不定期更新<strong>mykit-learn</strong>的内容，今天就从我认为最“美”的算法——支持向量机开始！</p><h2 id="一：算法思想"><a href="#一：算法思想" class="headerlink" title="一：算法思想"></a>一：算法思想</h2><p><img src="https://s1.328888.xyz/2022/06/07/zTc07.jpg" alt="zTc07.jpg"></p><p>说白了就是学习一个m-1维度的超平面，将两类样本分开</p><p>以二维空间的例子来理解一下</p><p>负类的样本就在红色区域内，正类的样本就在绿色区域内，以决策超平面为分界</p><p><img src="https://s1.328888.xyz/2022/06/07/zTmVX.png" alt="zTmVX.png"></p><h2 id="二：数学推导"><a href="#二：数学推导" class="headerlink" title="二：数学推导"></a>二：数学推导</h2><p>为了更好的分类，容错率更大，我们就希望间隔尽量大</p><p><img src="https://s1.328888.xyz/2022/06/07/zT3GZ.png" alt="zT3GZ.png"></p><p>与此同时，我们关注一下约束条件</p><p><img src="https://s1.328888.xyz/2022/06/07/zTKtC.png" alt="zTKtC.png"></p><p>加上约束条件后，再看看最终后的优化目标</p><p><img src="https://s1.328888.xyz/2022/06/07/zTax1.png" alt="zTax1.png"></p><p>运用拉格朗日方程优化</p><p><img src="https://s1.328888.xyz/2022/06/07/zTOag.png" alt="zTOag.png"></p><p><img src="https://s1.328888.xyz/2022/06/07/zTn4t.png" alt="zTn4t.png"></p><p>最终可以推出如下结论</p><p><img src="https://s1.328888.xyz/2022/06/07/zT1ce.png" alt="zT1ce.png"></p><p>也就是传说中的KKT条件</p><h2 id="三：对偶问题"><a href="#三：对偶问题" class="headerlink" title="三：对偶问题"></a>三：对偶问题</h2><p>为了进一步简化问题，引入原优化问题的对偶问题</p><p><img src="https://s1.328888.xyz/2022/06/07/zTbNO.png" alt="zTbNO.png"></p><p><img src="https://s1.328888.xyz/2022/06/07/zTViq.png" alt="zTViq.png"></p><p>由于原问题和对偶问题同解，所以我们可以通过求解对偶问题来获得原问题的解</p><p><img src="https://s1.328888.xyz/2022/06/07/zTViq.png" alt="zTViq.png"></p><p>将之前求得的KKT条件代入对偶问题的方程中，化简得到如下式子</p><p><img src="https://s1.328888.xyz/2022/06/07/zTjVm.png" alt="zTjVm.png"></p><p>结合KKT条件，问题最终化归为求解带等式及不等式约束的二次凸优化问题</p><p><img src="https://s1.328888.xyz/2022/06/07/zT6gA.png" alt="zT6gA.png"></p><p>例如：</p><p><img src="https://s1.328888.xyz/2022/06/08/zc09m.png" alt="zc09m.png"></p><p><img src="https://s1.328888.xyz/2022/06/08/zcsWA.jpg" alt="zcsWA.jpg"></p><p>这类问题，我稍后将用cvxopt来求解</p><h2 id="四：核技巧"><a href="#四：核技巧" class="headerlink" title="四：核技巧"></a>四：核技巧</h2><p>如果样本点在原来的空间中，不能用决策超平面分开</p><p><img src="https://s1.328888.xyz/2022/06/07/zTMtS.png" alt="zTMtS.png"></p><p><img src="https://s1.328888.xyz/2022/06/08/zcRUR.png" alt="zcRUR.png"></p><p>那么我们就将原来的样本点升到更高的维度中，在高维空间中决策超平面就可以将它们分开</p><p><img src="https://s1.328888.xyz/2022/06/07/zTPSJ.png" alt="zTPSJ.png"></p><p>但是吧，这个升维的函数很难找啊，就算找到了，在计算的过程也要付出存储高维数据的代价，所以就用核函数代替。</p><p><img src="https://s1.328888.xyz/2022/06/07/zcQiF.png" alt="zcQiF.png"></p><p>核函数其实就是高维空间的点积</p><p><img src="https://s1.328888.xyz/2022/06/07/zTNaR.png" alt="zTNaR.png"></p><p>例如：</p><p><img src="https://s1.328888.xyz/2022/06/07/zToQi.png" alt="zToQi.png"></p><p>寄希望于维度转换函数是不可靠的，升六维时，转换函数还有表达式，但无穷维呢？如何表达呢？</p><p>例如：</p><p><img src="https://s1.328888.xyz/2022/06/07/zTB4v.png" alt="zTB4v.png"></p><p>但是，高斯核函数却可以表示无穷维（本质上是指数函数可以用泰勒公式展开成无穷项）</p><p><img src="https://s1.328888.xyz/2022/06/07/zTEc0.png" alt="zTEc0.png"></p><p>说了那么多，让我们看看具体的程序代码吧</p><h2 id="五：编程实践"><a href="#五：编程实践" class="headerlink" title="五：编程实践"></a>五：编程实践</h2><p> 依赖引入</p><pre><code class="python">import numpy as npfrom cvxopt import matrix, solversfrom numpy import ndarrayfrom public.Kernel import Kernel# cvxopt设置解的精度，求解过程显示与否solvers.options[&#39;abstol&#39;] = 1e-12solvers.options[&#39;reltol&#39;] = 1e-10solvers.options[&#39;feastol&#39;] = 1e-10solvers.options[&#39;show_progress&#39;] = False</code></pre><p>核函数模块</p><pre><code class="python">import numpy as npfrom numpy import ndarrayclass Kernel:    def __init__(            self,            sigma: float = 1.0,            alpha: float = 1.0,            c: float = 1.0,            d: float = 1.0,            beta: float = 1.0,            theta: float = 1.0,    ):        self.theta = theta        self.beta = beta        self.d = d        self.c = c        self.alpha = alpha        self.sigma = sigma    def gauss(self, x: ndarray, y: ndarray) -&gt; float:        return np.exp(-np.sum((x - y) ** 2) / 2 * self.sigma ** 2)    def linear(self, x: ndarray, y: ndarray) -&gt; float:        return self.alpha * np.sum(x * y) + self.c    def poly(self, x: ndarray, y: ndarray) -&gt; float:        return self.linear(x, y) ** self.d    def sigmoid(self, x: ndarray, y: ndarray) -&gt; float:        return np.tanh(self.beta * np.sum(x * y) + self.theta)    def laplace(self, x: ndarray, y: ndarray) -&gt; float:        return np.exp(-np.sum(np.abs(x - y)) / self.sigma)</code></pre><p>SVC类的初始化函数</p><p>x: 训练数据 y: 标签 kernel: 核函数 w: 决策超平面的斜率数组 b: 决策超平面的截距 sigma: 高斯核与拉普拉斯核的参数 alpha: 线性核 多项式核的参数 c: 线性核 多项式核的参数 d: 多项式核的参数 beta: sigmoid的参数 theta: sigmoid参数</p><pre><code class="python">class SVC:    def __init__(            self,            kernel: str = None,            sigma: float = 1.0,            alpha: float = 1.0,            c: float = 1.0,            d: float = 1.0,            beta: float = 1.0,            theta: float = 1.0,    ):        self.w = None        self.b = None        self.kernel = kernel        self.theta = theta        self.beta = beta        self.d = d        self.c = c        self.alpha = alpha        self.sigma = sigma        self.support_vector = None</code></pre><p>计算参数的函数</p><p>这些参数包括了求解二次优化相关的参数</p><p>其中的P,q,G,h,A,b的含义可以参考下图</p><p><img src="https://s1.328888.xyz/2022/06/08/zcaJy.png" alt="zcaJy.png"></p><pre><code class="python">    def __calculate_parameter(self, x: ndarray, y: ndarray):        kernel = Kernel(sigma=self.sigma, alpha=self.alpha, c=self.c, d=self.d, beta=self.beta, theta=self.theta)        kernel_func = &#123;            &quot;linear&quot;: kernel.linear,            &quot;poly&quot;: kernel.poly,            &quot;sigmoid&quot;: kernel.sigmoid,            &quot;laplace&quot;: kernel.laplace,            &quot;gauss&quot;: kernel.gauss,            None: np.dot        &#125;        m, _ = np.shape(x)        P = np.zeros((m, m))        for i in range(m):            for j in range(m):                P[i, j] = kernel_func.get(self.kernel, None)(x[i], x[j]) * y[i] * y[j]        P = matrix(P)        q = matrix(-np.ones(m))        G = matrix(np.diag(-np.ones(m)))        h = matrix(np.zeros(m))        A = matrix(y, (1, m), tc=&#39;d&#39;)        b = matrix([0.0])        res = solvers.qp(P, q, G, h, A, b)        λ = np.around(np.array(res[&#39;x&#39;]).flatten(), 2)        self.w = np.array(reduce(lambda a, b: a + b, map(lambda item: item[0] * item[1] * item[2], zip(λ, y, x))))        self.__support_index = [index for index, data in enumerate(λ) if data != 0]</code></pre><p>训练函数</p><p>输入x（m*n）数据，y（1 * m）标签 （+1&#x2F;-1）</p><p>训练得到决策超平面的斜率和截距</p><pre><code class="python">    def fit(self, x: ndarray, y: ndarray):        self.__calculate_parameter(x, y)        self.support_vector = x[self.__support_index]        self.b = 1 / y[self.__support_index[0]] - np.dot(x[self.__support_index[0]], self.w)</code></pre><p>预测函数</p><p>输入：待预测的数据</p><p>输出：标签  +1或-1</p><pre><code class="python">    def predict(self, data: ndarray):        return 1 if np.dot(data, self.w) + self.b &gt;= 0 else -1</code></pre><h2 id="六：计算检验"><a href="#六：计算检验" class="headerlink" title="六：计算检验"></a>六：计算检验</h2><p>以老师PPT中的题目为例</p><p><img src="https://s1.328888.xyz/2022/06/08/zcbeQ.jpg" alt="zcbeQ.jpg"></p><p><img src="https://s1.328888.xyz/2022/06/08/zcVw4.jpg" alt="zcVw4.jpg"></p><pre><code class="python">if __name__ == &quot;__main__&quot;:    svc = SVC()    svc.fit(np.array([[3, 3], [4, 3], [1, 1]]), np.array([1, 1, -1]))    print(&quot;决策超平面的斜率是：&quot;, svc.w)    print(&quot;决策超平面的截距为：&quot;, svc.b)    print(&quot;支持向量是：\n&quot;, svc.support_vector)    print(&quot;预测 (0.5, 0.5)的类别：&quot;, svc.predict(np.array([0.5, 0.5])))</code></pre><p>结果如下，正确且非常直观</p><p><img src="https://s1.328888.xyz/2022/06/08/zcFq3.png" alt="zcFq3.png"></p><h2 id="七：完整代码"><a href="#七：完整代码" class="headerlink" title="七：完整代码"></a>七：完整代码</h2><p>放在我的GitHub了哦~</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 支持向量机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keras构建神经网络预测房价</title>
      <link href="/2022/03/16/%E3%80%90Python%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/"/>
      <url>/2022/03/16/%E3%80%90Python%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%91Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/</url>
      
        <content type="html"><![CDATA[<h3 id="一：综述"><a href="#一：综述" class="headerlink" title="一：综述"></a>一：综述</h3><h5 id="今天为大家介绍一个用Keras神经网络构建回归模型以预测房价的案例。"><a href="#今天为大家介绍一个用Keras神经网络构建回归模型以预测房价的案例。" class="headerlink" title="今天为大家介绍一个用Keras神经网络构建回归模型以预测房价的案例。"></a>今天为大家介绍一个用Keras神经网络构建回归模型以预测房价的案例。</h5><p><img src="http://files.mdnice.com/user/21987/0ecf1a9d-8e82-4d8b-b47e-469ba45cf750.jpg" alt="img"></p><h5 id="这个案例中依赖的库有"><a href="#这个案例中依赖的库有" class="headerlink" title="这个案例中依赖的库有"></a>这个案例中依赖的库有</h5><p><img src="https://files.mdnice.com/user/21987/69f2900b-ab0e-4699-8b04-173e9607835a.jpg" alt="img"></p><h5 id="数据集来自-https-github-com-emanhamed-Houses-dataset。该数据集包括csv文件以及图像。本文主要利用该数据集的csv文件部分来训练模型。"><a href="#数据集来自-https-github-com-emanhamed-Houses-dataset。该数据集包括csv文件以及图像。本文主要利用该数据集的csv文件部分来训练模型。" class="headerlink" title="数据集来自 https://github.com/emanhamed/Houses-dataset。该数据集包括csv文件以及图像。本文主要利用该数据集的csv文件部分来训练模型。"></a>数据集来自 <a href="https://github.com/emanhamed/Houses-dataset%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E6%8B%ACcsv%E6%96%87%E4%BB%B6%E4%BB%A5%E5%8F%8A%E5%9B%BE%E5%83%8F%E3%80%82%E6%9C%AC%E6%96%87%E4%B8%BB%E8%A6%81%E5%88%A9%E7%94%A8%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84csv%E6%96%87%E4%BB%B6%E9%83%A8%E5%88%86%E6%9D%A5%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E3%80%82">https://github.com/emanhamed/Houses-dataset。该数据集包括csv文件以及图像。本文主要利用该数据集的csv文件部分来训练模型。</a></h5><h5 id="我们先来看看数据集的样子"><a href="#我们先来看看数据集的样子" class="headerlink" title="我们先来看看数据集的样子"></a>我们先来看看数据集的样子</h5><pre><code class="python">cols = [&quot;bedrooms&quot;, &quot;bathrooms&quot;, &quot;area&quot;, &quot;zipcode&quot;, &quot;price&quot;]import pandas as pddf = pd.read_csv(&quot;E:\code\RegressionWithKeras\RegressionWithKeras\HousesDataset\HousesDataset\HousesInfo.txt&quot;, sep=&quot; &quot;, header=None, names=cols)</code></pre><p><img src="https://files.mdnice.com/user/21987/61dbfe53-a399-40e2-8705-4a2812d69f51.jpg" alt="img"></p><h5 id="包含五个列：房间数、浴室数、面积、邮政编码以及价格，其中价格是我们的目标列，其它则作为特征列。"><a href="#包含五个列：房间数、浴室数、面积、邮政编码以及价格，其中价格是我们的目标列，其它则作为特征列。" class="headerlink" title="包含五个列：房间数、浴室数、面积、邮政编码以及价格，其中价格是我们的目标列，其它则作为特征列。"></a>包含五个列：房间数、浴室数、面积、邮政编码以及价格，其中价格是我们的目标列，其它则作为特征列。</h5><h3 id="二：数据清洗"><a href="#二：数据清洗" class="headerlink" title="二：数据清洗"></a>二：数据清洗</h3><h5 id="在这个数据集中，有一些邮政编码对应的数据很少，是数据集的”噪声“，应该被去除。所以，编写一个函数，用以清洗数据集，代码如下："><a href="#在这个数据集中，有一些邮政编码对应的数据很少，是数据集的”噪声“，应该被去除。所以，编写一个函数，用以清洗数据集，代码如下：" class="headerlink" title="在这个数据集中，有一些邮政编码对应的数据很少，是数据集的”噪声“，应该被去除。所以，编写一个函数，用以清洗数据集，代码如下："></a>在这个数据集中，有一些邮政编码对应的数据很少，是数据集的”噪声“，应该被去除。所以，编写一个函数，用以清洗数据集，代码如下：</h5><pre><code class="python">def load_house_attributes(inputPath):    # 加载数据集    cols = [&quot;bedrooms&quot;, &quot;bathrooms&quot;, &quot;area&quot;, &quot;zipcode&quot;, &quot;price&quot;]    df = pd.read_csv(inputPath, sep=&quot; &quot;, header=None, names=cols)    # zipcodes是每个邮政编码的值    # counts则是每个邮政编码出现的次数    zipcodes = df[&quot;zipcode&quot;].value_counts().keys().tolist()    counts = df[&quot;zipcode&quot;].value_counts().tolist()    # 遍历它们的组合，找出小于阈值25的编码，把它们从df中删掉    for (zipcode, count) in zip(zipcodes, counts):        if count &lt; 25:            idxs = df[df[&quot;zipcode&quot;] == zipcode].index            df.drop(idxs, inplace=True)    return df</code></pre><h5 id="这个函数接受数据集路径，统计数据集中邮政编码的出现次数，剔除出现次数小于25的数据，返回清洗后的dataframe，调用此函数"><a href="#这个函数接受数据集路径，统计数据集中邮政编码的出现次数，剔除出现次数小于25的数据，返回清洗后的dataframe，调用此函数" class="headerlink" title="这个函数接受数据集路径，统计数据集中邮政编码的出现次数，剔除出现次数小于25的数据，返回清洗后的dataframe，调用此函数"></a>这个函数接受数据集路径，统计数据集中邮政编码的出现次数，剔除出现次数小于25的数据，返回清洗后的dataframe，调用此函数</h5><pre><code class="python"># 设置数据集的路径，加载数据集print(&quot;[INFO] loading house attributes...&quot;)inputPath = &quot;./HousesDataset/HousesDataset/HousesInfo.txt&quot;df = datasets.load_house_attributes(inputPath)</code></pre><h5 id="清洗后的数据如下所示："><a href="#清洗后的数据如下所示：" class="headerlink" title="清洗后的数据如下所示："></a>清洗后的数据如下所示：</h5><p><img src="https://files.mdnice.com/user/21987/af638f2f-33fe-42fd-8156-655bd00d75a0.jpg" alt="img"></p><h5 id="可以看出，清洗后的数据index从30开始，剔除了一些数据。"><a href="#可以看出，清洗后的数据index从30开始，剔除了一些数据。" class="headerlink" title="可以看出，清洗后的数据index从30开始，剔除了一些数据。"></a>可以看出，清洗后的数据index从30开始，剔除了一些数据。</h5><h5 id="清洗完成后，我们来划分训练集和测试集，借助sklearn，很容易完成。"><a href="#清洗完成后，我们来划分训练集和测试集，借助sklearn，很容易完成。" class="headerlink" title="清洗完成后，我们来划分训练集和测试集，借助sklearn，很容易完成。"></a>清洗完成后，我们来划分训练集和测试集，借助sklearn，很容易完成。</h5><pre><code class="python">print(&quot;[INFO] constructing training/testing split...&quot;)(train, test) = train_test_split(df, test_size=0.25, random_state=42)</code></pre><h3 id="三：特征工程"><a href="#三：特征工程" class="headerlink" title="三：特征工程"></a>三：特征工程</h3><h5 id="现在让我们把一些连续的数值特征作归一化处理，包括房间数、浴室数和面积，归一化处理能让我们训练的收敛速度更快。"><a href="#现在让我们把一些连续的数值特征作归一化处理，包括房间数、浴室数和面积，归一化处理能让我们训练的收敛速度更快。" class="headerlink" title="现在让我们把一些连续的数值特征作归一化处理，包括房间数、浴室数和面积，归一化处理能让我们训练的收敛速度更快。"></a>现在让我们把一些连续的数值特征作归一化处理，包括房间数、浴室数和面积，归一化处理能让我们训练的收敛速度更快。</h5><h5 id="连续特征处理完了，分类属性也要处理，一般我们就用one—hot编码对离散特征进行处理"><a href="#连续特征处理完了，分类属性也要处理，一般我们就用one—hot编码对离散特征进行处理" class="headerlink" title="连续特征处理完了，分类属性也要处理，一般我们就用one—hot编码对离散特征进行处理"></a>连续特征处理完了，分类属性也要处理，一般我们就用one—hot编码对离散特征进行处理</h5><h5 id="同样构造一个函数来完成。代码如下："><a href="#同样构造一个函数来完成。代码如下：" class="headerlink" title="同样构造一个函数来完成。代码如下："></a>同样构造一个函数来完成。代码如下：</h5><pre><code class="python">def process_house_attributes(df, train, test):    # 要处理的连续特征    continuous = [&quot;bedrooms&quot;, &quot;bathrooms&quot;, &quot;area&quot;]    # 通过MinMax转换器，把它们都变成0~1之间的数    cs = MinMaxScaler()    trainContinuous = cs.fit_transform(train[continuous])    testContinuous = cs.transform(test[continuous])    # 通过LabelBinarizer转换器，将邮政编码进行one-hot编码    zipBinarizer = LabelBinarizer().fit(df[&quot;zipcode&quot;])    trainCategorical = zipBinarizer.transform(train[&quot;zipcode&quot;])    testCategorical = zipBinarizer.transform(test[&quot;zipcode&quot;])    # 将处理后的特征拼接起来，构成训练集    trainX = np.hstack([trainCategorical, trainContinuous])    testX = np.hstack([testCategorical, testContinuous])    # return the concatenated training and testing data    return (trainX, testX)</code></pre><h5 id="这个函数接收清洗后的dataframe以及划分好的训练集，测试集，通过对连续属性进行归一化、离散属性one-hot化，对特征进行进一步处理，最后返回训练特征数据集和测试特征数据集（相比原来剔除了目标列）。"><a href="#这个函数接收清洗后的dataframe以及划分好的训练集，测试集，通过对连续属性进行归一化、离散属性one-hot化，对特征进行进一步处理，最后返回训练特征数据集和测试特征数据集（相比原来剔除了目标列）。" class="headerlink" title="这个函数接收清洗后的dataframe以及划分好的训练集，测试集，通过对连续属性进行归一化、离散属性one-hot化，对特征进行进一步处理，最后返回训练特征数据集和测试特征数据集（相比原来剔除了目标列）。"></a>这个函数接收清洗后的dataframe以及划分好的训练集，测试集，通过对连续属性进行归一化、离散属性one-hot化，对特征进行进一步处理，最后返回训练特征数据集和测试特征数据集（相比原来剔除了目标列）。</h5><pre><code class="python"># 对特征列进行归一化和one—hot编码print(&quot;[INFO] processing data...&quot;)(trainX, testX) = datasets.process_house_attributes(df, train, test)</code></pre><h5 id="因为目标列也是连续属性，所以也进行归一化处理"><a href="#因为目标列也是连续属性，所以也进行归一化处理" class="headerlink" title="因为目标列也是连续属性，所以也进行归一化处理"></a>因为目标列也是连续属性，所以也进行归一化处理</h5><pre><code class="python"># 对目标列也进行归一化maxPriceTrain = train[&quot;price&quot;].max()maxPriceTest = test[&quot;price&quot;].max()trainY = train[&quot;price&quot;] / maxPriceTraintestY = test[&quot;price&quot;] / maxPriceTrain</code></pre><h3 id="四：模型建立"><a href="#四：模型建立" class="headerlink" title="四：模型建立"></a>四：模型建立</h3><h5 id="使用Keras构建多层感知机模型，也叫人工神经网络ANN。MLP适用于分类预测问题，输入一组数据，输出一个分类。它们也适用于回归预测问题，输入一组数据，输出一个连续值。"><a href="#使用Keras构建多层感知机模型，也叫人工神经网络ANN。MLP适用于分类预测问题，输入一组数据，输出一个分类。它们也适用于回归预测问题，输入一组数据，输出一个连续值。" class="headerlink" title="使用Keras构建多层感知机模型，也叫人工神经网络ANN。MLP适用于分类预测问题，输入一组数据，输出一个分类。它们也适用于回归预测问题，输入一组数据，输出一个连续值。"></a>使用Keras构建多层感知机模型，也叫人工神经网络ANN。MLP适用于分类预测问题，输入一组数据，输出一个分类。它们也适用于回归预测问题，输入一组数据，输出一个连续值。</h5><h5 id="如图所示："><a href="#如图所示：" class="headerlink" title="如图所示："></a>如图所示：</h5><p><img src="https://files.mdnice.com/user/21987/e7127cb1-275c-47d2-b991-41c065f5dd30.png" alt="img"></p><h5 id="代码如下："><a href="#代码如下：" class="headerlink" title="代码如下："></a>代码如下：</h5><pre><code class="python">def create_mlp(dim, regress=False): # 定义MLP神经网络 model = Sequential() # 在网络上加两个层，一层节点为8，另一层节点为4，两层的激活函数都是relu model.add(Dense(8, input_dim=dim, activation=&quot;relu&quot;)) model.add(Dense(4, activation=&quot;relu&quot;)) # 如果用于回归任务，再加一层（输出层），激活函数为linear if regress:  model.add(Dense(1, activation=&quot;linear&quot;)) # 返回model return model# 构建MLP模型model = models.create_mlp(trainX.shape[1], regress=True)# 其中opt包含了lr学习率，以及decay衰减率opt = Adam(lr=1e-3, decay=1e-3 / 200)# 设置损失函数为MAPE，并且传入opt，完成模型的构建model.compile(loss=&quot;mean_absolute_percentage_error&quot;, optimizer=opt)</code></pre><h3 id="五：训练模型并预测"><a href="#五：训练模型并预测" class="headerlink" title="五：训练模型并预测"></a>五：训练模型并预测</h3><pre><code class="python"># 训练模型，训练2000轮，每一轮的用8个样本更新梯度（小批量梯度下降算法）print(&quot;[INFO] training model...&quot;)model.fit(x=trainX, y=trainY, validation_data=(testX, testY), epochs=2000, batch_size=8)# 在测试集上作预测print(&quot;[INFO] predicting house prices...&quot;)preds = model.predict(testX)# 计算误差、误差百分比、误差百分比的绝对值diff = preds.flatten() - testYpercentDiff = (diff / testY) * 100absPercentDiff = np.abs(percentDiff)# 对误差百分比的绝对值求均值和标准差mean = np.mean(absPercentDiff)std = np.std(absPercentDiff)# 打印模型训练的相关指标locale.setlocale(locale.LC_ALL, &quot;en_US.UTF-8&quot;)print(&quot;[INFO] avg. house price: &#123;&#125;, std house price: &#123;&#125;&quot;.format( locale.currency(df[&quot;price&quot;].mean(), grouping=True), locale.currency(df[&quot;price&quot;].std(), grouping=True)))print(&quot;[INFO] mean: &#123;:.2f&#125;%, std: &#123;:.2f&#125;%&quot;.format(mean, std))</code></pre><h5 id="运行结果如下："><a href="#运行结果如下：" class="headerlink" title="运行结果如下："></a>运行结果如下：</h5><p><img src="https://files.mdnice.com/user/21987/66e670cf-51e8-497d-b216-5690f8edb10e.jpg" alt="img"></p><h3 id="六：进一步学习"><a href="#六：进一步学习" class="headerlink" title="六：进一步学习"></a>六：进一步学习</h3><h5 id="对MLP不熟悉的朋友们可以看B站UP主-3blue1Brown-出品的深度神经网络视频。"><a href="#对MLP不熟悉的朋友们可以看B站UP主-3blue1Brown-出品的深度神经网络视频。" class="headerlink" title="对MLP不熟悉的朋友们可以看B站UP主 3blue1Brown 出品的深度神经网络视频。"></a>对MLP不熟悉的朋友们可以看B站UP主 3blue1Brown 出品的深度神经网络视频。</h5><h5 id="关于Keras的更多知识以及api使用可以访问它的中文文档进行进一步的学习：https-keras-io-zh"><a href="#关于Keras的更多知识以及api使用可以访问它的中文文档进行进一步的学习：https-keras-io-zh" class="headerlink" title="关于Keras的更多知识以及api使用可以访问它的中文文档进行进一步的学习：https://keras.io/zh/"></a>关于Keras的更多知识以及api使用可以访问它的中文文档进行进一步的学习：<a href="https://keras.io/zh/">https://keras.io/zh/</a></h5>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
