<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>基于spark structured streaming的端到端实时流计算系统</title>
      <link href="/2022/06/29/%E5%9F%BA%E4%BA%8Espark%20structured%20streaming%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AE%9E%E6%97%B6%E6%B5%81%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/"/>
      <url>/2022/06/29/%E5%9F%BA%E4%BA%8Espark%20structured%20streaming%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AE%9E%E6%97%B6%E6%B5%81%E8%AE%A1%E7%AE%97%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>大三下学期，我开始学习大数据学科，时光荏苒，一学期马上就结束了，也迎来了Spark期末作业。于是就想用自己所学，搭建一套端到端的实时流处理系统。这个项目经过我精心设计，覆盖了我在这个学期中学习到的许多大数据技术栈，包括hadoop、spark、flume、kafka、clickhouse、tableau等，作为本学期的收官之作。</p><p>在此感谢郑志硕老师一学期以来的的辛勤工作，祝您<strong>工作顺利，身体健康，万事如意！</strong></p><h1 id="项目演示"><a href="#项目演示" class="headerlink" title="项目演示"></a>项目演示</h1><p>做了一个项目演示视频放b站，视频外链b站做了限制，很模糊，可以去b站看</p><iframe         src="//player.bilibili.com/player.html?aid=855457073&bvid=BV1bL4y1w7bw&cid=759213961&page=1&high_quality=1&high_quality=1&danmaku=0" allowfullscreen="allowfullscreen" width="100%" height="500" scrolling="no" frameborder="0" sandbox="allow-top-navigation allow-same-origin allow-forms allow-scripts"></iframe><h1 id="项目架构"><a href="#项目架构" class="headerlink" title="项目架构"></a>项目架构</h1><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>目前的实时流计算系统一般架构是这样的</p><p><img src="https://s2.328888.xyz/2022/06/29/62bbe71b4e56d.jpg" alt="structured-streaming-kafka.jpg"></p><p>流数据先进入kafka，再由流计算引擎spark structured streaming 或者 flink进行后续计算，将计算结果再次写回kafka中，重复这个过程，最后将计算结果写入后续的数据介质中。</p><p>需要注意的是spark streaming已经淘汰</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbea8d3268f.png"><img src="https://s2.328888.xyz/2022/06/29/62bbea8d3268f.png" alt="ss原理.png"></a></p><p>业界一般都用更为强大的spark structured streaming 或者 flink作为计算引擎。在本项目中的选择的是spark structured streaming。</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbea8d13881.png"><img src="https://s2.328888.xyz/2022/06/29/62bbea8d13881.png" alt="sss原理.png"></a></p><h2 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h2><p>为使得项目贴近企业开发中的场景，我设定了一个业务场景：网上书城6.29购物节日，需要构建一个系统实时分析订单数据。</p><p>此网上书店这个Web应用会源源不断地产生实时的交易订单数据，长这个样子。</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbec9a1151f.png"><img src="https://s2.328888.xyz/2022/06/29/62bbec9a1151f.png" alt="数据的样子.png"></a></p><p>time字段是指订单的产生时间，id字段是下单的用户名，province是用户的下单地点，money是指订单金额，platform是指用户下单时使用的平台，category是指商品品类，ok字段是指订单付款成功与否，其中0，代表成功，1代表不成功。</p><p>我们需要对此数据源实时计算，最终构建一个可视化大屏用以决策分析。</p><h2 id="选型"><a href="#选型" class="headerlink" title="选型"></a>选型</h2><p>为了贴切企业实际生产过程，我调研了滴滴、去哪儿网、京东等互联网公司构建实时系统解决方案，结合我这个项目的业务背景，最终确定如下结构。</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbee6f47c14.png"><img src="https://s2.328888.xyz/2022/06/29/62bbee6f47c14.png" alt="项目架构.png"></a></p><p>首先是数据生成：网上书城这个web应用源源不断地产生数据，将其写入特定的日志文件中。</p><p>其次是数据采集：用flume这个日志采集模块，监控该日志文件，源源不断采集新数据到kafka中。</p><p>然后是数据存储与计算：其实用数据仓库的理论角度去看的话，在实时流计算系统中，kafka承担了ODS层、DWD层、DWS层的存储任务，具体实现就是将这些层的数据存进不同的topic中，以供其它业务复用。这些层之间的数据计算任务就交由spark structured streaming去做。</p><p>之后是数据分析：经过上面的步骤处理过后，数据已经可以直接分析使用了，这里我采用的是clickhouse这个OLAP数据库</p><p>最后是可视化：使用成熟的BI系统，对接实时更新的OLAP的数据源，构建实时更新的数据大屏。</p><h1 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h1><p>真实的场景中，应该是业务系统不断生成数据，通过http上报到日志服务器中。将上述逻辑抽取出来，我写了一个数据生成的程序，可以向日志文件中不断生成数据。它使用faker库生成随机数据，自己再添上商品种类，订单成功与否的信息，将它们都打包为json字符串，向日志文件中源源不断的写入数据，最后就可以达到上面订单数据的效果。</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbfaebf17d9.png"><img src="https://s2.328888.xyz/2022/06/29/62bbfaebf17d9.png" alt="安装faker.png"></a></p><p>这是本项目所需的依赖库：kafka-python可以不要</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbfaeb35b91.png"><img src="https://s2.328888.xyz/2022/06/29/62bbfaeb35b91.png" alt="依赖库.png"></a></p><p>生成数据代码如下，生成的速度可以通过延时代码控制。</p><pre><code class="python">import jsonfrom faker import Fakerimport timefrom random import uniformfake = Faker(locale=&#39;zh_CN&#39;)def get_faker_log_data():    platforms = (&quot;Android-APP&quot;, &quot;IOS-APP&quot;, &quot;PC&quot;)    ok = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)    category = (        &quot;哲学&quot;, &quot;宗教&quot;, &quot;伦理&quot;, &quot;逻辑&quot;, &quot;美学&quot;, &quot;心理&quot;, &quot;语言&quot;, &quot;文学&quot;, &quot;政治&quot;,        &quot;经济&quot;, &quot;军事&quot;, &quot;法律&quot;, &quot;教育&quot;, &quot;体育&quot;, &quot;传媒&quot;, &quot;资讯&quot;, &quot;艺术&quot;,        &quot;管理&quot;, &quot;商贸&quot;, &quot;历史&quot;, &quot;考古&quot;, &quot;民族&quot;, &quot;生活&quot;, &quot;财金&quot;, &quot;统计&quot;, &quot;社会&quot;    )    return &#123;        &quot;time&quot;: int(time.time()),        &quot;id&quot;: fake.free_email(),        &quot;province&quot;: fake.province(),        &quot;money&quot;: fake.random_int(min=9, max=9999) + round(uniform(0, 1), 2),        &quot;category&quot;: category[fake.random_int(min=0, max=len(category) - 1)],        &quot;platform&quot;: platforms[fake.random_int(min=0, max=len(platforms) - 1)],        &quot;ok&quot;:  ok[fake.random_int(min=0, max=len(ok) - 1)]    &#125;if __name__ == &quot;__main__&quot;:    for _ in range(10000):        time.sleep(0.5)        with open(&quot;/PycharmProject/ZengYueFinalHomework/Generator/Log/LogData.log&quot;, &quot;a+&quot;, encoding=&#39;utf-8&#39;) as f:            data = get_faker_log_data()            json_data = json.dumps(data, ensure_ascii=False)            f.write(json_data + &quot;\n&quot;)            print(json_data)</code></pre><h1 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h1><p>这个部分是Apache Flume完成的，它是一个高可用的分布式日志采集框架<a href="https://flume.apache.org/index.html">Welcome to Apache Flume — Apache Flume</a></p><p>只需要配置source、channel、sink的信息，不用写代码，就可以完成日志采集的工作</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbf4dcc2b97.png"><img src="https://s2.328888.xyz/2022/06/29/62bbf4dcc2b97.png" alt="flume.png"></a></p><h2 id="编写file到kafka的flume配置文件"><a href="#编写file到kafka的flume配置文件" class="headerlink" title="编写file到kafka的flume配置文件"></a>编写file到kafka的flume配置文件</h2><pre><code class="shell"># agent的名称：a1# 指定sources、channels、sinks组件的名称a1.sources=r1a1.sinks=k1a1.channels=c1# 配置sources组件a1.sources.r1.type=execa1.sources.r1.command=tail -F /PycharmProject/ZengYueFinalHomework/Generator/Log/LogData.log# 配置channels组件a1.channels.c1.type=filea1.channels.c1.checkpointDir=./chka1.channels.c1.dataDirs=./data# 配置sinks组件a1.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.topic=FakeLoga1.sinks.k1.kafka.bootstrap.servers=node1:9092,node2:9092,node3:9092a1.sinks.k1.kafka.flumeBatchSize=100a1.sinks.k1.kafka.producer.acks=1a1.sinks.k1.kafka.producer.linger.ms=1# 把组件连一块儿a1.sources.r1.channels=c1a1.sinks.k1.channel=c1</code></pre><p>我们的目的是把日志文件经过flume输送到kafka，所以我们启动的顺序是先开kafka，再开flume。由于我的kafka依赖于zookeeper</p><p>所以要先启动zookeeper。</p><h2 id="启动zookeeper"><a href="#启动zookeeper" class="headerlink" title="启动zookeeper"></a>启动zookeeper</h2><p>在三台虚拟机中同时执行</p><pre><code class="shell">cd /export/server/zookeeper/binzkServer.sh start</code></pre><p><a href="https://s2.328888.xyz/2022/06/29/62bbf766ad90f.png"><img src="https://s2.328888.xyz/2022/06/29/62bbf766ad90f.png" alt="zk启动成功.png"></a></p><p>三台机器中读出现此进程，代表启动成功，用客户端工具连接检查无误。</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbf7dc50dcc.png"><img src="https://s2.328888.xyz/2022/06/29/62bbf7dc50dcc.png" alt="zk集群连接成功.png"></a></p><h2 id="启动kafka"><a href="#启动kafka" class="headerlink" title="启动kafka"></a>启动kafka</h2><p>针对kafka，我编写了一键启动脚本，在node1上输入如下命令</p><pre><code class="shell">cd /export/onekeystart-kafka.sh</code></pre><p><a href="https://s2.328888.xyz/2022/06/29/62bbf92b9b3a4.png"><img src="https://s2.328888.xyz/2022/06/29/62bbf92b9b3a4.png" alt="启动kafka.png"></a></p><p>kafka启动成功。</p><h2 id="启动kafka-eagle"><a href="#启动kafka-eagle" class="headerlink" title="启动kafka-eagle"></a>启动kafka-eagle</h2><p>为方便对kafka进行可视化的管理，就开始这个东西，它是基于web的一个kafka客户端，非常方便</p><pre><code class="shell">cd /export/server/kafka-eagle/binke.sh start</code></pre><p><a href="https://s2.328888.xyz/2022/06/29/62bbfa623efab.png"><img src="https://s2.328888.xyz/2022/06/29/62bbfa623efab.png" alt="启动kafka监控.png"></a></p><p>登录监控页面</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbfa619e8a2.png"><img src="https://s2.328888.xyz/2022/06/29/62bbfa619e8a2.png" alt="登录监控.png"></a></p><p><a href="https://s2.328888.xyz/2022/06/29/62bbfaeb68512.png"><img src="https://s2.328888.xyz/2022/06/29/62bbfaeb68512.png" alt="监控页面.png"></a></p><p>创建Topic，这个topic就作为ODS层</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbfc35e54b0.png"><img src="https://s2.328888.xyz/2022/06/29/62bbfc35e54b0.png" alt="创建topic.png"></a></p><h2 id="启动flume"><a href="#启动flume" class="headerlink" title="启动flume"></a>启动flume</h2><p>启动flume之前要用到hdfs，后续的计算引擎也要依赖yarn，所以先开启hadoop</p><pre><code class="shell">start-all.sh</code></pre><p><a href="https://s2.328888.xyz/2022/06/29/62bbf8e1924dc.png"><img src="https://s2.328888.xyz/2022/06/29/62bbf8e1924dc.png" alt="启动hadoop.png"></a></p><p>开启flume前的必备进程</p><p><a href="https://s2.328888.xyz/2022/06/29/62bbfcd742b59.png"><img src="https://s2.328888.xyz/2022/06/29/62bbfcd742b59.png" alt="所需的进程.png"></a></p><p>确定无误后，开启flume</p><pre><code class="shell">bin/flume-ng agent -c ./conf -f ./conf/access-kafka.conf -n a1 -Dflume.root.logger=ERROR,console</code></pre><p><a href="https://s2.328888.xyz/2022/06/29/62bbf76ff242f.png"><img src="https://s2.328888.xyz/2022/06/29/62bbf76ff242f.png" alt="flume启动成功.png"></a></p><p>看到上面的提示，就说明启动成功！</p><h1 id="数据的存储与计算"><a href="#数据的存储与计算" class="headerlink" title="数据的存储与计算"></a>数据的存储与计算</h1><p>刚才已经在kafka中建立了FakeLog这个topic作为数据的ODS层，接下来再建立LogComputed这个topic，作为DWS层，因为我们的数据结构很简单，不需要经过太复杂的计算，所以就不需要DWD层了。整个数仓结构就是ODS层（FakeLog）、DWS层（LogComputed）以及后续再Clickhouse中的APP层。</p><h2 id="spark-structured-streaming"><a href="#spark-structured-streaming" class="headerlink" title="spark structured streaming"></a>spark structured streaming</h2><p>接下来就是最重要的计算引擎部分，在企业生产过程中，它将承担起复杂的数据计算任务，与kafka、redis反复交互。</p><p>在这个项目的业务需求很简单，它的作用就是从kafka的FakeLog中取出数据，把数据中的订单支付不成功的过滤掉，把数据中的时间戳转化为日期格式，最终写入kafka的LogComputed中，来看看代码。</p><pre><code class="python">from pyspark.sql import SparkSessionimport osfrom pyspark.sql.functions import from_json, to_json, struct, from_unixtimefrom pyspark.sql.types import StructType, StringType, IntegerType, LongType, DecimalTypeif __name__ == &quot;__main__&quot;:    os.environ[&quot;JAVA_HOME&quot;] = &quot;/export/server/jdk1.8.0_241&quot;    spark = SparkSession.builder \        .appName(&quot;StructuredKafkaTest&quot;) \        .getOrCreate()    df = spark \        .readStream \        .format(&quot;kafka&quot;) \        .option(&quot;kafka.bootstrap.servers&quot;, &quot;node1:9092&quot;) \        .option(&quot;subscribe&quot;, &quot;FakeLog&quot;) \        .load()    schema = StructType() \        .add(&quot;time&quot;, LongType()) \        .add(&quot;id&quot;, StringType()) \        .add(&quot;province&quot;, StringType()) \        .add(&quot;money&quot;, DecimalType()) \        .add(&quot;category&quot;, StringType()) \        .add(&quot;platform&quot;, StringType()) \        .add(&quot;ok&quot;, IntegerType())    # kafka中拿过来的value数据是字节类型，要转换为字符串类型    df_toString = df.selectExpr(&quot;CAST(value AS STRING) as value&quot;)    # 解析完成后就是json字符串，用from_json解析json字符串    df_parsed = df_toString.withColumn(&quot;value&quot;, from_json(&quot;value&quot;, schema))    # 取出所有value下的字段    df_values = df_parsed.select(&quot;value.*&quot;)    # 进行数据清洗：把时间戳转为日期、过滤未成功订单    # 之后取出有用的属性打包为value字段，准备发往kafka    df_computed = df_values.withColumn(&quot;time&quot;, from_unixtime(&quot;time&quot;, &quot;yyyy-MM-dd HH:mm:ss&quot;)).filter(&quot;ok == 1&quot;) \        .select(to_json(struct(&quot;time&quot;, &quot;province&quot;, &quot;money&quot;, &quot;category&quot;, &quot;platform&quot;)).alias(&quot;value&quot;))    df_computed.writeStream \        .format(&quot;kafka&quot;) \        .option(&quot;kafka.bootstrap.servers&quot;, &quot;node1:9092&quot;) \        .option(&quot;topic&quot;, &quot;LogComputed&quot;) \        .option(&quot;checkpointLocation&quot;, &quot;kafka-ckp&quot;) \        .start() \        .awaitTermination()</code></pre><p>运行此程序，再运行刚才的日志生成程序，就可以看到计算后的数据被源源不断写入FakeLog与LogComputed主题中。</p><p>用kafka SQL分别看看这些数据吧~</p><p><a href="https://s2.328888.xyz/2022/06/29/62bc0315c3758.png"><img src="https://s2.328888.xyz/2022/06/29/62bc0315c3758.png" alt="kafka数据.png"></a></p><p><a href="https://s2.328888.xyz/2022/06/29/62bc0315c0b3e.png"><img src="https://s2.328888.xyz/2022/06/29/62bc0315c0b3e.png" alt="kafka中的数据2.png"></a></p><p>数据正常，说明spark程序没问题。</p><h1 id="数据的分析"><a href="#数据的分析" class="headerlink" title="数据的分析"></a>数据的分析</h1><p>kafka中的LogComputed就是处理完成可以用于分析的数据了，一般会将其导入OLAP引擎中，传统的方案是Hbase，但在实时流计算系统中太慢了，目前有Apache Kylin、Apache Doris、Clickhouse等可选。Clickhouse是俄罗斯开源的OLAP数据库它性能强劲，对CPU的性能压榨到极致，目前许多企业都在用，它能很好地支持kafka中的数据源并且与下游的BI系统也有良好的生态支持，所以我在项目中选择了它。</p><p>clickhouse引入kafka数据是通过clickhouse的kafka引擎表和物化视图来配合完成的，此时clickhouse就相对于kafka的消费者。</p><p>在clickhouse，我们只选取了time、province、money、category、platform这5个字段。</p><h2 id="建立目标表"><a href="#建立目标表" class="headerlink" title="建立目标表"></a>建立目标表</h2><p>kafka过来的数据最终就到这里</p><pre><code class="sql">-- auto-generated definitioncreate table target(    time     DateTime64(3),    province Nullable(String),    money    Decimal(9, 3),    category Nullable(String),    platform Nullable(String))    engine = MergeTree PARTITION BY toYYYYMM(time)        ORDER BY money        SETTINGS index_granularity = 8192;</code></pre><h2 id="建立kafka引擎表"><a href="#建立kafka引擎表" class="headerlink" title="建立kafka引擎表"></a>建立kafka引擎表</h2><p>kafka的数据就是通过这张表源源不断抽取过来的</p><pre><code class="sql">-- auto-generated definitioncreate table queue(    time     DateTime64(3),    province Nullable(String),    money    Decimal(9, 3),    category Nullable(String),    platform Nullable(String))    engine = Kafka SETTINGS kafka_broker_list = &#39;node1:9092&#39;, kafka_topic_list = &#39;LogComputed&#39;, kafka_group_name = &#39;clickhouse&#39;, kafka_format = &#39;JSONEachRow&#39;;</code></pre><h2 id="建立物化视图"><a href="#建立物化视图" class="headerlink" title="建立物化视图"></a>建立物化视图</h2><p>如果没有物化视图，那么target表中的数据被读一次后就会被自动删除，要用物化视图固定下来</p><pre><code class="sql">-- auto-generated definitioncreate materialized view consumer To target asselect * from queue;</code></pre><p>建立好了这仨后，启动spark程序，启动数据生成程序，就可以发现target表中，源源不断有数据添加进来。</p><p><a href="https://s2.328888.xyz/2022/06/29/62bc09a344609.png"><img src="https://s2.328888.xyz/2022/06/29/62bc09a344609.png" alt="clickhouse中的数据.png"></a></p><h1 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h1><h2 id="连接clickhouse"><a href="#连接clickhouse" class="headerlink" title="连接clickhouse"></a>连接clickhouse</h2><p>在大二下学期的时候，我学习了BI软件tableau，它可以支持丰富的数据源，也包括clickhouse</p><p>tableau连接clickhouse是通过ODBC的方式连接的，所以先要下载安装clickhouse的ODBC驱动程序才行。</p><p><a href="https://s2.328888.xyz/2022/06/29/62bc0bb60ced2.png"><img src="https://s2.328888.xyz/2022/06/29/62bc0bb60ced2.png" alt="ckODBC.png"></a></p><p>连接上后，设置实时，就可以更新数据了</p><p><a href="https://s2.328888.xyz/2022/06/29/62bc0c8e4dd79.png"><img src="https://s2.328888.xyz/2022/06/29/62bc0c8e4dd79.png" alt="ck连接tableau.png"></a></p><h2 id="绘制图像"><a href="#绘制图像" class="headerlink" title="绘制图像"></a>绘制图像</h2><p>选取相关字段，分别绘制</p><p>各省市销售额汇总、各平台订单销售情况、各品类销售额汇总、各品类销售额占比、各品类订单数占比、各平台销售占比、各平台订单占比、总订单和总金额，这九张图，把这九张图摆在一块，构成一个仪表板。</p><p><a href="https://s2.328888.xyz/2022/06/29/62bc0fa344f39.png"><img src="https://s2.328888.xyz/2022/06/29/62bc0fa344f39.png" alt="tableau图.png"></a></p><p>在数据源中更新数据，就可以发现仪表板中的图像在变化</p><p><a href="https://s2.328888.xyz/2022/06/29/62bc115f1bb5c.png"><img src="https://s2.328888.xyz/2022/06/29/62bc115f1bb5c.png" alt="更新数据源.png"></a></p><h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><p>把先有的数据通过tableau的数据提取功能提取出来，然后把我的作品上传到云端，这样就可以在线访问啦！</p><p><a href="https://s2.328888.xyz/2022/06/29/62bc130c68412.png"><img src="https://s2.328888.xyz/2022/06/29/62bc130c68412.png" alt="在线仪表板.png"></a></p><p>点击下方连接，就可以与我一起洞察数据</p><p><a href="https://public.tableau.com/app/profile/.48701339/viz/6_29spark/6_29?publish=yes">6.29图书交易实时大屏（spark期末作业） | Tableau Public</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>京东防晒霜大数据分析</title>
      <link href="/2022/06/26/%E4%BA%AC%E4%B8%9C%E9%98%B2%E6%99%92%E9%9C%9C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/"/>
      <url>/2022/06/26/%E4%BA%AC%E4%B8%9C%E9%98%B2%E6%99%92%E9%9C%9C%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据架构下的爬虫数据分析"><a href="#大数据架构下的爬虫数据分析" class="headerlink" title="大数据架构下的爬虫数据分析"></a>大数据架构下的爬虫数据分析</h1><h2 id="一：前言"><a href="#一：前言" class="headerlink" title="一：前言"></a>一：前言</h2><p>此项目数据来源于京东防晒霜的商品评论页面，覆盖18个知名品牌的18000条评论数据，用自然语言处理技术，对评论内容进行情感分析，此外还采集了一些其它相关指标。</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b759473f8ba.png" alt="品牌.png"></p><p>项目中广泛应用了各种数据分析以及大数据技术框架，包括<strong>pandas</strong>、<strong>mysql</strong>、<strong>Hadoop</strong>、<strong>Hive</strong>、<strong>Spark</strong>、<strong>DataX</strong>、<strong>Clickhouse</strong>，旨在将之前学习过的技术框架，融入其中，真正做到学以致用。</p><h2 id="二：数据采集"><a href="#二：数据采集" class="headerlink" title="二：数据采集"></a>二：数据采集</h2><p>目标网站：</p><p><a href="https://item.jd.com/100006200839.html">https://item.jd.com/100006200839.html</a><br><a href="https://item.jd.com/68861774169.html">https://item.jd.com/68861774169.html</a><br><a href="https://item.jd.com/3126582.html">https://item.jd.com/3126582.html</a><br><a href="https://item.jd.com/100006229545.html">https://item.jd.com/100006229545.html</a><br><a href="https://item.jd.com/100023877752.html">https://item.jd.com/100023877752.html</a><br><a href="https://item.jd.com/100034791576.html">https://item.jd.com/100034791576.html</a><br><a href="https://item.jd.com/25434416028.html">https://item.jd.com/25434416028.html</a><br><a href="https://item.jd.com/100026937658.html">https://item.jd.com/100026937658.html</a><br><a href="https://item.jd.com/100002143889.html">https://item.jd.com/100002143889.html</a><br><a href="https://item.jd.com/100010921438.html">https://item.jd.com/100010921438.html</a><br><a href="https://item.jd.com/189592.html">https://item.jd.com/189592.html</a><br><a href="https://item.jd.com/100030536406.html">https://item.jd.com/100030536406.html</a><br><a href="https://item.jd.com/100012405930.html">https://item.jd.com/100012405930.html</a><br><a href="https://item.jd.com/100011386776.html">https://item.jd.com/100011386776.html</a><br><a href="https://item.jd.com/100030831926.html">https://item.jd.com/100030831926.html</a><br><a href="https://item.jd.com/100011410938.html">https://item.jd.com/100011410938.html</a><br><a href="https://item.jd.com/100024228010.html">https://item.jd.com/100024228010.html</a><br><a href="https://item.jd.com/100018271858.html">https://item.jd.com/100018271858.html</a></p><p>因为想尝试零代码爬虫方案，所以选择了《八爪鱼采集器》这款爬虫即服务软件，根据模板，仅需配置，就可爬取目标数据。</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b75c5bcd0e8.png" alt="爬虫模板.png"></p><p>爬取过程</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b75c5c8b77d.png" alt="爬虫过程.png"></p><p>爬取完毕后，将其导出为csv文件，便于后续处理</p><h2 id="三：数据预处理"><a href="#三：数据预处理" class="headerlink" title="三：数据预处理"></a>三：数据预处理</h2><p>得到数据后，用pandas进行数据的清洗</p><pre><code class="python">import pandas as pdimport numpy as npfrom snownlp import SnowNLP</code></pre><pre><code class="python">brands = (    &quot;AHC&quot;,&quot;安热沙&quot;,&quot;百雀羚&quot;,&quot;碧柔&quot;,&quot;大宝&quot;,&quot;高姿&quot;,&quot;曼秀雷敦&quot;,&quot;美肤宝&quot;,&quot;蜜丝婷&quot;,    &quot;娜丽丝&quot;,&quot;妮维雅&quot;,&quot;欧莱雅&quot;,&quot;水宝宝&quot;,&quot;雅漾&quot;,&quot;怡思丁&quot;,&quot;玉兰油&quot;,&quot;玥之秘&quot;,&quot;资生堂&quot;)</code></pre><pre><code class="python">dfs = [pd.read_csv(f&quot;./data/&#123;name&#125;.csv&quot;) for name in brands]</code></pre><p>加载数据，18个品牌，每个品牌的数据1000条，刚刚好18000条，其实我也想爬更多，但是京东做了限制，100页之后的评论不可见，所以就没办法啦~</p><pre><code class="python">df_all = pd.concat(dfs, ignore_index=True)len(df_all)</code></pre><pre><code>18000</code></pre><p>看看数据的样子</p><pre><code class="python">df_all.head(1)</code></pre><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>会员</th>      <th>级别</th>      <th>评价星级</th>      <th>评价内容</th>      <th>时间</th>      <th>点赞数</th>      <th>评论数</th>      <th>追评时间</th>      <th>追评内容</th>      <th>商品属性</th>      <th>页面网址</th>      <th>页面标题</th>      <th>采集时间</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>影***i     ...</td>      <td>PLUS会员</td>      <td>star5</td>      <td>收到货，第一时间拆包装，质量非常好，包装很仔细、严实。开始的时候是通过朋友介绍买的，用了以后...</td>      <td>2021-09-23 23:39</td>      <td>0</td>      <td>1</td>      <td>NaN</td>      <td>NaN</td>      <td>一支50g</td>      <td>https://item.jd.com/68861774169.html</td>      <td>\n                                        \n  ...</td>      <td>2022-06-24 15:09:59.717</td>    </tr>  </tbody></table>清洗数据<pre><code class="python">df_all[&quot;级别&quot;] = df_all[&quot;级别&quot;].map(&#123;np.nan: False, &quot;PLUS会员&quot;: True&#125;)</code></pre><pre><code class="python">df_all[&quot;评价星级&quot;] = df_all[&quot;评价星级&quot;].map(&#123;&quot;star5&quot;: 5, &quot;star4&quot;: 4, &quot;star3&quot;: 3, &quot;star2&quot;: 2, &quot;star1&quot;: 1&#125;)df_all[&quot;评价星级&quot;] = pd.to_numeric(df_all[&quot;评价星级&quot;], downcast=&quot;signed&quot;)</code></pre><pre><code class="python">df_all[&quot;追评时间&quot;] = df_all[&quot;追评时间&quot;].str.extract(&quot;(\d+)&quot;)df_all[&quot;追评时间&quot;].fillna(&quot;0&quot;, inplace=True)df_all[&quot;追评时间&quot;] = pd.to_numeric(df_all[&quot;追评时间&quot;], downcast=&quot;signed&quot;)df_all[&quot;追评内容&quot;].fillna(&quot;&quot;, inplace=True)</code></pre><pre><code class="python">df_all[&quot;商品属性&quot;] = df_all[&quot;商品属性&quot;].str.strip()df_all[&quot;页面标题&quot;] = df_all[&quot;页面标题&quot;].str.strip()</code></pre><pre><code class="python">def brand(x):    for name in brands:        if name in x:            return name</code></pre><pre><code class="python">df_all[&quot;品牌&quot;] = df_all[&quot;页面标题&quot;].apply(brand)</code></pre><pre><code class="python">df_all[&quot;商品名称&quot;] = df_all[&quot;品牌&quot;] + df_all[&quot;商品属性&quot;]</code></pre><pre><code class="python">df_all[&quot;总体评价&quot;] = df_all[&quot;评价内容&quot;] + df_all[&quot;追评内容&quot;]</code></pre><pre><code class="python">df_all.head(1)</code></pre><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>会员</th>      <th>级别</th>      <th>评价星级</th>      <th>评价内容</th>      <th>时间</th>      <th>点赞数</th>      <th>评论数</th>      <th>追评时间</th>      <th>追评内容</th>      <th>商品属性</th>      <th>页面网址</th>      <th>页面标题</th>      <th>采集时间</th>      <th>品牌</th>      <th>商品名称</th>      <th>总体评价</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>影***i     ...</td>      <td>True</td>      <td>5</td>      <td>收到货，第一时间拆包装，质量非常好，包装很仔细、严实。开始的时候是通过朋友介绍买的，用了以后...</td>      <td>2021-09-23 23:39</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td></td>      <td>一支50g</td>      <td>https://item.jd.com/68861774169.html</td>      <td>【亰东7仓发货】韩国AHC防晒霜防晒乳小蓝瓶防晒隔离遮瑕三合一女学生军训防水防汗男孕妇SPF...</td>      <td>2022-06-24 15:09:59.717</td>      <td>AHC</td>      <td>AHC一支50g</td>      <td>收到货，第一时间拆包装，质量非常好，包装很仔细、严实。开始的时候是通过朋友介绍买的，用了以后...</td>    </tr>  </tbody></table>清洗完成，利用snownlp对评论进行情感分析，得分区间是0-1，接近0代表消极，1则代表积极<pre><code class="python">def sentiment(text):    s = SnowNLP(text)    return s.sentiments</code></pre><pre><code class="python">df_all[&quot;情感评分&quot;] = df_all[&quot;总体评价&quot;].apply(sentiment)</code></pre><p>得到情感评分数据后，选取有用的列</p><pre><code class="python">df_out = df_all[[&quot;级别&quot;,&quot;评价星级&quot;,&quot;时间&quot;,&quot;点赞数&quot;,&quot;评论数&quot;,&quot;追评时间&quot;,&quot;品牌&quot;,&quot;商品名称&quot;,&quot;情感评分&quot;]]df_out.columns = [&quot;vip&quot;,&quot;level&quot;,&quot;time&quot;,&quot;like_value&quot;,&quot;comment_value&quot;,&quot;day&quot;,&quot;brand&quot;,&quot;goods&quot;,&quot;sentiment&quot;]</code></pre><pre><code class="python">df_out.head(1)</code></pre><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>vip</th>      <th>level</th>      <th>time</th>      <th>like_value</th>      <th>comment_value</th>      <th>day</th>      <th>brand</th>      <th>goods</th>      <th>sentiment</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>True</td>      <td>5</td>      <td>2021-09-23 23:39</td>      <td>0</td>      <td>1</td>      <td>0</td>      <td>AHC</td>      <td>AHC一支50g</td>      <td>0.959606</td>    </tr>  </tbody></table>之后，导出处理好的数据文件<pre><code class="python">df_out.to_csv(&quot;output.csv&quot;, encoding=&quot;utf_8_sig&quot;)</code></pre><h2 id="三：将csv数据导入mysql"><a href="#三：将csv数据导入mysql" class="headerlink" title="三：将csv数据导入mysql"></a>三：将csv数据导入mysql</h2><p>这一步是为了更好模拟实际开发中的场景而添加的，并不会在mysql中作任何处理。</p><p>导入的时候要移除一个空列，它是用pandas导出csv的时候未设置忽略行索引产生的。</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b75fbf9dd91.png" alt="导入mysql.png"></p><p>导入成功</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b75fbfcf8c3.png" alt="导入mysql成功.png"></p><h2 id="四：从Mysql导入Hive"><a href="#四：从Mysql导入Hive" class="headerlink" title="四：从Mysql导入Hive"></a>四：从Mysql导入Hive</h2><p>首先创建Hive表，字段一模一样</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b75fbf88004.png" alt="创建hive表.png"></p><p>数据导出过程，我用了阿里巴巴开源的数据同步工具DataX，相较于Sqoop，它简单，易配置，通用性强，不依赖Hadoop生态。</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b75fc04f9e4.png" alt="使用datax.png"></p><p>配置文件如下</p><pre><code class="json">&#123;    &quot;job&quot;: &#123;        &quot;content&quot;: [            &#123;                &quot;reader&quot;: &#123;                    &quot;name&quot;: &quot;mysqlreader&quot;,                    &quot;parameter&quot;: &#123;                        &quot;username&quot;: &quot;root&quot;,                        &quot;password&quot;: &quot;123456&quot;,                        &quot;column&quot;: [                            &quot;vip&quot;,                            &quot;level&quot;,                            &quot;time&quot;,                            &quot;like_value&quot;,                            &quot;comment_value&quot;,                            &quot;day&quot;,                            &quot;brand&quot;,                            &quot;goods&quot;,                            &quot;sentiment&quot;                        ],                        &quot;connection&quot;: [                            &#123;                                &quot;table&quot;: [                                    &quot;sunblock&quot;                                ],                                &quot;jdbcUrl&quot;: [                                    &quot;jdbc:mysql://node1:3306/sunblock&quot;                                ]                            &#125;                        ]                    &#125;                &#125;,                &quot;writer&quot;: &#123;                    &quot;name&quot;: &quot;hdfswriter&quot;,                    &quot;parameter&quot;: &#123;                        &quot;defaultFS&quot;: &quot;hdfs://node1:8020&quot;,                        &quot;fileType&quot;: &quot;text&quot;,                        &quot;path&quot;: &quot;/user/hive/warehouse/sparkfinalhomework.db/sunblock&quot;,                        &quot;fileName&quot;: &quot;sunblock.txt&quot;,                        &quot;column&quot;: [                            &#123;                                &quot;name&quot;: &quot;vip&quot;,                                &quot;type&quot;: &quot;string&quot;                            &#125;,                            &#123;                                &quot;name&quot;: &quot;level&quot;,                                &quot;type&quot;: &quot;int&quot;                            &#125;,                            &#123;                                &quot;name&quot;: &quot;time&quot;,                                &quot;type&quot;: &quot;date&quot;                            &#125;,                            &#123;                                &quot;name&quot;: &quot;like_value&quot;,                                &quot;type&quot;: &quot;int&quot;                            &#125;,                            &#123;                                &quot;name&quot;: &quot;comment_value&quot;,                                &quot;type&quot;: &quot;int&quot;                            &#125;,                            &#123;                                &quot;name&quot;: &quot;day&quot;,                                &quot;type&quot;: &quot;int&quot;                            &#125;,                            &#123;                                &quot;name&quot;: &quot;brand&quot;,                                &quot;type&quot;: &quot;string&quot;                            &#125;,                            &#123;                                &quot;name&quot;: &quot;goods&quot;,                                &quot;type&quot;: &quot;string&quot;                            &#125;,                            &#123;                                &quot;name&quot;: &quot;sentiment&quot;,                                &quot;type&quot;: &quot;string&quot;                            &#125;                        ],                        &quot;writeMode&quot;: &quot;append&quot;,                        &quot;fieldDelimiter&quot;: &quot;\t&quot;                    &#125;                &#125;            &#125;        ],        &quot;setting&quot;: &#123;            &quot;speed&quot;: &#123;                &quot;channel&quot;: 6            &#125;        &#125;    &#125;&#125;</code></pre><p>加入dataX目录，执行</p><pre><code class="shell">python bin/datax.py job/mysql2hdfs.json</code></pre><p>dataX提示成功</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b75fc041c15.png" alt="导入hive成功.png"></p><p>在Hive表中查询数据，果然能查到</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b75fbfd4617.png" alt="hive表映射成功.png"></p><h2 id="五：Spark-on-Hive构建数仓"><a href="#五：Spark-on-Hive构建数仓" class="headerlink" title="五：Spark on Hive构建数仓"></a>五：Spark on Hive构建数仓</h2><p>SparkSql基于内存计算，相较于基于mapreduce的HiveSql，性能更强大。但Spark无源数据管理，因此，可以借用Hive的源数据管理功能。这就是Spark on Hive的模式——用Spark算，用Hive存。</p><p>用数仓理论的眼光看，刚才从mysql中直接导出到hive的数据，就算是ODS层</p><p>那么用SparkSql执行以下语句，创建这些中间表，就算是DWS层和APP层了</p><pre><code class="sql">use sparkfinalhomework;create table brand_vip as (    select brand, count(*) as vip_count    from sunblock    where vip = &quot;True&quot;    group by brand);create table total_vip as (    select count(*) as count    from sunblock    group by vip);create table brand_sentiment as (    select brand, avg(sentiment) as sentiment_avg    from sunblock    group by brand);create table brand_vip_level_sentiment as (    select brand, avg(level) as vip_level, avg(sentiment) as vip_sentiment    from sunblock    where vip = &quot;True&quot;    group by brand);create table goods_sentiment as (    select goods, avg(sentiment) as sentiment_avg    from sunblock    group by goods);create table brand_level as (    select brand, avg(level) as level    from sunblock    group by brand);create table goods_level as (    select goods, avg(level) as level    from sunblock    group by goods);create table time_sentiment as (    select `time`, sentiment    from sunblock    order by `time`);create table likes_comments as (    select sum(like_value) as total_like, sum(comment_value) as total_comments, count(*) total    from sunblock);create table day_comment as (    select day, count(*) as count    from sunblock    group by day    order by day);create table vip_level_sentiment as (    select vip, avg(level) as level_avg, avg(sentiment) as sentiment_avg    from sunblock    group by vip);create table about_brand as (    select bv.brand, bv.vip_count, bs.sentiment_avg, bl.level, bvls.vip_level, bvls.vip_sentiment    from brand_vip bv        join brand_sentiment bs on bv.brand = bs.brand        join brand_level bl on bs.brand = bl.brand        join brand_vip_level_sentiment bvls on bl.brand = bvls.brand);create table about_goods as (    select gl.goods, gl.level, gs.sentiment_avg    from goods_level gl join goods_sentiment gs on gl.goods = gs.goods);</code></pre><h2 id="六：实现即席查询"><a href="#六：实现即席查询" class="headerlink" title="六：实现即席查询"></a>六：实现即席查询</h2><p>刚才用Spark计算得到APP层这种可以直接用于查询分析的数据了，可是Spark依然满足不了即席查询的需求，所以我选择了更为强大的OLAP引擎——ClickHouse，它是俄罗斯开源的一款数据库软件，以性能强大闻名于世。</p><p>首先，依然是用dataX将hive表中的数据导入clickhouse中，当然，得要先在clickhouse中有相对应的表</p><p>以about_brand表为例，创建表</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b76967d1b0a.png" alt="创建clickhouse表.png"></p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b76969cd392.png" alt="ck表空.png"></p><p>创建完了就可以用DataX导数据啦，这次DataX的配置文件是这样的</p><pre><code class="json">&#123;    &quot;job&quot;: &#123;        &quot;content&quot;: [            &#123;                &quot;reader&quot;: &#123;                    &quot;name&quot;: &quot;hdfsreader&quot;,                    &quot;parameter&quot;: &#123;                    &quot;column&quot;: [&quot;*&quot;],                        &quot;defaultFS&quot;: &quot;hdfs://node1:8020&quot;,                        &quot;encoding&quot;: &quot;UTF-8&quot;,                        &quot;fieldDelimiter&quot;: &quot;\001&quot;,                        &quot;fileType&quot;: &quot;textfile&quot;,                        &quot;path&quot;: &quot;/user/hive/warehouse/sparkfinalhomework.db/about_brand/*&quot;                    &#125;                &#125;,                &quot;writer&quot;: &#123;                    &quot;name&quot;: &quot;clickhousewriter&quot;,                    &quot;parameter&quot;: &#123;                        &quot;batchByteSize&quot;: 134217728,                        &quot;batchSize&quot;: 65536,                        &quot;column&quot;: [                            &quot;brand&quot;,                            &quot;vip_count&quot;,                            &quot;sentiment_avg&quot;,                            &quot;level&quot;,                            &quot;vip_level&quot;,                            &quot;vip_sentiment&quot;                        ],                        &quot;connection&quot;: [                            &#123;                                &quot;jdbcUrl&quot;: &quot;jdbc:clickhouse://node1:8123/JD_goods&quot;,                                &quot;table&quot;: [                                    &quot;about_brand&quot;,                                ]                            &#125;                        ],                        &quot;dryRun&quot;: false,                        &quot;password&quot;: &quot;&quot;,                        &quot;postSql&quot;: [],                        &quot;preSql&quot;: [],                        &quot;username&quot;: &quot;&quot;,                        &quot;writeMode&quot;: &quot;insert&quot;                    &#125;                &#125;            &#125;        ],        &quot;setting&quot;: &#123;            &quot;speed&quot;: &#123;                &quot;channel&quot;: &quot;&quot;            &#125;        &#125;    &#125;&#125;</code></pre><p>执行dataX的命令，就可以导入成功</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b7696e54c6b.png" alt="导入ck表成功.png"></p><p>对每个想要导入的表，设置相应配置文件，重复上述步骤，即可将APP层的数据导入clickhouse这个数据库中，效果如下</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b7696ece0fa.png" alt="ck导入完毕.png"></p><h2 id="七：用BI工具进行可视化"><a href="#七：用BI工具进行可视化" class="headerlink" title="七：用BI工具进行可视化"></a>七：用BI工具进行可视化</h2><p>因为上学期刚刚学了Tableau，为了温故而知新，此次就选择它担任可视化的任务</p><p>我分成了品牌——商品——vip——评论四个角度，组织了12张<strong>交互式</strong>的图表，呈现在了</p><p><a href="https://public.tableau.com/shared/P6R3WDD6M?:display_count=n&amp;:origin=viz_share_link">https://public.tableau.com/shared/P6R3WDD6M?:display_count=n&amp;:origin=viz_share_link</a></p><p>这个网页中，点击进去和我一同探索数据奥秘。</p><p>它们是这样的</p><p><img src="https://i-s2.328888.xyz/2022/06/26/62b76b591bc1e.png" alt="1656187732330.png"><br><img src="https://i-s2.328888.xyz/2022/06/26/62b76b829d640.png" alt="1656187788618.png"><br><img src="https://i-s2.328888.xyz/2022/06/26/62b76ba37e7b3.png" alt="1656187821538.png"><br><img src="https://i-s2.328888.xyz/2022/06/26/62b76bba1033d.png" alt="1656187841477.png"></p><h2 id="八：总结"><a href="#八：总结" class="headerlink" title="八：总结"></a>八：总结</h2><p>上面的图表中，我提炼出四条最为重要结论</p><ol><li>玉兰油的品牌的防晒霜的顾客口碑显著低于其他品牌</li><li>高姿、欧莱雅、资生堂、雅漾的顾客口碑表现优异</li><li>防晒霜顾客评论情绪波动随时间逐渐趋于平稳</li><li>vip客户对产品容忍度更高</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark MLlib极速入门</title>
      <link href="/2022/06/13/%E4%B8%80%E4%B8%AA%E6%A1%88%E4%BE%8B%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8SparkMLlib/"/>
      <url>/2022/06/13/%E4%B8%80%E4%B8%AA%E6%A1%88%E4%BE%8B%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8SparkMLlib/</url>
      
        <content type="html"><![CDATA[<h1 id="一个案例快速入门Spark-MLlib"><a href="#一个案例快速入门Spark-MLlib" class="headerlink" title="一个案例快速入门Spark MLlib"></a>一个案例快速入门Spark MLlib</h1><h2 id="一：前言"><a href="#一：前言" class="headerlink" title="一：前言"></a>一：前言</h2><p><strong>MLlib</strong>是<strong>Apache Spark</strong>的可扩展机器学习库，是一个分布式的机器学习框架，可用于海量数据的机器学习场景。</p><p><img src="https://s1.328888.xyz/2022/06/13/p2VDg.png" alt="p2VDg.png"></p><p>今天用一个美国 1994 年人口普查的数据集，通过研究预测居民收入是否超过年收入50万美元这个二分类问题，带大家快速入门<strong>Spark MLlib</strong>。</p><p>这个数据集来源于<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/adult/">https://archive.ics.uci.edu/ml/machine-learning-databases/adult/</a></p><p>下载后是.data格式的文件，把后缀改成.csv后，发现长这个样子</p><p><img src="https://s1.328888.xyz/2022/06/13/p2jBt.png" alt="p2jBt.png"></p><p>一共有15列，其中最后一列是目标变量，其它14列是属性变量。</p><p>它们的列名以及含义如下图所示</p><p><img src="https://s1.328888.xyz/2022/06/13/p2MIO.png" alt="p2MIO.png"></p><p>用于分类的模型有很多</p><p><img src="https://s1.328888.xyz/2022/06/13/p268e.png" alt="p268e.png"></p><p>从中，我挑取逻辑回归、决策树、随机森林、梯度提升树、线性支持向量机模型进行演示</p><p>用ROC曲线下面积作为评价指标（越大证明模型越好）</p><h2 id="二：数据预处理"><a href="#二：数据预处理" class="headerlink" title="二：数据预处理"></a>二：数据预处理</h2><p>引入pyspark, 构造spark程序会话，这是编写spark程序的标准开头</p><pre><code class="python">import pysparkfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;一个案例快速入门Spark MLlib&quot;).getOrCreate()</code></pre><pre><code class="python">spark</code></pre><p><img src="https://s1.328888.xyz/2022/06/13/p2FeC.png" alt="p2FeC.png"></p><p>用read.csv方法读取数据集</p><pre><code class="python">df_spark = spark.read.csv(&quot;./data/adult.csv&quot;, header=True, inferSchema=True)df_spark.printSchema()</code></pre><pre><code>root |-- age: integer (nullable = true) |-- workclass: string (nullable = true) |-- fnlwgt: integer (nullable = true) |-- education: string (nullable = true) |-- education_num: integer (nullable = true) |-- maritial_status: string (nullable = true) |-- occupation: string (nullable = true) |-- relationship: string (nullable = true) |-- race: string (nullable = true) |-- sex: string (nullable = true) |-- capitial_gain: integer (nullable = true) |-- capitial_loss: integer (nullable = true) |-- hours_per_week: integer (nullable = true) |-- native_country: string (nullable = true) |-- income: string (nullable = true)</code></pre><p>​    </p><h4 id="处理属性变量"><a href="#处理属性变量" class="headerlink" title="处理属性变量"></a>处理属性变量</h4><p>把数值型的属性变量和类别型的属性变量挑出来</p><pre><code class="python">numeric = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education_num&#39;, &#39;capitial_gain&#39;,  &#39;capitial_gain&#39;, &#39;capitial_loss&#39;]category = [&#39;workclass&#39;, &#39;education&#39;, &#39;maritial_status&#39;, &#39;occupation&#39;,             &#39;relationship&#39;, &#39;race&#39;, &#39;sex&#39;, &#39;native_country&#39;]</code></pre><p>接下来 要准备先将类别变量转化为数值变量，再将数值变量进行onehot编码，<br>先后用StringIndexer和OneHotEncoder就可以实现</p><pre><code class="python">from pyspark.ml.feature import StringIndexercategory_index = [f&#39;&#123;c&#125;_index&#39; for c in category]indexer = StringIndexer(inputCols = category, outputCols = category_index)</code></pre><pre><code class="python">from pyspark.ml.feature import OneHotEncodercategory_onehot = [f&#39;&#123;c&#125;_onehot&#39; for c in category]onehoter = OneHotEncoder(inputCols = category_index, outputCols = category_onehot)</code></pre><p>而后 将数值变量和转换后的类别变量分别打包为两个变量，<br>numeric_features和category_features_onehot，<br>使用VectorAssembler即可</p><pre><code class="python">from pyspark.ml.feature import VectorAssemblernumeric_features_assembler = VectorAssembler(inputCols = numeric, outputCol = &quot;numeric_features&quot;)category_features_assembler = VectorAssembler(inputCols = category_onehot, outputCol = &quot;category_features_onehot&quot;)</code></pre><p>紧接着 要把打包后的数值变量形成的列——numeric_features，<br>进行min-max标准化，也就是把值映射到0-1中间的数，<br>用MinMaxScaler完成</p><pre><code class="python">from pyspark.ml.feature import MinMaxScalerminmaxer = MinMaxScaler(inputCol = &quot;numeric_features&quot; , outputCol = &quot;numeric_features_minmax&quot;)</code></pre><p>准备好了上面的东西后，就按顺序对数据集进行处理</p><pre><code class="python">df_spark = indexer.fit(df_spark).transform(df_spark)df_spark = onehoter.fit(df_spark).transform(df_spark)df_spark = numeric_features_assembler.transform(df_spark)df_spark = category_features_assembler.transform(df_spark)df_spark = minmaxer.fit(df_spark).transform(df_spark)</code></pre><h4 id="处理标签变量"><a href="#处理标签变量" class="headerlink" title="处理标签变量"></a>处理标签变量</h4><p>对属性变量的处理完成后，就要处理标签变量，<br>我们对标签列，进行数值化处理即可，使用StringIndexer完成</p><pre><code class="python">indexer = StringIndexer(inputCol = &quot;income&quot;, outputCol = &quot;label&quot;)</code></pre><pre><code class="python">df_spark = indexer.fit(df_spark).transform(df_spark)</code></pre><h4 id="整体拼接"><a href="#整体拼接" class="headerlink" title="整体拼接"></a>整体拼接</h4><p>把刚才处理好的类别属性——category_features_onehot，<br>数值属性——numeric_features_minmax以及标签label拿出来看看</p><pre><code class="python">df_spark = df_spark.select([&quot;category_features_onehot&quot;, &quot;numeric_features_minmax&quot;, &quot;label&quot;])df_spark.show(1)</code></pre><pre><code>+------------------------+-----------------------+-----+|category_features_onehot|numeric_features_minmax|label|+------------------------+-----------------------+-----+|    (94,[4,10,24,32,4...|   [0.30136986301369...|  0.0|+------------------------+-----------------------+-----+only showing top 1 row</code></pre><p>​    </p><p>接下来 把所有属性都打包为一个列——features，<br>也是使用VectorAssembler完成</p><pre><code class="python">assembler = VectorAssembler(inputCols = [&quot;category_features_onehot&quot;, &quot;numeric_features_minmax&quot;], outputCol = &quot;features&quot;)df_spark = assembler.transform(df_spark)</code></pre><p>选出features列和label列，其中feature包含了属性的所有信息，label就是目标变量，<br>最终就得到了清洗好的数据集，可以作为模型的输入值，<br>来看看是啥样的</p><pre><code class="python">final_df = df_spark.select([&quot;features&quot;, &quot;label&quot;])final_df.show(10)</code></pre><pre><code>+--------------------+-----+|            features|label|+--------------------+-----+|(100,[4,10,24,32,...|  0.0||(100,[1,10,23,31,...|  0.0||(100,[0,8,25,38,4...|  0.0||(100,[0,13,23,38,...|  0.0||(100,[0,10,23,29,...|  0.0||(100,[0,11,23,31,...|  0.0||(100,[0,18,28,34,...|  0.0||(100,[1,8,23,31,4...|  1.0||(100,[0,11,24,29,...|  1.0||(100,[0,10,23,31,...|  1.0|+--------------------+-----+only showing top 10 rows</code></pre><p>​    </p><h2 id="三：模型训练"><a href="#三：模型训练" class="headerlink" title="三：模型训练"></a>三：模型训练</h2><p>接下来我们就用常见的分类模型，对数据对模型进行训练，<br>并且使用ROC曲线下面积，作为模型的的评价指标</p><p>先把数据集整体划分为训练集和测试集，七三开</p><pre><code class="python">train_data, test_data =final_df.randomSplit([0.7, 0.3])</code></pre><h3 id="1-逻辑回归"><a href="#1-逻辑回归" class="headerlink" title="1. 逻辑回归"></a>1. 逻辑回归</h3><p>先用LogisticRegression构建逻辑回归模型，<br>用fit方法对训练集进行训练，<br>再用evaluate方法，将测试集应用到训练好模型，得到评价结果，<br>最后查看ROC曲线下面积</p><pre><code class="python">from pyspark.ml.classification import LogisticRegressionlr = LogisticRegression(featuresCol=&quot;features&quot;, labelCol=&quot;label&quot;)lrModel = lr.fit(train_data)lr_pred_results = lrModel.evaluate(test_data)lr_pred_results.areaUnderROC</code></pre><pre><code>0.9045632288306538</code></pre><h3 id="2-决策树"><a href="#2-决策树" class="headerlink" title="2. 决策树"></a>2. 决策树</h3><p>先用DecisionTreeClassifier构建决策树模型，<br>用fit方法对训练集进行训练，<br>以测试集作为输入，对训练得到的模型使用transform方法，得到测试结果，<br>选取其中的prediction，label两列，前者是模型预测，后者是真实值，</p><pre><code class="python">from pyspark.ml.classification import DecisionTreeClassifierdt = DecisionTreeClassifier(featuresCol=&quot;features&quot;, labelCol=&quot;label&quot;)dtModel = dt.fit(train_data)dt_transform_test = dtModel.transform(test_data).select(&quot;prediction&quot;, &quot;label&quot;)dt_transform_test.show(1)</code></pre><pre><code>+----------+-----+|prediction|label|+----------+-----+|       0.0|  0.0|+----------+-----+only showing top 1 row</code></pre><p>​    </p><p>由于其它分类模型目前没有实现evaluate方法，所以想得到指标得绕一绕</p><p>先用BinaryClassificationEvaluator构建二分类专用的评价器<br>以测试结果为输入，对评价器使用evaluate方法<br>加上{evaluator.metricName: “areaUnderROC”}参数，得到ROC曲线下面积</p><pre><code class="python">from pyspark.ml.evaluation import BinaryClassificationEvaluatordt_evaluator = BinaryClassificationEvaluator(labelCol = &quot;label&quot;, rawPredictionCol = &quot;prediction&quot; )dt_pred_results = dt_evaluator.evaluate(dt_transform_test, &#123;dt_evaluator.metricName: &quot;areaUnderROC&quot;&#125;)dt_pred_results</code></pre><pre><code>0.7276855987873679</code></pre><h3 id="3-随机森林"><a href="#3-随机森林" class="headerlink" title="3. 随机森林"></a>3. 随机森林</h3><p>使用RandomForestClassifier构建随机森林模型<br>numTrees&#x3D;10是模型中决策树的个数<br>其它步骤和2.决策树一样</p><pre><code class="python">from pyspark.ml.classification import RandomForestClassifierrf = RandomForestClassifier(labelCol = &quot;label&quot;, featuresCol = &quot;features&quot;, numTrees=10)rfModel = rf.fit(train_data)rf_transform_test = rfModel.transform(test_data).select(&quot;prediction&quot;, &quot;label&quot;)rf_transform_test.show(1)</code></pre><pre><code>+----------+-----+|prediction|label|+----------+-----+|       0.0|  0.0|+----------+-----+only showing top 1 row</code></pre><pre><code class="python">rf_evaluator = BinaryClassificationEvaluator(labelCol = &quot;label&quot;, rawPredictionCol = &quot;prediction&quot; )rf_pred_results = rf_evaluator.evaluate(rf_transform_test, &#123;rf_evaluator.metricName: &quot;areaUnderROC&quot;&#125;)rf_pred_results</code></pre><pre><code>0.6299789143912213</code></pre><h3 id="4-梯度提升树"><a href="#4-梯度提升树" class="headerlink" title="4. 梯度提升树"></a>4. 梯度提升树</h3><p>使用GBTClassifier构建梯度提升树模型，<br>maxIter&#x3D;10是最大迭代次数，<br>其它步骤和2.决策树一样</p><pre><code class="python">from pyspark.ml.classification import GBTClassifiergbt = GBTClassifier(labelCol = &quot;label&quot;, featuresCol = &quot;features&quot;, maxIter=10)gbtModel = gbt.fit(train_data)gbt_transform_test = gbtModel.transform(test_data).select(&quot;prediction&quot;, &quot;label&quot;)gbt_transform_test.show(1)</code></pre><pre><code>+----------+-----+|prediction|label|+----------+-----+|       0.0|  0.0|+----------+-----+only showing top 1 row</code></pre><p>​    </p><pre><code class="python">gbt_evaluator = BinaryClassificationEvaluator(labelCol = &quot;label&quot;, rawPredictionCol = &quot;prediction&quot; )gbt_pred_results = gbt_evaluator.evaluate(gbt_transform_test, &#123;gbt_evaluator.metricName: &quot;areaUnderROC&quot;&#125;)gbt_pred_results</code></pre><pre><code>0.7533512673325949</code></pre><h3 id="5-线性支持向量机"><a href="#5-线性支持向量机" class="headerlink" title="5. 线性支持向量机"></a>5. 线性支持向量机</h3><p>使用LinearSVCr构建支持向量机模型，<br>maxIter&#x3D;10是最大迭代次数，regParam&#x3D;0.1是正则项的值，<br>其它步骤和2.决策树一样</p><pre><code class="python">from pyspark.ml.classification import LinearSVClsvc = LinearSVC(maxIter=10, regParam=0.1)lsvcModel = lsvc.fit(train_data)lsvc_transform_test = lsvcModel.transform(test_data).select(&quot;prediction&quot;, &quot;label&quot;)lsvc_transform_test.show(1)</code></pre><pre><code>+----------+-----+|prediction|label|+----------+-----+|       0.0|  0.0|+----------+-----+only showing top 1 row</code></pre><p>​    </p><pre><code class="python">lsvc_evaluator = BinaryClassificationEvaluator(labelCol = &quot;label&quot;, rawPredictionCol = &quot;prediction&quot; )lsvc_pred_results = lsvc_evaluator.evaluate(lsvc_transform_test, &#123;lsvc_evaluator.metricName: &quot;areaUnderROC&quot;&#125;)lsvc_pred_results</code></pre><pre><code>0.722097945988789</code></pre><h3 id="6-模型比较"><a href="#6-模型比较" class="headerlink" title="6. 模型比较"></a>6. 模型比较</h3><p>下面用一个柱状图来直观感受以下刚才几个模型的ROC曲线下面积值</p><pre><code class="python">data = [lr_pred_results.areaUnderROC, dt_pred_results, rf_pred_results, gbt_pred_results, lsvc_pred_results]labels = [&#39;LR&#39;, &#39;DT&#39;, &#39;RF&#39;, &#39;GBT&#39;, &#39;LSVC&#39;]import matplotlib.pyplot as pltplt.bar(range(len(data)), data, tick_label=labels)plt.show()</code></pre><p><img src="https://s1.328888.xyz/2022/06/13/p2Zw1.png" alt="p2Zw1.png"></p><h2 id="四：总结"><a href="#四：总结" class="headerlink" title="四：总结"></a>四：总结</h2><p>Spark MLlib的使用方式总体上来说和sklearn比较类似，并且拥有分布式的处理能力。但是目前的模型还没有sklearn那么全。另外，比起sklearn，Spark MLlib用于分类模型训练的数据集需要打包成属性和标签两个列，多了一个打包的过程。如果大家感兴趣，可以到官方文档中，进一步学习。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark MLlib </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PySpark极速入门</title>
      <link href="/2022/06/12/PySpark%E6%9E%81%E9%80%9F%E5%85%A5%E9%97%A8/"/>
      <url>/2022/06/12/PySpark%E6%9E%81%E9%80%9F%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="PySpark极速入门"><a href="#PySpark极速入门" class="headerlink" title="PySpark极速入门"></a>PySpark极速入门</h1><h2 id="一：Pyspark简介与安装"><a href="#一：Pyspark简介与安装" class="headerlink" title="一：Pyspark简介与安装"></a>一：Pyspark简介与安装</h2><h4 id="什么是Pyspark？"><a href="#什么是Pyspark？" class="headerlink" title="什么是Pyspark？"></a>什么是Pyspark？</h4><p>PySpark是Spark的Python语言接口，通过它，可以使用Python API编写Spark应用程序，目前支持绝大多数Spark功能。目前Spark官方在其支持的所有语言中，将Python置于首位。</p><p><img src="https://s1.328888.xyz/2022/06/12/Co98B.png" alt="Co98B.png"></p><h4 id="如何安装？"><a href="#如何安装？" class="headerlink" title="如何安装？"></a>如何安装？</h4><p>在终端输入</p><pre><code class="shell">pip intsall pyspark</code></pre><p><img src="https://s1.328888.xyz/2022/06/12/CvNZA.png" alt="CvNZA.png"></p><p>或者使用pycharm，在GUI界面安装</p><p><img src="https://s1.328888.xyz/2022/06/12/CvMsm.png" alt="CvMsm.png"></p><h2 id="二：编程实践"><a href="#二：编程实践" class="headerlink" title="二：编程实践"></a>二：编程实践</h2><h3 id="加载、转换数据"><a href="#加载、转换数据" class="headerlink" title="加载、转换数据"></a>加载、转换数据</h3><pre><code class="python"># 导入pyspark# 导入pandas, 稍后与pyspark中的数据结构做对比import pysparkimport pandas as pd</code></pre><p>在编写spark程序前，我们要创建一个SparkSession对象</p><pre><code class="python">from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;Spark极速入门&quot;).getOrCreate()</code></pre><p>可以看到会话的一些信息：使用的Spark版本、运行模式、应用程序名字</p><p>演示环境用的是local本地模式， * 代表的是使用全部线程<br>如果想用集群模式的话，可以去查看集群搭建的相关教程<br>届时pyspark程序作为spark的客户端，设置连接集群，就是真正的分布式计算了<br>目前只是本地模式，用多线程去模拟分布式计算。</p><pre><code class="python">spark</code></pre><p><img src="https://s1.328888.xyz/2022/06/12/CoGE4.png" alt="CoGE4.png"></p><p>看看我们将用到的test1数据吧</p><p><img src="https://s1.328888.xyz/2022/06/12/CNziq.png" alt="CNziq.png"></p><p>使用read方法，用option设置是否读取csv的头，再指定路径就可以读取数据了</p><pre><code class="python">df_spark = spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(&quot;./data/test1.csv&quot;)</code></pre><p>看看是什么类型</p><pre><code class="python">type(df_spark)</code></pre><pre><code>pyspark.sql.dataframe.DataFrame</code></pre><p>再看看用pandas读取是什么类型</p><pre><code class="python">type(pd.read_csv(&quot;./data/test1.csv&quot;))</code></pre><pre><code>pandas.core.frame.DataFrame</code></pre><p>可以发现Spark读取这种结构化数据时，用的也是和pandas类似的dataframe结构<br>这也是Spark应用最广泛的数据结构</p><p>使用show方法打印数据</p><pre><code class="python">df_spark.show()</code></pre><pre><code>+---------+---+----------+------+|     Name|age|Experience|Salary|+---------+---+----------+------+|    Krish| 31|        10| 30000||Sudhanshu| 30|         8| 25000||    Sunny| 29|         4| 20000||     Paul| 24|         3| 20000||   Harsha| 21|         1| 15000||  Shubham| 23|         2| 18000|+---------+---+----------+------+</code></pre><p>​    </p><p>使用printSchema方法打印元数据信息，发现明明是数值类型的，它却读取为了字符串类型</p><pre><code class="python">df_spark.printSchema()</code></pre><pre><code>root |-- Name: string (nullable = true) |-- age: string (nullable = true) |-- Experience: string (nullable = true) |-- Salary: string (nullable = true)</code></pre><p>​    </p><p>在读取时，加上类型推断,发现此时已经能正确读取了</p><pre><code class="python">df_spark = spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(&quot;./data/test1.csv&quot;,inferSchema=True)df_spark.printSchema()</code></pre><pre><code>root |-- Name: string (nullable = true) |-- age: integer (nullable = true) |-- Experience: integer (nullable = true) |-- Salary: integer (nullable = true)</code></pre><p>​    </p><p>选择某些列, 可以发现不管选多列还是选单列，返回的都是dataframe<br>返回的也同样可以printSchema、show等dataframe使用的方法，做到了结构的统一</p><pre><code class="python">df_spark.select([&quot;Name&quot;, &quot;age&quot;])</code></pre><pre><code>DataFrame[Name: string, age: int]</code></pre><pre><code class="python">df_spark.select(&quot;Name&quot;)</code></pre><pre><code>DataFrame[Name: string]</code></pre><pre><code class="python">df_spark.select([&quot;Name&quot;, &quot;age&quot;, &quot;Salary&quot;]).printSchema()</code></pre><pre><code>root |-- Name: string (nullable = true) |-- age: integer (nullable = true) |-- Salary: integer (nullable = true)</code></pre><p>​    </p><p>不用select，而用[]直接选取，就有点类似与pandas的series了</p><pre><code class="python">df_spark[&quot;Name&quot;]</code></pre><pre><code>Column&lt;&#39;Name&#39;&gt;</code></pre><p>column就不能直接show了</p><pre><code class="python">df_spark[&quot;age&quot;].show()</code></pre><pre><code>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)Input In [15], in &lt;cell line: 1&gt;()----&gt; 1 df_spark[&quot;age&quot;].show()TypeError: &#39;Column&#39; object is not callable</code></pre><p>用describe方法可以对dataframe做一些简单的统计</p><pre><code class="python">df_spark.describe().show()</code></pre><pre><code>+-------+------+------------------+-----------------+------------------+|summary|  Name|               age|       Experience|            Salary|+-------+------+------------------+-----------------+------------------+|  count|     6|                 6|                6|                 6||   mean|  null|26.333333333333332|4.666666666666667|21333.333333333332|| stddev|  null| 4.179314138308661|3.559026084010437| 5354.126134736337||    min|Harsha|                21|                1|             15000||    max| Sunny|                31|               10|             30000|+-------+------+------------------+-----------------+------------------+</code></pre><p>​    </p><p>用withColumn方法给dataframe加上一列</p><pre><code class="python">df_spark = df_spark.withColumn(&quot;Experience After 3 year&quot;, df_spark[&quot;Experience&quot;] + 3)</code></pre><pre><code class="python">df_spark.show()</code></pre><pre><code>+---------+---+----------+------+-----------------------+|     Name|age|Experience|Salary|Experience After 3 year|+---------+---+----------+------+-----------------------+|    Krish| 31|        10| 30000|                     13||Sudhanshu| 30|         8| 25000|                     11||    Sunny| 29|         4| 20000|                      7||     Paul| 24|         3| 20000|                      6||   Harsha| 21|         1| 15000|                      4||  Shubham| 23|         2| 18000|                      5|+---------+---+----------+------+-----------------------+</code></pre><p>​    </p><p>用drop方法删除列</p><pre><code class="python">df_spark = df_spark.drop(&quot;Experience After 3 year&quot;)df_spark.show()</code></pre><pre><code>+---------+---+----------+------+|     Name|age|Experience|Salary|+---------+---+----------+------+|    Krish| 31|        10| 30000||Sudhanshu| 30|         8| 25000||    Sunny| 29|         4| 20000||     Paul| 24|         3| 20000||   Harsha| 21|         1| 15000||  Shubham| 23|         2| 18000|+---------+---+----------+------+</code></pre><p>​    </p><p>用withColumnRename方法重命名列</p><pre><code class="python">df_spark.withColumnRenamed(&quot;Name&quot;, &quot;New Name&quot;).show()</code></pre><pre><code>+---------+---+----------+------+| New Name|age|Experience|Salary|+---------+---+----------+------+|    Krish| 31|        10| 30000||Sudhanshu| 30|         8| 25000||    Sunny| 29|         4| 20000||     Paul| 24|         3| 20000||   Harsha| 21|         1| 15000||  Shubham| 23|         2| 18000|+---------+---+----------+------+</code></pre><p>​    </p><h3 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h3><p>看看接下来要带缺失值的test2数据吧</p><p><img src="https://s1.328888.xyz/2022/06/12/CSeoe.png" alt="CSeoe.png"></p><pre><code class="python">df_spark = spark.read.csv(&quot;./data/test2.csv&quot;, header=True, inferSchema=True)df_spark.show()</code></pre><pre><code>+---------+----+----------+------+|     Name| age|Experience|Salary|+---------+----+----------+------+|    Krish|  31|        10| 30000||Sudhanshu|  30|         8| 25000||    Sunny|  29|         4| 20000||     Paul|  24|         3| 20000||   Harsha|  21|         1| 15000||  Shubham|  23|         2| 18000||   Mahesh|null|      null| 40000||     null|  34|        10| 38000||     null|  36|      null|  null|+---------+----+----------+------+</code></pre><p>​    </p><p>用na.drop删除缺失值<br>how参数设置策略，any意思是只要一行里有缺失值，那就删了<br>any也是how的默认参数</p><pre><code class="python">df_spark.na.drop(how=&quot;any&quot;).show()</code></pre><pre><code>+---------+---+----------+------+|     Name|age|Experience|Salary|+---------+---+----------+------+|    Krish| 31|        10| 30000||Sudhanshu| 30|         8| 25000||    Sunny| 29|         4| 20000||     Paul| 24|         3| 20000||   Harsha| 21|         1| 15000||  Shubham| 23|         2| 18000|+---------+---+----------+------+</code></pre><p>​    </p><p>可以通过thresh参数设置阈值，代表超过一行中缺失值的数量超过这个值，才会被删除</p><pre><code class="python">df_spark.na.drop(how=&quot;any&quot;, thresh=2).show()</code></pre><pre><code>+---------+----+----------+------+|     Name| age|Experience|Salary|+---------+----+----------+------+|    Krish|  31|        10| 30000||Sudhanshu|  30|         8| 25000||    Sunny|  29|         4| 20000||     Paul|  24|         3| 20000||   Harsha|  21|         1| 15000||  Shubham|  23|         2| 18000||   Mahesh|null|      null| 40000||     null|  34|        10| 38000|+---------+----+----------+------+</code></pre><p>​    </p><p>也可以用subset参数设置关注的列<br>下面代码意思是，在Experience列中，只要有缺失值就删掉</p><pre><code class="python">df_spark.na.drop(how=&quot;any&quot;, subset=[&quot;Experience&quot;]).show()</code></pre><pre><code>+---------+---+----------+------+|     Name|age|Experience|Salary|+---------+---+----------+------+|    Krish| 31|        10| 30000||Sudhanshu| 30|         8| 25000||    Sunny| 29|         4| 20000||     Paul| 24|         3| 20000||   Harsha| 21|         1| 15000||  Shubham| 23|         2| 18000||     null| 34|        10| 38000|+---------+---+----------+------+</code></pre><p>​    </p><p>用fillna填充缺失值, 可以用字典对各列的填充值进行设置</p><pre><code class="python">df_spark.fillna(&#123;&#39;Name&#39;: &#39;unknown&#39;, &#39;age&#39;: 18, &#39;Experience&#39;: 0, &#39;Salary&#39;: 0&#125;).show()</code></pre><pre><code>+---------+---+----------+------+|     Name|age|Experience|Salary|+---------+---+----------+------+|    Krish| 31|        10| 30000||Sudhanshu| 30|         8| 25000||    Sunny| 29|         4| 20000||     Paul| 24|         3| 20000||   Harsha| 21|         1| 15000||  Shubham| 23|         2| 18000||   Mahesh| 18|         0| 40000||  unknown| 34|        10| 38000||  unknown| 36|         0|     0|+---------+---+----------+------+</code></pre><p>​    </p><p>还可以调用机器学习模块的相关方法，<br>通过设置策略，可以用平均数、众数等方式填充</p><pre><code class="python">from pyspark.ml.feature import Imputerimputer = Imputer(    inputCols = [&#39;age&#39;, &#39;Experience&#39;, &#39;Salary&#39;],    outputCols = [f&quot;&#123;c&#125;_imputed&quot; for c in [&#39;age&#39;, &#39;Experience&#39;, &#39;Salary&#39;]]).setStrategy(&quot;mean&quot;)</code></pre><pre><code class="python">imputer.fit(df_spark).transform(df_spark).show()</code></pre><pre><code>+---------+----+----------+------+-----------+------------------+--------------+|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|+---------+----+----------+------+-----------+------------------+--------------+|    Krish|  31|        10| 30000|         31|                10|         30000||Sudhanshu|  30|         8| 25000|         30|                 8|         25000||    Sunny|  29|         4| 20000|         29|                 4|         20000||     Paul|  24|         3| 20000|         24|                 3|         20000||   Harsha|  21|         1| 15000|         21|                 1|         15000||  Shubham|  23|         2| 18000|         23|                 2|         18000||   Mahesh|null|      null| 40000|         28|                 5|         40000||     null|  34|        10| 38000|         34|                10|         38000||     null|  36|      null|  null|         36|                 5|         25750|+---------+----+----------+------+-----------+------------------+--------------+</code></pre><p>​    </p><h3 id="过滤操作"><a href="#过滤操作" class="headerlink" title="过滤操作"></a>过滤操作</h3><p>还是切换到test1数据</p><pre><code class="python">df_spark = spark.read.csv(&quot;./data/test1.csv&quot;, header=True, inferSchema=True)df_spark.show()</code></pre><pre><code>+---------+---+----------+------+|     Name|age|Experience|Salary|+---------+---+----------+------+|    Krish| 31|        10| 30000||Sudhanshu| 30|         8| 25000||    Sunny| 29|         4| 20000||     Paul| 24|         3| 20000||   Harsha| 21|         1| 15000||  Shubham| 23|         2| 18000|+---------+---+----------+------+</code></pre><p>​    </p><p>可以使用filter方法对数据进行过滤操作，类似于SQL中的where<br>可以使用字符串的方式，也可以利用column方式去传递条件</p><pre><code class="python">df_spark.filter(&quot;Salary &lt;= 20000&quot;).show()</code></pre><pre><code>+-------+---+----------+------+|   Name|age|Experience|Salary|+-------+---+----------+------+|  Sunny| 29|         4| 20000||   Paul| 24|         3| 20000|| Harsha| 21|         1| 15000||Shubham| 23|         2| 18000|+-------+---+----------+------+</code></pre><p>​    </p><pre><code class="python">df_spark.filter(df_spark[&quot;Salary&quot;]&lt;=20000).show()</code></pre><pre><code>+-------+---+----------+------+|   Name|age|Experience|Salary|+-------+---+----------+------+|  Sunny| 29|         4| 20000||   Paul| 24|         3| 20000|| Harsha| 21|         1| 15000||Shubham| 23|         2| 18000|+-------+---+----------+------+</code></pre><p>​    </p><p>如果是字符串，用 and 表示同时满足多个条件<br>如果是用column，用( &amp; ) 连接多个条件</p><pre><code class="python">df_spark.filter(&quot;Salary &lt;= 20000 and age &lt;= 24&quot;).show()</code></pre><pre><code>+-------+---+----------+------+|   Name|age|Experience|Salary|+-------+---+----------+------+|   Paul| 24|         3| 20000|| Harsha| 21|         1| 15000||Shubham| 23|         2| 18000|+-------+---+----------+------+</code></pre><p>​    </p><pre><code class="python">df_spark.filter(    (df_spark[&quot;Salary&quot;]&lt;=20000)    &amp; (df_spark[&quot;age&quot;]&lt;=24)).show()</code></pre><pre><code>+-------+---+----------+------+|   Name|age|Experience|Salary|+-------+---+----------+------+|   Paul| 24|         3| 20000|| Harsha| 21|         1| 15000||Shubham| 23|         2| 18000|+-------+---+----------+------+</code></pre><p>​    </p><pre><code class="python">column中，用|表示或， ~表示取反</code></pre><pre><code class="python">df_spark.filter(    (df_spark[&quot;Salary&quot;]&lt;=20000)    | (df_spark[&quot;age&quot;]&lt;=24)).show()</code></pre><pre><code>+-------+---+----------+------+|   Name|age|Experience|Salary|+-------+---+----------+------+|  Sunny| 29|         4| 20000||   Paul| 24|         3| 20000|| Harsha| 21|         1| 15000||Shubham| 23|         2| 18000|+-------+---+----------+------+</code></pre><p>​    </p><pre><code class="python">df_spark.filter(    (df_spark[&quot;Salary&quot;]&lt;=20000)    | ~(df_spark[&quot;age&quot;]&lt;=24)).show()</code></pre><pre><code>+---------+---+----------+------+|     Name|age|Experience|Salary|+---------+---+----------+------+|    Krish| 31|        10| 30000||Sudhanshu| 30|         8| 25000||    Sunny| 29|         4| 20000||     Paul| 24|         3| 20000||   Harsha| 21|         1| 15000||  Shubham| 23|         2| 18000|+---------+---+----------+------+</code></pre><p>​    </p><h3 id="分组聚合"><a href="#分组聚合" class="headerlink" title="分组聚合"></a>分组聚合</h3><p>换一个数据集test3</p><p><img src="https://s1.328888.xyz/2022/06/12/CS63i.png" alt="CS63i.png"></p><pre><code class="python">df_spark = spark.read.csv(&quot;./data/test3.csv&quot;, header=True, inferSchema=True)df_spark.show()</code></pre><pre><code>+---------+------------+------+|     Name| Departments|salary|+---------+------------+------+|    Krish|Data Science| 10000||    Krish|         IOT|  5000||   Mahesh|    Big Data|  4000||    Krish|    Big Data|  4000||   Mahesh|Data Science|  3000||Sudhanshu|Data Science| 20000||Sudhanshu|         IOT| 10000||Sudhanshu|    Big Data|  5000||    Sunny|Data Science| 10000||    Sunny|    Big Data|  2000|+---------+------------+------+</code></pre><p>​    </p><p>使用groupby方法对dataframe某些列进行分组</p><pre><code class="python">df_spark.groupBy(&quot;Name&quot;)</code></pre><pre><code>&lt;pyspark.sql.group.GroupedData at 0x227454d4be0&gt;</code></pre><p>可以看到分组的结果是GroupedData对象，它不能使用show等方法打印<br>GroupedData对象需要进行聚合操作，才能重新转换为dataframe<br>聚合函数有sum、count、avg、max、min等</p><pre><code class="python">df_spark.groupBy(&quot;Departments&quot;).sum().show()</code></pre><pre><code>+------------+-----------+| Departments|sum(salary)|+------------+-----------+|         IOT|      15000||    Big Data|      15000||Data Science|      43000|+------------+-----------+</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速选择学习笔记</title>
      <link href="/2022/06/11/%E5%BF%AB%E9%80%9F%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/06/11/%E5%BF%AB%E9%80%9F%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="快速选择算法学习笔记"><a href="#快速选择算法学习笔记" class="headerlink" title="快速选择算法学习笔记"></a>快速选择算法学习笔记</h1><h5 id="找到第K小的数"><a href="#找到第K小的数" class="headerlink" title="找到第K小的数"></a>找到第K小的数</h5><h3 id="一：思想"><a href="#一：思想" class="headerlink" title="一：思想"></a>一：思想</h3><p>和快排的思想类似，先在中点找到一个基准值pivot，把 &lt;&#x3D; pivot的移到数组左边，把 &gt;&#x3D; pivot的移到右边。把数组分成两段，左边一段都是比较小的，右边一段都是比较大的。</p><p>接下来，判断一下目标值在左边还是右边，如果在左边就去递归左边，右边就递归右边，递归中止条件是只有一个数。</p><h3 id="二：-代码实现"><a href="#二：-代码实现" class="headerlink" title="二： 代码实现"></a>二： 代码实现</h3><pre><code class="java">public class Solution &#123;    public int kthSmallestElement(int k, int[] nums) &#123;        if(nums == null) &#123;            return -1;        &#125;        return quickSelect(nums, 0, nums.length - 1, k);    &#125;        private int quickSelect(int[] nums, int start, int end, int k) &#123;        if(start == end) &#123;            return nums[start];        &#125;                int left = start, right = end;        int pivot = nums[(left + right) / 2];                while(left &lt;= right) &#123;            while(left &lt;= right &amp;&amp; nums[left] &lt; pivot) &#123;                left++;            &#125;            while(left &lt;= right &amp;&amp; nums[r] &gt; pivot) &#123;                right--;            &#125;            if(left &lt;= right) &#123;                int temp = nums[left];                num[left] = nums[right];                nums[right] = nums[left];                left++;                right--;            &#125;        &#125;        # 此时两种情况：[..][...] 和[..]num[..]                    # right前的都 &lt;= pivot，left后都 &gt;= pivot，如果有num，num == pivot                # start + k - 1是从start到end这段区间中，第k小的元素经排序后的下标                # 如果排序后的下标没超过左边界 right, 就说明目标逃不出[start: right+1]        # 化归为[start: right+1]区间中找第k小元素        if(start + k - 1 &lt;= right) &#123;            return quickSelect(nums, start, right, k);        &#125;        # 如果排序后的下标超过了右边界，说明目标在[left: end+1]                       # 此时化归为[left: end+1]区间中找第k - (left - start)小元素的问题        # 因为把左边的元素全扔了，左边的元素一共(left - start)个        if(start + k - 1 &gt;= left) &#123;            return quickSelect(nums, left, end, k - (left - start));        &#125;        如果排序的下标不属于上面的情况，那么意味着正中num        return nums[right + 1];    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 排序算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>归并排序学习笔记</title>
      <link href="/2022/06/10/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/06/10/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="归并排序学习笔记"><a href="#归并排序学习笔记" class="headerlink" title="归并排序学习笔记"></a>归并排序学习笔记</h1><h3 id="一：思想"><a href="#一：思想" class="headerlink" title="一：思想"></a>一：思想</h3><p>把一个数组对半分，左右两边递归处理，直到区域只剩下一个元素。</p><p>区域只剩下一个元素了，必然是有序的，接下来就是回溯递归栈，完成有序数组的合并。</p><h3 id="二：代码实现"><a href="#二：代码实现" class="headerlink" title="二：代码实现"></a>二：代码实现</h3><pre><code class="java">public class Solution &#123;    public void sortIntegers(int[] A) &#123;        if(A == null || A.length == 0) return;                int[] temp = new int[A.length];                mergeSort(A, 0, A.length - 1, temp);    &#125;        # 类似二叉树的后序遍历    private void mergeSort(int[] A, int start, int end, int[] temp) &#123;        if(start &gt;= end) return;        mergeSort(A, start, (start + end) / 2, temp);        mergeSort(A, (start + end) / 2 + 1, end, temp);        merge(A, start, end, temp);        # 完成merge后，就代表着[start: end+1]已经有序了，返回到上一层递归中    &#125;        # 对两段有序数组进行合并    private void merge(int[] A, int start, int end, int[] temp) &#123;        int middle = (start + end) / 2;        int leftIndex = start, rightIndex = middle + 1;                int index = leftIndex;        while(leftIndex &lt;= middle &amp;&amp; rightIndex &lt;= end) &#123;            if(A[leftIndex] &lt; A[rightIndex]) &#123;                temp[index++] = A[leftIndex++];            &#125;            else temp[index++] = A[rightIndex++];        &#125;                while(leftIndex &lt;= middle) temp[index++] = A[leftIndex++];        while(rightIndex &lt;= end) temp[index++] = A[rightIndex++];                # temp作为临时存储空间，用于放置合并有序数组过程的中间结果        # 最后，temp存放着[start: end+1]有序数组，反过来赋值给A，完成原地排序。        for(int i = start; i &lt;= end; i++) A[i] = temp[i];            &#125;&#125;</code></pre><h3 id="三：注意事项"><a href="#三：注意事项" class="headerlink" title="三：注意事项"></a>三：注意事项</h3><ol><li>temp数组的申请内存操作要放在最开始，然后作为参数传到递归函数内。</li></ol><h3 id="四：与快速排序的比较"><a href="#四：与快速排序的比较" class="headerlink" title="四：与快速排序的比较"></a>四：与快速排序的比较</h3><p>时间复杂度上：快排是不严格的O(NlogN)，而归并是严格的O(NlogN)。原因是快排的过程中子问题的规模并不是完全对半分，具体的规模划分依赖于基准值的选取。子问题规模有可能是[…….] 和 [..]，不均匀。而归并则是严格的对半分。</p><p>空间复杂度上：快排O(1)，归并O(N)。</p><p>稳定性上：快排非稳定，归并稳定。</p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 排序算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速排序学习笔记</title>
      <link href="/2022/06/10/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/06/10/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="快速排序学习笔记"><a href="#快速排序学习笔记" class="headerlink" title="快速排序学习笔记"></a>快速排序学习笔记</h1><h3 id="一：思想"><a href="#一：思想" class="headerlink" title="一：思想"></a>一：思想</h3><p>在数组中随机挑出一个数，作为基准值p。</p><p>用一个<strong>分组函数</strong>将数组中**&lt;&#x3D;<strong>基准值p的元素挪到基准值的左半部分，**&gt;&#x3D;</strong> 基准值p的元素挪到右半部分。</p><p>在左半部分和右半部分递归上述过程。</p><p><strong>重点：理解为什么是&lt;&#x3D;和&gt;&#x3D;，为什么取等</strong></p><p>答：因为当出现数组中很多元素都是一样的情况，举个极端例子[1,1,…..1,1]。</p><p>如果是&lt;和&gt;&#x3D;，或者是&lt;&#x3D;和&gt;，那么此时就会出现一边没有没有元素的情况，不平衡。</p><h3 id="二：代码实现"><a href="#二：代码实现" class="headerlink" title="二：代码实现"></a>二：代码实现</h3><pre><code class="java">public class Solution &#123;        private void quickSort(int[] A, int start, int end) &#123;        if (start &gt;= end) return;                int left = start, right = end;                int pivot = A[(start + end) / 2];                while(left &lt;= right) &#123;            # 从左往右扫，直到找到一个不应该在左边的数            while(left &lt;= right &amp;&amp; A[left] &lt; pivot) &#123;                left++;            &#125;            # 从右往左扫，直到找到一个不应该在右边的数            while(left &lt;= right &amp;&amp; A[right] &gt; pivot) &#123;                right--;            &#125;            if(left &lt;= right) &#123;                int temp = A[left];                A[left] = A[right];                A[right] = temp;                left++;                right--;            &#125;        &#125;        # 到这里，说明两种情况        # [....] [.....]        # [...] num [....]   此时这个num已经在其w        quickSort(A, start, right);        quickSort(A, left, end);               &#125;        public void sortIntegers(int[] A) &#123;        if (A == null || A.length == 0) return;        quickSort(A, 0, A.length - 1)    &#125;&#125;    </code></pre><p><strong>重点1： 理解为什么是 left &lt;&#x3D; right， 而不是 &lt;</strong> </p><ol><li><p>首先保证程序的统一， while和if部分的 &lt;&#x3D; 或 &lt; 要保持一致，不能while写 &lt;&#x3D;, if又写 &lt;。要么全部 &lt;&#x3D;，要么全部 &lt; 。</p></li><li><p>如果while部分改成了 &lt; ，那么退出while的时候，left &#x3D;&#x3D; right，进入下一层递归时，就会出现quickSort(A, start, right), quickSort(A, left, end)的情况，也就是重合了，A[left]这个元素会参与两边的排序，出现了重复。如果是 &lt;&#x3D; 那么退出循环时，left &gt; right, 不会出现重复。</p></li><li><p>如果if部分改成了 &lt; ，那么上面的while退出时，left &#x3D;&#x3D; right，不进入if，直接进入下一层递归。</p><p>假设对[1,2]进行快排，那么pivot &#x3D; 1，最后A[left] &#x3D;&#x3D; 1，A[right] &#x3D;&#x3D; 1, left &#x3D;&#x3D; right，不进入if，直接下一层递归。其中quickSort(A, start, right) 中 start &#x3D;&#x3D; right，能正常退出。但是进入quickSort(A, , left， end)时，处理的还是[1,2]，这将会导致无限循环，栈会溢出。</p></li></ol><p><strong>重点2：理解为什么 A[left] &lt; pivot   A[right] &gt; pivot   而不是 &lt;&#x3D; 或 &gt;&#x3D;</strong> </p><p>​如果带了&#x3D;，遇到[1,1…,1,1]这种情况，那么left循环后会 &gt; right, 退出循环，转向下一层递归。但此时right都没动过，下层递归quickSort(A, start, right)遇到的还是[1,1…,1,1]，规模没有缩小，无限循环，栈溢出。</p><p><strong>总结：和基准值作比较，不带 &#x3D; ，流程控制结构带 &#x3D; （while、if）用 &lt;&#x3D;</strong></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 排序算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>支持向量机——从方程到代码</title>
      <link href="/2022/06/07/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E2%80%94%E2%80%94%E4%BB%8E%E6%96%B9%E7%A8%8B%E5%88%B0%E4%BB%A3%E7%A0%81/"/>
      <url>/2022/06/07/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E2%80%94%E2%80%94%E4%BB%8E%E6%96%B9%E7%A8%8B%E5%88%B0%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h2 id="写在前面："><a href="#写在前面：" class="headerlink" title="写在前面："></a>写在前面：</h2><p>  最近《机器学习》课程即将迎来期末考，就想借此机会把机器学习相关的知识系统梳理一遍。具体想法是将学过的算法自己推导一遍，然后将数学结论翻译为代码。</p><p>  在此过程中，借鉴<strong>scikit-learn</strong>的api设计，仿照其形式实现<strong>mykit-learn</strong>。全程只用<strong>numpy</strong>以及python自带的数据结构完成所有内容（在支持向量机部分还会用到<strong>cvxopt</strong>，用以求解二次优化问题）。</p><p>  mykit-learn现在的代码结构分为：decomposition降维、cluster聚类、classification分类、Regression回归以及public公共。</p><p><img src="https://s1.328888.xyz/2022/06/07/zTITT.png" alt="zTITT.png"></p><p>  目前，降维部分有MDS、PCA以及KernelPCA；聚类部分有KMeans以及KMeans++；分类有Softmax回归、逻辑回归、决策树、K近邻以及SVC（支持向量机分类）；回归有线性回归。public部分则是存放一些公用模块的地方，例如核函数。（目前在PCA和SVC中运用，共有五种）</p><p>  接下来将本在博客不定期更新<strong>mykit-learn</strong>的内容，今天就从我认为最“美”的算法——支持向量机开始！</p><h2 id="一：算法思想"><a href="#一：算法思想" class="headerlink" title="一：算法思想"></a>一：算法思想</h2><p><img src="https://s1.328888.xyz/2022/06/07/zTc07.jpg" alt="zTc07.jpg"></p><p>说白了就是学习一个m-1维度的超平面，将两类样本分开</p><p>以二维空间的例子来理解一下</p><p>负类的样本就在红色区域内，正类的样本就在绿色区域内，以决策超平面为分界</p><p><img src="https://s1.328888.xyz/2022/06/07/zTmVX.png" alt="zTmVX.png"></p><h2 id="二：数学推导"><a href="#二：数学推导" class="headerlink" title="二：数学推导"></a>二：数学推导</h2><p>为了更好的分类，容错率更大，我们就希望间隔尽量大</p><p><img src="https://s1.328888.xyz/2022/06/07/zT3GZ.png" alt="zT3GZ.png"></p><p>与此同时，我们关注一下约束条件</p><p><img src="https://s1.328888.xyz/2022/06/07/zTKtC.png" alt="zTKtC.png"></p><p>加上约束条件后，再看看最终后的优化目标</p><p><img src="https://s1.328888.xyz/2022/06/07/zTax1.png" alt="zTax1.png"></p><p>运用拉格朗日方程优化</p><p><img src="https://s1.328888.xyz/2022/06/07/zTOag.png" alt="zTOag.png"></p><p><img src="https://s1.328888.xyz/2022/06/07/zTn4t.png" alt="zTn4t.png"></p><p>最终可以推出如下结论</p><p><img src="https://s1.328888.xyz/2022/06/07/zT1ce.png" alt="zT1ce.png"></p><p>也就是传说中的KKT条件</p><h2 id="三：对偶问题"><a href="#三：对偶问题" class="headerlink" title="三：对偶问题"></a>三：对偶问题</h2><p>为了进一步简化问题，引入原优化问题的对偶问题</p><p><img src="https://s1.328888.xyz/2022/06/07/zTbNO.png" alt="zTbNO.png"></p><p><img src="https://s1.328888.xyz/2022/06/07/zTViq.png" alt="zTViq.png"></p><p>由于原问题和对偶问题同解，所以我们可以通过求解对偶问题来获得原问题的解</p><p><img src="https://s1.328888.xyz/2022/06/07/zTViq.png" alt="zTViq.png"></p><p>将之前求得的KKT条件代入对偶问题的方程中，化简得到如下式子</p><p><img src="https://s1.328888.xyz/2022/06/07/zTjVm.png" alt="zTjVm.png"></p><p>结合KKT条件，问题最终化归为求解带等式及不等式约束的二次凸优化问题</p><p><img src="https://s1.328888.xyz/2022/06/07/zT6gA.png" alt="zT6gA.png"></p><p>例如：</p><p><img src="https://s1.328888.xyz/2022/06/08/zc09m.png" alt="zc09m.png"></p><p><img src="https://s1.328888.xyz/2022/06/08/zcsWA.jpg" alt="zcsWA.jpg"></p><p>这类问题，我稍后将用cvxopt来求解</p><h2 id="四：核技巧"><a href="#四：核技巧" class="headerlink" title="四：核技巧"></a>四：核技巧</h2><p>如果样本点在原来的空间中，不能用决策超平面分开</p><p><img src="https://s1.328888.xyz/2022/06/07/zTMtS.png" alt="zTMtS.png"></p><p><img src="https://s1.328888.xyz/2022/06/08/zcRUR.png" alt="zcRUR.png"></p><p>那么我们就将原来的样本点升到更高的维度中，在高维空间中决策超平面就可以将它们分开</p><p><img src="https://s1.328888.xyz/2022/06/07/zTPSJ.png" alt="zTPSJ.png"></p><p>但是吧，这个升维的函数很难找啊，就算找到了，在计算的过程也要付出存储高维数据的代价，所以就用核函数代替。</p><p><img src="https://s1.328888.xyz/2022/06/07/zcQiF.png" alt="zcQiF.png"></p><p>核函数其实就是高维空间的点积</p><p><img src="https://s1.328888.xyz/2022/06/07/zTNaR.png" alt="zTNaR.png"></p><p>例如：</p><p><img src="https://s1.328888.xyz/2022/06/07/zToQi.png" alt="zToQi.png"></p><p>寄希望于维度转换函数是不可靠的，升六维时，转换函数还有表达式，但无穷维呢？如何表达呢？</p><p>例如：</p><p><img src="https://s1.328888.xyz/2022/06/07/zTB4v.png" alt="zTB4v.png"></p><p>但是，高斯核函数却可以表示无穷维（本质上是指数函数可以用泰勒公式展开成无穷项）</p><p><img src="https://s1.328888.xyz/2022/06/07/zTEc0.png" alt="zTEc0.png"></p><p>说了那么多，让我们看看具体的程序代码吧</p><h2 id="五：编程实践"><a href="#五：编程实践" class="headerlink" title="五：编程实践"></a>五：编程实践</h2><p> 依赖引入</p><pre><code class="python">import numpy as npfrom cvxopt import matrix, solversfrom numpy import ndarrayfrom public.Kernel import Kernel# cvxopt设置解的精度，求解过程显示与否solvers.options[&#39;abstol&#39;] = 1e-12solvers.options[&#39;reltol&#39;] = 1e-10solvers.options[&#39;feastol&#39;] = 1e-10solvers.options[&#39;show_progress&#39;] = False</code></pre><p>核函数模块</p><pre><code class="python">import numpy as npfrom numpy import ndarrayclass Kernel:    def __init__(            self,            sigma: float = 1.0,            alpha: float = 1.0,            c: float = 1.0,            d: float = 1.0,            beta: float = 1.0,            theta: float = 1.0,    ):        self.theta = theta        self.beta = beta        self.d = d        self.c = c        self.alpha = alpha        self.sigma = sigma    def gauss(self, x: ndarray, y: ndarray) -&gt; float:        return np.exp(-np.sum((x - y) ** 2) / 2 * self.sigma ** 2)    def linear(self, x: ndarray, y: ndarray) -&gt; float:        return self.alpha * np.sum(x * y) + self.c    def poly(self, x: ndarray, y: ndarray) -&gt; float:        return self.linear(x, y) ** self.d    def sigmoid(self, x: ndarray, y: ndarray) -&gt; float:        return np.tanh(self.beta * np.sum(x * y) + self.theta)    def laplace(self, x: ndarray, y: ndarray) -&gt; float:        return np.exp(-np.sum(np.abs(x - y)) / self.sigma)</code></pre><p>SVC类的初始化函数</p><p>x: 训练数据 y: 标签 kernel: 核函数 w: 决策超平面的斜率数组 b: 决策超平面的截距 sigma: 高斯核与拉普拉斯核的参数 alpha: 线性核 多项式核的参数 c: 线性核 多项式核的参数 d: 多项式核的参数 beta: sigmoid的参数 theta: sigmoid参数</p><pre><code class="python">class SVC:    def __init__(            self,            kernel: str = None,            sigma: float = 1.0,            alpha: float = 1.0,            c: float = 1.0,            d: float = 1.0,            beta: float = 1.0,            theta: float = 1.0,    ):        self.w = None        self.b = None        self.kernel = kernel        self.theta = theta        self.beta = beta        self.d = d        self.c = c        self.alpha = alpha        self.sigma = sigma        self.support_vector = None</code></pre><p>计算参数的函数</p><p>这些参数包括了求解二次优化相关的参数</p><p>其中的P,q,G,h,A,b的含义可以参考下图</p><p><img src="https://s1.328888.xyz/2022/06/08/zcaJy.png" alt="zcaJy.png"></p><pre><code class="python">    def __calculate_parameter(self, x: ndarray, y: ndarray):        kernel = Kernel(sigma=self.sigma, alpha=self.alpha, c=self.c, d=self.d, beta=self.beta, theta=self.theta)        kernel_func = &#123;            &quot;linear&quot;: kernel.linear,            &quot;poly&quot;: kernel.poly,            &quot;sigmoid&quot;: kernel.sigmoid,            &quot;laplace&quot;: kernel.laplace,            &quot;gauss&quot;: kernel.gauss,            None: np.dot        &#125;        m, _ = np.shape(x)        P = np.zeros((m, m))        for i in range(m):            for j in range(m):                P[i, j] = kernel_func.get(self.kernel, None)(x[i], x[j]) * y[i] * y[j]        P = matrix(P)        q = matrix(-np.ones(m))        G = matrix(np.diag(-np.ones(m)))        h = matrix(np.zeros(m))        A = matrix(y, (1, m), tc=&#39;d&#39;)        b = matrix([0.0])        res = solvers.qp(P, q, G, h, A, b)        λ = np.around(np.array(res[&#39;x&#39;]).flatten(), 2)        self.w = np.array(reduce(lambda a, b: a + b, map(lambda item: item[0] * item[1] * item[2], zip(λ, y, x))))        self.__support_index = [index for index, data in enumerate(λ) if data != 0]</code></pre><p>训练函数</p><p>输入x（m*n）数据，y（1 * m）标签 （+1&#x2F;-1）</p><p>训练得到决策超平面的斜率和截距</p><pre><code class="python">    def fit(self, x: ndarray, y: ndarray):        self.__calculate_parameter(x, y)        self.support_vector = x[self.__support_index]        self.b = 1 / y[self.__support_index[0]] - np.dot(x[self.__support_index[0]], self.w)</code></pre><p>预测函数</p><p>输入：待预测的数据</p><p>输出：标签  +1或-1</p><pre><code class="python">    def predict(self, data: ndarray):        return 1 if np.dot(data, self.w) + self.b &gt;= 0 else -1</code></pre><h2 id="六：计算检验"><a href="#六：计算检验" class="headerlink" title="六：计算检验"></a>六：计算检验</h2><p>以老师PPT中的题目为例</p><p><img src="https://s1.328888.xyz/2022/06/08/zcbeQ.jpg" alt="zcbeQ.jpg"></p><p><img src="https://s1.328888.xyz/2022/06/08/zcVw4.jpg" alt="zcVw4.jpg"></p><pre><code class="python">if __name__ == &quot;__main__&quot;:    svc = SVC()    svc.fit(np.array([[3, 3], [4, 3], [1, 1]]), np.array([1, 1, -1]))    print(&quot;决策超平面的斜率是：&quot;, svc.w)    print(&quot;决策超平面的截距为：&quot;, svc.b)    print(&quot;支持向量是：\n&quot;, svc.support_vector)    print(&quot;预测 (0.5, 0.5)的类别：&quot;, svc.predict(np.array([0.5, 0.5])))</code></pre><p>结果如下，正确且非常直观</p><p><img src="https://s1.328888.xyz/2022/06/08/zcFq3.png" alt="zcFq3.png"></p><h2 id="七：完整代码"><a href="#七：完整代码" class="headerlink" title="七：完整代码"></a>七：完整代码</h2><p>放在我的GitHub了哦~</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 支持向量机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keras构建神经网络预测房价</title>
      <link href="/2022/03/16/Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/"/>
      <url>/2022/03/16/Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/</url>
      
        <content type="html"><![CDATA[<h3 id="一：综述"><a href="#一：综述" class="headerlink" title="一：综述"></a>一：综述</h3><h5 id="今天为大家介绍一个用Keras神经网络构建回归模型以预测房价的案例。"><a href="#今天为大家介绍一个用Keras神经网络构建回归模型以预测房价的案例。" class="headerlink" title="今天为大家介绍一个用Keras神经网络构建回归模型以预测房价的案例。"></a>今天为大家介绍一个用Keras神经网络构建回归模型以预测房价的案例。</h5><p><img src="http://files.mdnice.com/user/21987/0ecf1a9d-8e82-4d8b-b47e-469ba45cf750.jpg" alt="img"></p><h5 id="这个案例中依赖的库有"><a href="#这个案例中依赖的库有" class="headerlink" title="这个案例中依赖的库有"></a>这个案例中依赖的库有</h5><p><img src="http://files.mdnice.com/user/21987/69f2900b-ab0e-4699-8b04-173e9607835a.jpg" alt="img"></p><h5 id="数据集来自-https-github-com-emanhamed-Houses-dataset。该数据集包括csv文件以及图像。本文主要利用该数据集的csv文件部分来训练模型。"><a href="#数据集来自-https-github-com-emanhamed-Houses-dataset。该数据集包括csv文件以及图像。本文主要利用该数据集的csv文件部分来训练模型。" class="headerlink" title="数据集来自 https://github.com/emanhamed/Houses-dataset。该数据集包括csv文件以及图像。本文主要利用该数据集的csv文件部分来训练模型。"></a>数据集来自 <a href="https://github.com/emanhamed/Houses-dataset%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E6%8B%ACcsv%E6%96%87%E4%BB%B6%E4%BB%A5%E5%8F%8A%E5%9B%BE%E5%83%8F%E3%80%82%E6%9C%AC%E6%96%87%E4%B8%BB%E8%A6%81%E5%88%A9%E7%94%A8%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84csv%E6%96%87%E4%BB%B6%E9%83%A8%E5%88%86%E6%9D%A5%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E3%80%82">https://github.com/emanhamed/Houses-dataset。该数据集包括csv文件以及图像。本文主要利用该数据集的csv文件部分来训练模型。</a></h5><h5 id="我们先来看看数据集的样子"><a href="#我们先来看看数据集的样子" class="headerlink" title="我们先来看看数据集的样子"></a>我们先来看看数据集的样子</h5><pre><code class="python">cols = [&quot;bedrooms&quot;, &quot;bathrooms&quot;, &quot;area&quot;, &quot;zipcode&quot;, &quot;price&quot;]import pandas as pddf = pd.read_csv(&quot;E:\code\RegressionWithKeras\RegressionWithKeras\HousesDataset\HousesDataset\HousesInfo.txt&quot;, sep=&quot; &quot;, header=None, names=cols)</code></pre><p><img src="http://files.mdnice.com/user/21987/61dbfe53-a399-40e2-8705-4a2812d69f51.jpg" alt="img"></p><h5 id="包含五个列：房间数、浴室数、面积、邮政编码以及价格，其中价格是我们的目标列，其它则作为特征列。"><a href="#包含五个列：房间数、浴室数、面积、邮政编码以及价格，其中价格是我们的目标列，其它则作为特征列。" class="headerlink" title="包含五个列：房间数、浴室数、面积、邮政编码以及价格，其中价格是我们的目标列，其它则作为特征列。"></a>包含五个列：房间数、浴室数、面积、邮政编码以及价格，其中价格是我们的目标列，其它则作为特征列。</h5><h3 id="二：数据清洗"><a href="#二：数据清洗" class="headerlink" title="二：数据清洗"></a>二：数据清洗</h3><h5 id="在这个数据集中，有一些邮政编码对应的数据很少，是数据集的”噪声“，应该被去除。所以，编写一个函数，用以清洗数据集，代码如下："><a href="#在这个数据集中，有一些邮政编码对应的数据很少，是数据集的”噪声“，应该被去除。所以，编写一个函数，用以清洗数据集，代码如下：" class="headerlink" title="在这个数据集中，有一些邮政编码对应的数据很少，是数据集的”噪声“，应该被去除。所以，编写一个函数，用以清洗数据集，代码如下："></a>在这个数据集中，有一些邮政编码对应的数据很少，是数据集的”噪声“，应该被去除。所以，编写一个函数，用以清洗数据集，代码如下：</h5><pre><code class="python">def load_house_attributes(inputPath):    # 加载数据集    cols = [&quot;bedrooms&quot;, &quot;bathrooms&quot;, &quot;area&quot;, &quot;zipcode&quot;, &quot;price&quot;]    df = pd.read_csv(inputPath, sep=&quot; &quot;, header=None, names=cols)    # zipcodes是每个邮政编码的值    # counts则是每个邮政编码出现的次数    zipcodes = df[&quot;zipcode&quot;].value_counts().keys().tolist()    counts = df[&quot;zipcode&quot;].value_counts().tolist()    # 遍历它们的组合，找出小于阈值25的编码，把它们从df中删掉    for (zipcode, count) in zip(zipcodes, counts):        if count &lt; 25:            idxs = df[df[&quot;zipcode&quot;] == zipcode].index            df.drop(idxs, inplace=True)    return df</code></pre><h5 id="这个函数接受数据集路径，统计数据集中邮政编码的出现次数，剔除出现次数小于25的数据，返回清洗后的dataframe，调用此函数"><a href="#这个函数接受数据集路径，统计数据集中邮政编码的出现次数，剔除出现次数小于25的数据，返回清洗后的dataframe，调用此函数" class="headerlink" title="这个函数接受数据集路径，统计数据集中邮政编码的出现次数，剔除出现次数小于25的数据，返回清洗后的dataframe，调用此函数"></a>这个函数接受数据集路径，统计数据集中邮政编码的出现次数，剔除出现次数小于25的数据，返回清洗后的dataframe，调用此函数</h5><pre><code class="python"># 设置数据集的路径，加载数据集print(&quot;[INFO] loading house attributes...&quot;)inputPath = &quot;./HousesDataset/HousesDataset/HousesInfo.txt&quot;df = datasets.load_house_attributes(inputPath)</code></pre><h5 id="清洗后的数据如下所示："><a href="#清洗后的数据如下所示：" class="headerlink" title="清洗后的数据如下所示："></a>清洗后的数据如下所示：</h5><p><img src="http://files.mdnice.com/user/21987/af638f2f-33fe-42fd-8156-655bd00d75a0.jpg" alt="img"></p><h5 id="可以看出，清洗后的数据index从30开始，剔除了一些数据。"><a href="#可以看出，清洗后的数据index从30开始，剔除了一些数据。" class="headerlink" title="可以看出，清洗后的数据index从30开始，剔除了一些数据。"></a>可以看出，清洗后的数据index从30开始，剔除了一些数据。</h5><h5 id="清洗完成后，我们来划分训练集和测试集，借助sklearn，很容易完成。"><a href="#清洗完成后，我们来划分训练集和测试集，借助sklearn，很容易完成。" class="headerlink" title="清洗完成后，我们来划分训练集和测试集，借助sklearn，很容易完成。"></a>清洗完成后，我们来划分训练集和测试集，借助sklearn，很容易完成。</h5><pre><code class="python">print(&quot;[INFO] constructing training/testing split...&quot;)(train, test) = train_test_split(df, test_size=0.25, random_state=42)</code></pre><h3 id="三：特征工程"><a href="#三：特征工程" class="headerlink" title="三：特征工程"></a>三：特征工程</h3><h5 id="现在让我们把一些连续的数值特征作归一化处理，包括房间数、浴室数和面积，归一化处理能让我们训练的收敛速度更快。"><a href="#现在让我们把一些连续的数值特征作归一化处理，包括房间数、浴室数和面积，归一化处理能让我们训练的收敛速度更快。" class="headerlink" title="现在让我们把一些连续的数值特征作归一化处理，包括房间数、浴室数和面积，归一化处理能让我们训练的收敛速度更快。"></a>现在让我们把一些连续的数值特征作归一化处理，包括房间数、浴室数和面积，归一化处理能让我们训练的收敛速度更快。</h5><h5 id="连续特征处理完了，分类属性也要处理，一般我们就用one—hot编码对离散特征进行处理"><a href="#连续特征处理完了，分类属性也要处理，一般我们就用one—hot编码对离散特征进行处理" class="headerlink" title="连续特征处理完了，分类属性也要处理，一般我们就用one—hot编码对离散特征进行处理"></a>连续特征处理完了，分类属性也要处理，一般我们就用one—hot编码对离散特征进行处理</h5><h5 id="同样构造一个函数来完成。代码如下："><a href="#同样构造一个函数来完成。代码如下：" class="headerlink" title="同样构造一个函数来完成。代码如下："></a>同样构造一个函数来完成。代码如下：</h5><pre><code class="python">def process_house_attributes(df, train, test):    # 要处理的连续特征    continuous = [&quot;bedrooms&quot;, &quot;bathrooms&quot;, &quot;area&quot;]    # 通过MinMax转换器，把它们都变成0~1之间的数    cs = MinMaxScaler()    trainContinuous = cs.fit_transform(train[continuous])    testContinuous = cs.transform(test[continuous])    # 通过LabelBinarizer转换器，将邮政编码进行one-hot编码    zipBinarizer = LabelBinarizer().fit(df[&quot;zipcode&quot;])    trainCategorical = zipBinarizer.transform(train[&quot;zipcode&quot;])    testCategorical = zipBinarizer.transform(test[&quot;zipcode&quot;])    # 将处理后的特征拼接起来，构成训练集    trainX = np.hstack([trainCategorical, trainContinuous])    testX = np.hstack([testCategorical, testContinuous])    # return the concatenated training and testing data    return (trainX, testX)</code></pre><h5 id="这个函数接收清洗后的dataframe以及划分好的训练集，测试集，通过对连续属性进行归一化、离散属性one-hot化，对特征进行进一步处理，最后返回训练特征数据集和测试特征数据集（相比原来剔除了目标列）。"><a href="#这个函数接收清洗后的dataframe以及划分好的训练集，测试集，通过对连续属性进行归一化、离散属性one-hot化，对特征进行进一步处理，最后返回训练特征数据集和测试特征数据集（相比原来剔除了目标列）。" class="headerlink" title="这个函数接收清洗后的dataframe以及划分好的训练集，测试集，通过对连续属性进行归一化、离散属性one-hot化，对特征进行进一步处理，最后返回训练特征数据集和测试特征数据集（相比原来剔除了目标列）。"></a>这个函数接收清洗后的dataframe以及划分好的训练集，测试集，通过对连续属性进行归一化、离散属性one-hot化，对特征进行进一步处理，最后返回训练特征数据集和测试特征数据集（相比原来剔除了目标列）。</h5><pre><code class="python"># 对特征列进行归一化和one—hot编码print(&quot;[INFO] processing data...&quot;)(trainX, testX) = datasets.process_house_attributes(df, train, test)</code></pre><h5 id="因为目标列也是连续属性，所以也进行归一化处理"><a href="#因为目标列也是连续属性，所以也进行归一化处理" class="headerlink" title="因为目标列也是连续属性，所以也进行归一化处理"></a>因为目标列也是连续属性，所以也进行归一化处理</h5><pre><code class="python"># 对目标列也进行归一化maxPriceTrain = train[&quot;price&quot;].max()maxPriceTest = test[&quot;price&quot;].max()trainY = train[&quot;price&quot;] / maxPriceTraintestY = test[&quot;price&quot;] / maxPriceTrain</code></pre><h3 id="四：模型建立"><a href="#四：模型建立" class="headerlink" title="四：模型建立"></a>四：模型建立</h3><h5 id="使用Keras构建多层感知机模型，也叫人工神经网络ANN。MLP适用于分类预测问题，输入一组数据，输出一个分类。它们也适用于回归预测问题，输入一组数据，输出一个连续值。"><a href="#使用Keras构建多层感知机模型，也叫人工神经网络ANN。MLP适用于分类预测问题，输入一组数据，输出一个分类。它们也适用于回归预测问题，输入一组数据，输出一个连续值。" class="headerlink" title="使用Keras构建多层感知机模型，也叫人工神经网络ANN。MLP适用于分类预测问题，输入一组数据，输出一个分类。它们也适用于回归预测问题，输入一组数据，输出一个连续值。"></a>使用Keras构建多层感知机模型，也叫人工神经网络ANN。MLP适用于分类预测问题，输入一组数据，输出一个分类。它们也适用于回归预测问题，输入一组数据，输出一个连续值。</h5><h5 id="如图所示："><a href="#如图所示：" class="headerlink" title="如图所示："></a>如图所示：</h5><p><img src="http://files.mdnice.com/user/21987/e7127cb1-275c-47d2-b991-41c065f5dd30.png" alt="img"></p><h5 id="代码如下："><a href="#代码如下：" class="headerlink" title="代码如下："></a>代码如下：</h5><pre><code class="python">def create_mlp(dim, regress=False): # 定义MLP神经网络 model = Sequential() # 在网络上加两个层，一层节点为8，另一层节点为4，两层的激活函数都是relu model.add(Dense(8, input_dim=dim, activation=&quot;relu&quot;)) model.add(Dense(4, activation=&quot;relu&quot;)) # 如果用于回归任务，再加一层（输出层），激活函数为linear if regress:  model.add(Dense(1, activation=&quot;linear&quot;)) # 返回model return model# 构建MLP模型model = models.create_mlp(trainX.shape[1], regress=True)# 其中opt包含了lr学习率，以及decay衰减率opt = Adam(lr=1e-3, decay=1e-3 / 200)# 设置损失函数为MAPE，并且传入opt，完成模型的构建model.compile(loss=&quot;mean_absolute_percentage_error&quot;, optimizer=opt)</code></pre><h3 id="五：训练模型并预测"><a href="#五：训练模型并预测" class="headerlink" title="五：训练模型并预测"></a>五：训练模型并预测</h3><pre><code class="python"># 训练模型，训练2000轮，每一轮的用8个样本更新梯度（小批量梯度下降算法）print(&quot;[INFO] training model...&quot;)model.fit(x=trainX, y=trainY, validation_data=(testX, testY), epochs=2000, batch_size=8)# 在测试集上作预测print(&quot;[INFO] predicting house prices...&quot;)preds = model.predict(testX)# 计算误差、误差百分比、误差百分比的绝对值diff = preds.flatten() - testYpercentDiff = (diff / testY) * 100absPercentDiff = np.abs(percentDiff)# 对误差百分比的绝对值求均值和标准差mean = np.mean(absPercentDiff)std = np.std(absPercentDiff)# 打印模型训练的相关指标locale.setlocale(locale.LC_ALL, &quot;en_US.UTF-8&quot;)print(&quot;[INFO] avg. house price: &#123;&#125;, std house price: &#123;&#125;&quot;.format( locale.currency(df[&quot;price&quot;].mean(), grouping=True), locale.currency(df[&quot;price&quot;].std(), grouping=True)))print(&quot;[INFO] mean: &#123;:.2f&#125;%, std: &#123;:.2f&#125;%&quot;.format(mean, std))</code></pre><h5 id="运行结果如下："><a href="#运行结果如下：" class="headerlink" title="运行结果如下："></a>运行结果如下：</h5><p><img src="http://files.mdnice.com/user/21987/66e670cf-51e8-497d-b216-5690f8edb10e.jpg" alt="img"></p><h3 id="六：进一步学习"><a href="#六：进一步学习" class="headerlink" title="六：进一步学习"></a>六：进一步学习</h3><h5 id="对MLP不熟悉的朋友们可以看B站UP主-3blue1Brown-出品的深度神经网络视频。"><a href="#对MLP不熟悉的朋友们可以看B站UP主-3blue1Brown-出品的深度神经网络视频。" class="headerlink" title="对MLP不熟悉的朋友们可以看B站UP主 3blue1Brown 出品的深度神经网络视频。"></a>对MLP不熟悉的朋友们可以看B站UP主 3blue1Brown 出品的深度神经网络视频。</h5><h5 id="关于Keras的更多知识以及api使用可以访问它的中文文档进行进一步的学习：https-keras-io-zh"><a href="#关于Keras的更多知识以及api使用可以访问它的中文文档进行进一步的学习：https-keras-io-zh" class="headerlink" title="关于Keras的更多知识以及api使用可以访问它的中文文档进行进一步的学习：https://keras.io/zh/"></a>关于Keras的更多知识以及api使用可以访问它的中文文档进行进一步的学习：<a href="https://keras.io/zh/">https://keras.io/zh/</a></h5>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>十分钟入门简单而强大的可视化库</title>
      <link href="/2022/01/16/%E5%8D%81%E5%88%86%E9%92%9F%E5%85%A5%E9%97%A8%E7%AE%80%E5%8D%95%E8%80%8C%E5%BC%BA%E5%A4%A7%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E5%BA%93%E2%80%94%E2%80%94plot%20express/"/>
      <url>/2022/01/16/%E5%8D%81%E5%88%86%E9%92%9F%E5%85%A5%E9%97%A8%E7%AE%80%E5%8D%95%E8%80%8C%E5%BC%BA%E5%A4%A7%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E5%BA%93%E2%80%94%E2%80%94plot%20express/</url>
      
        <content type="html"><![CDATA[<h5 id="在python的世界里，用于可视化分析的库有很多，有最原始的matplotlib，还有更加简单好用的serborn（对matplotlib进行封装），而pyecharts则是以动态、可交互的特点吸引着众人。"><a href="#在python的世界里，用于可视化分析的库有很多，有最原始的matplotlib，还有更加简单好用的serborn（对matplotlib进行封装），而pyecharts则是以动态、可交互的特点吸引着众人。" class="headerlink" title="在python的世界里，用于可视化分析的库有很多，有最原始的matplotlib，还有更加简单好用的serborn（对matplotlib进行封装），而pyecharts则是以动态、可交互的特点吸引着众人。"></a>在python的世界里，用于可视化分析的库有很多，有最原始的matplotlib，还有更加简单好用的serborn（对matplotlib进行封装），而pyecharts则是以动态、可交互的特点吸引着众人。</h5><h5 id="今天为大家介绍一个兼具pyecharts的动态性和serborn的简便性与一体的可视化库——plotly-express"><a href="#今天为大家介绍一个兼具pyecharts的动态性和serborn的简便性与一体的可视化库——plotly-express" class="headerlink" title="今天为大家介绍一个兼具pyecharts的动态性和serborn的简便性与一体的可视化库——plotly express"></a>今天为大家介绍一个兼具pyecharts的动态性和serborn的简便性与一体的可视化库——plotly express</h5><h5 id="让我们对比一下pyecharts与plotly-express完成桑基图的步骤"><a href="#让我们对比一下pyecharts与plotly-express完成桑基图的步骤" class="headerlink" title="让我们对比一下pyecharts与plotly express完成桑基图的步骤"></a>让我们对比一下pyecharts与plotly express完成桑基图的步骤</h5><h5 id="首先看pyecharts，由于其需要苛刻的数据结构，所以需要花费大量的精力用于数据处理。"><a href="#首先看pyecharts，由于其需要苛刻的数据结构，所以需要花费大量的精力用于数据处理。" class="headerlink" title="首先看pyecharts，由于其需要苛刻的数据结构，所以需要花费大量的精力用于数据处理。"></a>首先看pyecharts，由于其需要苛刻的数据结构，所以需要花费大量的精力用于数据处理。</h5><p><img src="http://files.mdnice.com/user/21987/f3d6d980-e028-43a7-bdb7-9fd79c75193b.png" alt="img"></p><h5 id="然后我们看plotly-express，简简单单几行代码就实现了类似的效果，可以节省大量的时间精力，提升工作效率！"><a href="#然后我们看plotly-express，简简单单几行代码就实现了类似的效果，可以节省大量的时间精力，提升工作效率！" class="headerlink" title="然后我们看plotly express，简简单单几行代码就实现了类似的效果，可以节省大量的时间精力，提升工作效率！"></a>然后我们看plotly express，简简单单几行代码就实现了类似的效果，可以节省大量的时间精力，提升工作效率！</h5><p><img src="http://files.mdnice.com/user/21987/0a161e0c-359e-494f-82a1-6e653f079dc5.jpg" alt="img"> <img src="https://files.mdnice.com/user/21987/0225e76a-cbce-4ffa-862c-98c24bbd94af.jpg" alt="img"></p><h5 id="接下来我们就来学习plotly-express的使用方法吧！"><a href="#接下来我们就来学习plotly-express的使用方法吧！" class="headerlink" title="接下来我们就来学习plotly express的使用方法吧！"></a>接下来我们就来学习plotly express的使用方法吧！</h5><h5 id="首先导入我们的可视化库-加载数据集，并且查看数据集的结构"><a href="#首先导入我们的可视化库-加载数据集，并且查看数据集的结构" class="headerlink" title="首先导入我们的可视化库,加载数据集，并且查看数据集的结构"></a>首先导入我们的可视化库,加载数据集，并且查看数据集的结构</h5><pre><code>import pandas as pdimport numpy as npimport plotly.express as px# 数据集gapminder = px.data.gapminder()gapminder.head(10)</code></pre><p><img src="http://files.mdnice.com/user/21987/e1b5a299-b5b0-47e7-b5f1-f9fec9e41e4e.jpg" alt="img"></p><h5 id="线图：用year列作为x轴，lifeExp为y轴，以continent列的不同值作颜色区分，线条的数量就是country数量。"><a href="#线图：用year列作为x轴，lifeExp为y轴，以continent列的不同值作颜色区分，线条的数量就是country数量。" class="headerlink" title="线图：用year列作为x轴，lifeExp为y轴，以continent列的不同值作颜色区分，线条的数量就是country数量。"></a>线图：用year列作为x轴，lifeExp为y轴，以continent列的不同值作颜色区分，线条的数量就是country数量。</h5><pre><code># line 图fig = px.line(  gapminder,  # 数据集  x=&quot;year&quot;,  # 横坐标  y=&quot;lifeExp&quot;,  # 纵坐标  color=&quot;continent&quot;,  # 颜色的数据  line_group=&quot;continent&quot;,  # 线性分组  hover_name=&quot;country&quot;,   # 悬停hover的数据  line_shape=&quot;spline&quot;,  # 线的形状  render_mode=&quot;svg&quot;  # 生成的图片模式)fig.show()</code></pre><p><img src="http://files.mdnice.com/user/21987/d43e815c-27a4-48a4-bf09-33a36840b09b.jpg" alt="img"></p><h5 id="因为国家的数量太多了，所以线条很多，有点乱，可以点击图例隐藏一些数据以更好展现图表"><a href="#因为国家的数量太多了，所以线条很多，有点乱，可以点击图例隐藏一些数据以更好展现图表" class="headerlink" title="因为国家的数量太多了，所以线条很多，有点乱，可以点击图例隐藏一些数据以更好展现图表"></a>因为国家的数量太多了，所以线条很多，有点乱，可以点击图例隐藏一些数据以更好展现图表</h5><h5 id="面积图"><a href="#面积图" class="headerlink" title="面积图"></a>面积图</h5><pre><code># area 图fig = px.area(  gapminder,  # 数据集  x=&quot;year&quot;,  # 横坐标  y=&quot;pop&quot;,  # 纵坐标  color=&quot;continent&quot;,   # 颜色  line_group=&quot;country&quot;  # 线性组别)fig.show()</code></pre><p><img src="http://files.mdnice.com/user/21987/d86cea0e-1914-4de9-a360-362364b9e63f.jpg" alt="img"></p><h5 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h5><pre><code># 散点图px.scatter(  gapminder   # 绘图DataFrame数据集  ,x=&quot;gdpPercap&quot;  # 横坐标  ,y=&quot;lifeExp&quot;  # 纵坐标  ,color=&quot;continent&quot;  # 区分颜色  ,size=&quot;pop&quot;   # 区分圆的大小  ,size_max=60  # 散点大小)</code></pre><p><img src="http://files.mdnice.com/user/21987/3a24e9bf-e894-42f6-b1f8-d2c50bb213a2.jpg" alt="img"></p><pre><code>px.scatter(  gapminder   # 绘图使用的数据  ,x=&quot;gdpPercap&quot; # 横纵坐标使用的数据  ,y=&quot;lifeExp&quot;  # 纵坐标数据  ,color=&quot;continent&quot;  # 区分颜色的属性  ,size=&quot;pop&quot;   # 区分圆的大小  ,size_max=60  # 圆的最大值  ,hover_name=&quot;country&quot;  # 图中可视化最上面的名字  ,animation_frame=&quot;year&quot;  # 横轴滚动栏的属性year  ,animation_group=&quot;country&quot;  # 标注的分组  ,facet_col=&quot;continent&quot;   # 按照国家country属性进行分格显示  ,log_x=True  # 横坐标表取对数  ,range_x=[100,100000]  # 横轴取值范围  ,range_y=[25,90]  # 纵轴范围  ,labels=dict(pop=&quot;Populations&quot;,  # 属性名字的变化，更直观               gdpPercap=&quot;GDP per Capital&quot;,               lifeExp=&quot;Life Expectancy&quot;))</code></pre><p><img src="http://files.mdnice.com/user/21987/33eb49a6-1ff5-4469-a118-f5de47fb42a7.jpg" alt="img"></p><h5 id="地理图"><a href="#地理图" class="headerlink" title="地理图"></a>地理图</h5><pre><code># 地理图px.choropleth(  gapminder,  # 数据集  locations=&quot;iso_alpha&quot;,  # 配合颜色color显示  color=&quot;lifeExp&quot;, # 颜色的字段选择  hover_name=&quot;country&quot;,  # 悬停字段名字  animation_frame=&quot;year&quot;,  # 注释  color_continuous_scale=px.colors.sequential.Plasma,  # 颜色变化  projection=&quot;natural earth&quot;  # 全球地图             )</code></pre><p><img src="http://files.mdnice.com/user/21987/7b47b551-aae1-4859-a39e-a1418ac1e072.jpg" alt="img"></p><pre><code>fig = px.scatter_geo(  gapminder,   # 数据  locations=&quot;iso_alpha&quot;,  # 配合颜色color显示  color=&quot;continent&quot;, # 颜色  hover_name=&quot;country&quot;, # 悬停数据  size=&quot;pop&quot;,  # 大小  animation_frame=&quot;year&quot;,  # 数据帧的选择  projection=&quot;natural earth&quot;  # 全球地图                    )fig.show()</code></pre><p><img src="http://files.mdnice.com/user/21987/f4b9cff7-f791-4e5d-9d2f-0a0306c22285.jpg" alt="img"></p><pre><code>fig = px.line_geo(  gapminder,  # 数据集  locations=&quot;iso_alpha&quot;,  # 配合和color显示数据  color=&quot;continent&quot;,  # 颜色  projection=&quot;orthographic&quot;)   # 球形的地图fig.show()</code></pre><p><img src="http://files.mdnice.com/user/21987/e5a0d0e6-ba01-4830-b5ca-a2bca5ea3b45.jpg" alt="img"></p><h5 id="接下来换上大名鼎鼎内置的鸢尾花数据集"><a href="#接下来换上大名鼎鼎内置的鸢尾花数据集" class="headerlink" title="接下来换上大名鼎鼎内置的鸢尾花数据集"></a>接下来换上大名鼎鼎内置的鸢尾花数据集</h5><pre><code># iris数据集iris = px.data.iris()iris.head(10)</code></pre><p><img src="http://files.mdnice.com/user/21987/5a2ddf15-7ff4-447b-a8ea-131c2b0fd518.jpg" alt="img"></p><h5 id="继续探索散点图"><a href="#继续探索散点图" class="headerlink" title="继续探索散点图"></a>继续探索散点图</h5><pre><code># 选择两个属性作为横纵坐标来绘制散点图fig = px.scatter(  iris,  # 数据集  x=&quot;sepal_width&quot;,  # 横坐标  y=&quot;sepal_length&quot;,# 纵坐标  color=&quot;species&quot;                    )fig.show()</code></pre><p><img src="http://files.mdnice.com/user/21987/93e9db65-0c77-4463-a472-a43cf84bc14d.jpg" alt="img"></p><pre><code>px.scatter(  iris,  # 数据集  x=&quot;sepal_width&quot;, # 横坐标  y=&quot;sepal_length&quot;,  # 纵坐标  color=&quot;species&quot;,  # 颜色  marginal_x=&quot;histogram&quot;,  # 横坐标直方图  marginal_y=&quot;rug&quot;   # 细条图)</code></pre><p><img src="http://files.mdnice.com/user/21987/d07940be-3ceb-4297-824b-b70deb600023.jpg" alt="img"></p><pre><code>px.scatter(  iris,  # 数据集  x=&quot;sepal_width&quot;,  # 横坐标  y=&quot;sepal_length&quot;,  # 纵坐标  color=&quot;species&quot;,  # 颜色  marginal_y=&quot;violin&quot;,  # 纵坐标小提琴图  marginal_x=&quot;box&quot;,  # 横坐标箱型图  trendline=&quot;ols&quot;  # 趋势线)</code></pre><p><img src="http://files.mdnice.com/user/21987/b8d2cb57-b816-47e5-a860-f8157384ccde.jpg" alt="img"></p><pre><code>px.scatter_matrix(  iris,  # 数据  dimensions=[&quot;sepal_width&quot;,&quot;sepal_length&quot;,&quot;petal_width&quot;,&quot;petal_length&quot;],  # 维度选择  color=&quot;species&quot;)  # 颜色</code></pre><p><img src="http://files.mdnice.com/user/21987/0dc9c262-cf42-441e-b053-51832a534d54.jpg" alt="img"></p><h5 id="平行对比图-：通过这张图，可以直观发现数据之间的相关性"><a href="#平行对比图-：通过这张图，可以直观发现数据之间的相关性" class="headerlink" title="平行对比图 ：通过这张图，可以直观发现数据之间的相关性"></a>平行对比图 ：通过这张图，可以直观发现数据之间的相关性</h5><pre><code>px.parallel_coordinates(  iris,   # 数据集  color=&quot;species_id&quot;,  # 颜色  labels=&#123;&quot;species_id&quot;:&quot;Species&quot;,  # 各种标签值          &quot;sepal_width&quot;:&quot;Sepal Width&quot;,          &quot;sepal_length&quot;:&quot;Sepal Length&quot;,          &quot;petal_length&quot;:&quot;Petal Length&quot;,          &quot;petal_width&quot;:&quot;Petal Width&quot;&#125;,  color_continuous_scale=px.colors.diverging.Tealrose,  color_continuous_midpoint=2)</code></pre><p><img src="http://files.mdnice.com/user/21987/7052c80e-6708-43bd-a2ce-c2021d87e23a.jpg" alt="img"></p><h5 id="等高密度图"><a href="#等高密度图" class="headerlink" title="等高密度图"></a>等高密度图</h5><pre><code>px.density_contour(  iris,  # 绘图数据集  x=&quot;sepal_width&quot;,  # 横坐标  y=&quot;sepal_length&quot;,  # 纵坐标值  color=&quot;species&quot; , # 颜色  marginal_x=&quot;histogram&quot;,   marginal_y=&quot;histogram&quot;)</code></pre><p><img src="http://files.mdnice.com/user/21987/2fb43c94-f40d-4435-868c-d808cb989151.jpg" alt="img"></p><pre><code>px.density_contour(  iris, # 数据集  x=&quot;sepal_width&quot;,  # 横坐标值  y=&quot;sepal_length&quot;,  # 纵坐标值  color=&quot;species&quot;,  # 颜色  marginal_x=&quot;rug&quot;,  # 横轴为线条图  marginal_y=&quot;histogram&quot;   # 纵轴为直方图                  )          </code></pre><p><img src="http://files.mdnice.com/user/21987/5f88f765-87e8-4798-81ba-c2a665354091.jpg" alt="img"></p><h5 id="热图"><a href="#热图" class="headerlink" title="热图"></a>热图</h5><pre><code>px.density_heatmap(  iris,  # 数据集  x=&quot;sepal_width&quot;,   # 横坐标值  y=&quot;sepal_length&quot;,  # 纵坐标值  marginal_y=&quot;rug&quot;,  # 纵坐标值为线型图  marginal_x=&quot;histogram&quot;  # 直方图                  )</code></pre><p><img src="http://files.mdnice.com/user/21987/a6d05892-8882-4ef3-9a52-d4c36bbfbeb3.jpg" alt="img"></p><h5 id="再换个数据集看看"><a href="#再换个数据集看看" class="headerlink" title="再换个数据集看看"></a>再换个数据集看看</h5><pre><code># 小票数据集tips = px.data.tips()tips.head()</code></pre><p><img src="http://files.mdnice.com/user/21987/548bb410-10c3-4c62-bb54-3e24fdc30013.jpg" alt="img"></p><h5 id="平行种类图-前面一开始提到的桑基图就是基于此制作的"><a href="#平行种类图-前面一开始提到的桑基图就是基于此制作的" class="headerlink" title="平行种类图   前面一开始提到的桑基图就是基于此制作的"></a>平行种类图   前面一开始提到的桑基图就是基于此制作的</h5><pre><code>fig = px.parallel_categories(  tips,  # 数据集   color=&quot;size&quot;,  # 颜色  color_continuous_scale=px.colors.sequential.Inferno)  # 颜色变化取值fig.show()</code></pre><p><img src="http://files.mdnice.com/user/21987/547574ab-3f16-41d4-a9d3-d650f57fcaa5.jpg" alt="img"></p><h5 id="柱状图"><a href="#柱状图" class="headerlink" title="柱状图"></a>柱状图</h5><pre><code>bar = px.bar(    tips,        x=&quot;sex&quot;,        y=&quot;total_bill&quot;,        color=&quot;smoker&quot;,        barmode=&quot;group&quot;)bar.show()</code></pre><p><img src="http://files.mdnice.com/user/21987/3c87eb14-f953-42a8-8c4f-262c4f4d6a04.jpg" alt="img"></p><pre><code>fig = px.bar(  tips,  # 数据集  x=&quot;sex&quot;,  # 横轴  y=&quot;total_bill&quot;,  # 纵轴  color=&quot;smoker&quot;,  # 颜色参数取值  barmode=&quot;group&quot;,  # 柱状图模式取值  facet_row=&quot;time&quot;,  # 行取值  facet_col=&quot;day&quot;,  # 列元素取值  category_orders=&#123;    &quot;day&quot;: [&quot;Thur&quot;,&quot;Fri&quot;,&quot;Sat&quot;,&quot;Sun&quot;],  # 分类顺序    &quot;time&quot;:[&quot;Lunch&quot;, &quot;Dinner&quot;]&#125;)fig.show()</code></pre><p><img src="http://files.mdnice.com/user/21987/26b34a2f-4804-4f87-bc50-962b2977ff80.jpg" alt="img"></p><h5 id="盒须图"><a href="#盒须图" class="headerlink" title="盒须图"></a>盒须图</h5><pre><code>px.box(tips,  # 数据集       x=&quot;day&quot;,  # 横轴数据       y=&quot;total_bill&quot;,  # 纵轴数据       color=&quot;smoker&quot;,  # 颜色       notched=True)  # 连接处的锥形部分显示出来</code></pre><p><img src="http://files.mdnice.com/user/21987/4d5f8d6d-dbd1-4413-b01c-a14a6f0b983d.jpg" alt="img"></p><h5 id="小提琴图"><a href="#小提琴图" class="headerlink" title="小提琴图"></a>小提琴图</h5><pre><code>px.violin(    tips,   # 数据集    x=&quot;smoker&quot;,  # 横轴坐标    y=&quot;tip&quot;,  # 纵轴坐标      color=&quot;sex&quot;,   # 颜色参数取值    box=True,   # box是显示内部的箱体    points=&quot;all&quot;,  # 同时显示数值点    hover_data=tips.columns)  # 结果中显示全部数据</code></pre><p><img src="http://files.mdnice.com/user/21987/00bb2cd1-3865-45bc-aebb-67207ea7f5a2.jpg" alt="img"></p><h5 id="最后再来看一个内置数据集吧"><a href="#最后再来看一个内置数据集吧" class="headerlink" title="最后再来看一个内置数据集吧"></a>最后再来看一个内置数据集吧</h5><pre><code># 风数据集wind = px.data.wind()wind.head(10)</code></pre><p><img src="http://files.mdnice.com/user/21987/ea4b72fd-8a22-4f4f-9e1b-d8c64cd2b72c.jpg" alt="img"></p><h5 id="极坐标图"><a href="#极坐标图" class="headerlink" title="极坐标图"></a>极坐标图</h5><pre><code>fig = px.scatter_polar(    wind,     r = &quot;frequency&quot; , # 半径    theta = &quot;direction&quot;, # 角度    color = &quot;strength&quot;, # 颜色    symbol = &quot;strength&quot;, # 图标    color_continuous_scale=px.colors.sequential.Inferno  # 颜色变化取值    )fig.show()</code></pre><h5 id="与散点图结合"><a href="#与散点图结合" class="headerlink" title="与散点图结合"></a>与散点图结合</h5><p><img src="http://files.mdnice.com/user/21987/2bfa3f56-83dc-41f2-b1e2-dc199b9a255d.jpg" alt="img"></p><pre><code>fig = px.line_polar(    wind,  # 数据集    r=&quot;frequency&quot;,  # 半径    theta=&quot;direction&quot;,  # 角度    color=&quot;strength&quot;,  # 颜色    line_close=True,  # 线性闭合    color_discrete_sequence=px.colors.sequential.Plasma_r)  # 颜色变化fig.show()</code></pre><h5 id="与线图结合"><a href="#与线图结合" class="headerlink" title="与线图结合"></a>与线图结合</h5><p><img src="http://files.mdnice.com/user/21987/d9f3bec2-d1aa-40fc-aab9-6a2baa423e7f.jpg" alt="img"></p><h5 id="与柱状图结合"><a href="#与柱状图结合" class="headerlink" title="与柱状图结合"></a>与柱状图结合</h5><pre><code>fig = px.bar_polar(    wind,   # 数据集    r=&quot;frequency&quot;,   # 半径    theta=&quot;direction&quot;,  # 角度    color=&quot;strength&quot;,  # 颜色    template=&quot;plotly_dark&quot;,  # 主题    color_discrete_sequence=px.colors.sequential.Plasma_r)  # 颜色变化fig.show()</code></pre><p><img src="http://files.mdnice.com/user/21987/49154cbe-fb88-44f6-b86b-d33d1b44798f.jpg" alt="img"></p><h5 id="看完了这些是不是很想学习这个库呢？那就快去找官方文档看吧！"><a href="#看完了这些是不是很想学习这个库呢？那就快去找官方文档看吧！" class="headerlink" title="看完了这些是不是很想学习这个库呢？那就快去找官方文档看吧！"></a>看完了这些是不是很想学习这个库呢？那就快去找官方文档看吧！</h5>]]></content>
      
      
      <categories>
          
          <category> 数据可视化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> plot express </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于随机森林填补缺失值</title>
      <link href="/2022/01/10/%E5%9F%BA%E4%BA%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%A1%AB%E8%A1%A5%E7%BC%BA%E5%A4%B1%E5%80%BC/"/>
      <url>/2022/01/10/%E5%9F%BA%E4%BA%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%A1%AB%E8%A1%A5%E7%BC%BA%E5%A4%B1%E5%80%BC/</url>
      
        <content type="html"><![CDATA[<h4 id="众所周知，进行机器学习建模的第一步就是数据预处理，在数据预处理的过程中，处理缺失值则是关键一步。在数据集规模较小的情况下，如果对缺失值进行贸然的删除，则会导致本就不多的数据更为稀少。所以我们需要对缺失值进行一定的填补。"><a href="#众所周知，进行机器学习建模的第一步就是数据预处理，在数据预处理的过程中，处理缺失值则是关键一步。在数据集规模较小的情况下，如果对缺失值进行贸然的删除，则会导致本就不多的数据更为稀少。所以我们需要对缺失值进行一定的填补。" class="headerlink" title="众所周知，进行机器学习建模的第一步就是数据预处理，在数据预处理的过程中，处理缺失值则是关键一步。在数据集规模较小的情况下，如果对缺失值进行贸然的删除，则会导致本就不多的数据更为稀少。所以我们需要对缺失值进行一定的填补。"></a>众所周知，进行机器学习建模的第一步就是数据预处理，在数据预处理的过程中，处理缺失值则是关键一步。在数据集规模较小的情况下，如果对缺失值进行贸然的删除，则会导致本就不多的数据更为稀少。所以我们需要对缺失值进行一定的填补。</h4><h4 id="在填补的方法中，有直接用0填补的，有用均值的，有用中位数的，还有用众数的"><a href="#在填补的方法中，有直接用0填补的，有用均值的，有用中位数的，还有用众数的" class="headerlink" title="在填补的方法中，有直接用0填补的，有用均值的，有用中位数的，还有用众数的"></a>在填补的方法中，有直接用0填补的，有用均值的，有用中位数的，还有用众数的</h4><h4 id="这些方法虽然简单，但是对数据集的还原程度不高，所以今天为大家介绍如何使用随机森林的方法预测并且填补缺失值"><a href="#这些方法虽然简单，但是对数据集的还原程度不高，所以今天为大家介绍如何使用随机森林的方法预测并且填补缺失值" class="headerlink" title="这些方法虽然简单，但是对数据集的还原程度不高，所以今天为大家介绍如何使用随机森林的方法预测并且填补缺失值"></a>这些方法虽然简单，但是对数据集的还原程度不高，所以今天为大家介绍如何使用随机森林的方法预测并且填补缺失值</h4><h4 id="我们先来看看这个数据集"><a href="#我们先来看看这个数据集" class="headerlink" title="我们先来看看这个数据集"></a>我们先来看看这个数据集</h4><p><img src="http://files.mdnice.com/user/21987/e4eae558-6f69-41f7-877d-84656998f4e6.jpg" alt="img"></p><h4 id="它有许多缺失值，我们先对这个数据集进行探索"><a href="#它有许多缺失值，我们先对这个数据集进行探索" class="headerlink" title="它有许多缺失值，我们先对这个数据集进行探索"></a>它有许多缺失值，我们先对这个数据集进行探索</h4><pre><code class="python">import numpy as npimport pandas as pddata = pd.read_csv(&quot;test-2.csv&quot;)</code></pre><h2 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h2><pre><code>data</code></pre><table><thead><tr><th align="left"></th><th align="left">year</th><th align="left">selling_price</th><th align="left">km_driven</th><th align="left">fuel</th><th align="left">seller_type</th><th align="left">transmission</th><th align="left">owner</th><th align="left">mileage</th><th align="left">engine</th><th align="left">seats</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">2014</td><td align="left">450000</td><td align="left">145500</td><td align="left">Diesel</td><td align="left">Individual</td><td align="left">Manual</td><td align="left">First Owner</td><td align="left">23.40</td><td align="left">1248.0</td><td align="left">5.0</td></tr><tr><td align="left">1</td><td align="left">2014</td><td align="left">370000</td><td align="left">120000</td><td align="left">Diesel</td><td align="left">Individual</td><td align="left">Manual</td><td align="left">Second Owner</td><td align="left">21.14</td><td align="left">1498.0</td><td align="left">5.0</td></tr><tr><td align="left">2</td><td align="left">2006</td><td align="left">158000</td><td align="left">140000</td><td align="left">Petrol</td><td align="left">Individual</td><td align="left">Manual</td><td align="left">Third Owner</td><td align="left">17.70</td><td align="left">1497.0</td><td align="left">5.0</td></tr><tr><td align="left">3</td><td align="left">2010</td><td align="left">225000</td><td align="left">127000</td><td align="left">Diesel</td><td align="left">Individual</td><td align="left">Manual</td><td align="left">First Owner</td><td align="left">23.00</td><td align="left">1396.0</td><td align="left">5.0</td></tr><tr><td align="left">4</td><td align="left">2007</td><td align="left">130000</td><td align="left">120000</td><td align="left">Petrol</td><td align="left">Individual</td><td align="left">Manual</td><td align="left">First Owner</td><td align="left">16.10</td><td align="left">1298.0</td><td align="left">5.0</td></tr><tr><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td></tr><tr><td align="left">94</td><td align="left">2009</td><td align="left">175000</td><td align="left">55500</td><td align="left">Petrol</td><td align="left">Dealer</td><td align="left">Manual</td><td align="left">First Owner</td><td align="left">18.20</td><td align="left">998.0</td><td align="left">5.0</td></tr><tr><td align="left">95</td><td align="left">2013</td><td align="left">525000</td><td align="left">61500</td><td align="left">Petrol</td><td align="left">Dealer</td><td align="left">Manual</td><td align="left">First Owner</td><td align="left">18.50</td><td align="left">1197.0</td><td align="left">5.0</td></tr><tr><td align="left">96</td><td align="left">2016</td><td align="left">600000</td><td align="left">150000</td><td align="left">Diesel</td><td align="left">Individual</td><td align="left">Manual</td><td align="left">First Owner</td><td align="left">26.59</td><td align="left">1248.0</td><td align="left">5.0</td></tr><tr><td align="left">97</td><td align="left">2016</td><td align="left">565000</td><td align="left">72000</td><td align="left">Petrol</td><td align="left">Dealer</td><td align="left">Automatic</td><td align="left">First Owner</td><td align="left">19.10</td><td align="left">1197.0</td><td align="left">5.0</td></tr><tr><td align="left">98</td><td align="left">2008</td><td align="left">120000</td><td align="left">68000</td><td align="left">Petrol</td><td align="left">Dealer</td><td align="left">Manual</td><td align="left">Third Owner</td><td align="left">19.70</td><td align="left">796.0</td><td align="left">5.0</td></tr></tbody></table><p>99 rows × 10 columns</p><pre><code>data.info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 99 entries, 0 to 98Data columns (total 10 columns): #   Column         Non-Null Count  Dtype  ---  ------         --------------  -----   0   year           99 non-null     int64   1   selling_price  99 non-null     int64   2   km_driven      99 non-null     int64   3   fuel           99 non-null     object  4   seller_type    99 non-null     object  5   transmission   99 non-null     object  6   owner          99 non-null     object  7   mileage        94 non-null     float64 8   engine         96 non-null     float64 9   seats          94 non-null     float64dtypes: float64(3), int64(3), object(4)memory usage: 7.9+ KB</code></pre><h1 id="只有mileage、-engine-、seats有缺失值，其余都完整"><a href="#只有mileage、-engine-、seats有缺失值，其余都完整" class="headerlink" title="只有mileage、 engine 、seats有缺失值，其余都完整"></a>只有mileage、 engine 、seats有缺失值，其余都完整</h1><h2 id="重点关注文本数据"><a href="#重点关注文本数据" class="headerlink" title="重点关注文本数据"></a>重点关注文本数据</h2><pre><code>data[&quot;fuel&quot;].value_counts()Petrol    48Diesel    48LPG        2CNG        1Name: fuel, dtype: int64data[&quot;seller_type&quot;].value_counts()Individual    65Dealer        34Name: seller_type, dtype: int64data[&quot;transmission&quot;].value_counts()Manual       87Automatic    12Name: transmission, dtype: int64data[&quot;owner&quot;].value_counts()First Owner     69Second Owner    26Third Owner      4Name: owner, dtype: int64</code></pre><h4 id="对文本数据，一般采用onehot编码或者label编码"><a href="#对文本数据，一般采用onehot编码或者label编码" class="headerlink" title="对文本数据，一般采用onehot编码或者label编码"></a>对文本数据，一般采用onehot编码或者label编码</h4><h4 id="从语义上看owner这个属性的值是有明显的定序特征，不宜采用onehot编码，而其余都是分类属性，并且属性值的种类不多"><a href="#从语义上看owner这个属性的值是有明显的定序特征，不宜采用onehot编码，而其余都是分类属性，并且属性值的种类不多" class="headerlink" title="从语义上看owner这个属性的值是有明显的定序特征，不宜采用onehot编码，而其余都是分类属性，并且属性值的种类不多"></a>从语义上看owner这个属性的值是有明显的定序特征，不宜采用onehot编码，而其余都是分类属性，并且属性值的种类不多</h4><h4 id="不会对随机森林算法有过度的负面作用，所以可以采用onehot编码"><a href="#不会对随机森林算法有过度的负面作用，所以可以采用onehot编码" class="headerlink" title="不会对随机森林算法有过度的负面作用，所以可以采用onehot编码"></a>不会对随机森林算法有过度的负面作用，所以可以采用onehot编码</h4><pre><code>def func(x: str) -&gt; int:    if x == &quot;First Owner&quot;:        return 1    elif x == &quot;Second Owner&quot;:        return 2    elif x == &quot;Third Owner&quot;:        return 3</code></pre><h2 id="对owner进行label编码"><a href="#对owner进行label编码" class="headerlink" title="对owner进行label编码"></a>对owner进行label编码</h2><pre><code>data[&quot;owner&quot;].apply(func)data[&quot;owner&quot;].value_counts()First Owner     69Second Owner    26Third Owner      4Name: owner, dtype: int64</code></pre><h2 id="对其余文本属性，统一使用get-dummies方法进行onehot编码"><a href="#对其余文本属性，统一使用get-dummies方法进行onehot编码" class="headerlink" title="对其余文本属性，统一使用get_dummies方法进行onehot编码"></a>对其余文本属性，统一使用get_dummies方法进行onehot编码</h2><pre><code>data = pd.get_dummies(data)data</code></pre><table><thead><tr><th align="left"></th><th align="left">year</th><th align="left">selling_price</th><th align="left">km_driven</th><th align="left">mileage</th><th align="left">engine</th><th align="left">seats</th><th align="left">fuel_CNG</th><th align="left">fuel_Diesel</th><th align="left">fuel_LPG</th><th align="left">fuel_Petrol</th><th align="left">seller_type_Dealer</th><th align="left">seller_type_Individual</th><th align="left">transmission_Automatic</th><th align="left">transmission_Manual</th><th align="left">owner_First Owner</th><th align="left">owner_Second Owner</th><th align="left">owner_Third Owner</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">2014</td><td align="left">450000</td><td align="left">145500</td><td align="left">23.40</td><td align="left">1248.0</td><td align="left">5.0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">1</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">1</td><td align="left">2014</td><td align="left">370000</td><td align="left">120000</td><td align="left">21.14</td><td align="left">1498.0</td><td align="left">5.0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">0</td></tr><tr><td align="left">2</td><td align="left">2006</td><td align="left">158000</td><td align="left">140000</td><td align="left">17.70</td><td align="left">1497.0</td><td align="left">5.0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">1</td></tr><tr><td align="left">3</td><td align="left">2010</td><td align="left">225000</td><td align="left">127000</td><td align="left">23.00</td><td align="left">1396.0</td><td align="left">5.0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">1</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">4</td><td align="left">2007</td><td align="left">130000</td><td align="left">120000</td><td align="left">16.10</td><td align="left">1298.0</td><td align="left">5.0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">1</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td></tr><tr><td align="left">94</td><td align="left">2009</td><td align="left">175000</td><td align="left">55500</td><td align="left">18.20</td><td align="left">998.0</td><td align="left">5.0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">1</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">95</td><td align="left">2013</td><td align="left">525000</td><td align="left">61500</td><td align="left">18.50</td><td align="left">1197.0</td><td align="left">5.0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">1</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">96</td><td align="left">2016</td><td align="left">600000</td><td align="left">150000</td><td align="left">26.59</td><td align="left">1248.0</td><td align="left">5.0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">1</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">97</td><td align="left">2016</td><td align="left">565000</td><td align="left">72000</td><td align="left">19.10</td><td align="left">1197.0</td><td align="left">5.0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">98</td><td align="left">2008</td><td align="left">120000</td><td align="left">68000</td><td align="left">19.70</td><td align="left">796.0</td><td align="left">5.0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">1</td></tr></tbody></table><p>99 rows × 17 columns</p><h2 id="查看编码后的数据"><a href="#查看编码后的数据" class="headerlink" title="查看编码后的数据"></a>查看编码后的数据</h2><pre><code>data.info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 99 entries, 0 to 98Data columns (total 17 columns): #   Column                  Non-Null Count  Dtype  ---  ------                  --------------  -----   0   year                    99 non-null     int64   1   selling_price           99 non-null     int64   2   km_driven               99 non-null     int64   3   mileage                 94 non-null     float64 4   engine                  96 non-null     float64 5   seats                   94 non-null     float64 6   fuel_CNG                99 non-null     uint8   7   fuel_Diesel             99 non-null     uint8   8   fuel_LPG                99 non-null     uint8   9   fuel_Petrol             99 non-null     uint8   10  seller_type_Dealer      99 non-null     uint8   11  seller_type_Individual  99 non-null     uint8   12  transmission_Automatic  99 non-null     uint8   13  transmission_Manual     99 non-null     uint8   14  owner_First Owner       99 non-null     uint8   15  owner_Second Owner      99 non-null     uint8   16  owner_Third Owner       99 non-null     uint8  dtypes: float64(3), int64(3), uint8(11)memory usage: 5.8 KB</code></pre><h2 id="对每一列属性的缺失值个数进行求和统计"><a href="#对每一列属性的缺失值个数进行求和统计" class="headerlink" title="对每一列属性的缺失值个数进行求和统计"></a>对每一列属性的缺失值个数进行求和统计</h2><pre><code>data.isnull().sum(axis=0)year                      0selling_price             0km_driven                 0mileage                   5engine                    3seats                     5fuel_CNG                  0fuel_Diesel               0fuel_LPG                  0fuel_Petrol               0seller_type_Dealer        0seller_type_Individual    0transmission_Automatic    0transmission_Manual       0owner_First Owner         0owner_Second Owner        0owner_Third Owner         0dtype: int64</code></pre><h2 id="随机森林算法填充缺失值"><a href="#随机森林算法填充缺失值" class="headerlink" title="随机森林算法填充缺失值"></a>随机森林算法填充缺失值</h2><h3 id="先填充缺失值较少的的列，之后再填多的"><a href="#先填充缺失值较少的的列，之后再填多的" class="headerlink" title="先填充缺失值较少的的列，之后再填多的"></a>先填充缺失值较少的的列，之后再填多的</h3><h4 id="原理：将要填补的列作为目标列，其余列作为属性列，用随机森林预测目标列的值进行填充"><a href="#原理：将要填补的列作为目标列，其余列作为属性列，用随机森林预测目标列的值进行填充" class="headerlink" title="原理：将要填补的列作为目标列，其余列作为属性列，用随机森林预测目标列的值进行填充"></a>原理：将要填补的列作为目标列，其余列作为属性列，用随机森林预测目标列的值进行填充</h4><h4 id="用非空的行作为训练集，空的行作为测试集，训练集中的数据有空值，则先用0填充"><a href="#用非空的行作为训练集，空的行作为测试集，训练集中的数据有空值，则先用0填充" class="headerlink" title="用非空的行作为训练集，空的行作为测试集，训练集中的数据有空值，则先用0填充"></a>用非空的行作为训练集，空的行作为测试集，训练集中的数据有空值，则先用0填充</h4><pre><code># 引入随机森林模型和填补缺失值的模型from sklearn.impute import SimpleImputerfrom sklearn.ensemble import RandomForestRegressor</code></pre><h4 id="首先去除特定的列得到属性列，记为X；选取特定的列作为目标列，记为Y"><a href="#首先去除特定的列得到属性列，记为X；选取特定的列作为目标列，记为Y" class="headerlink" title="首先去除特定的列得到属性列，记为X；选取特定的列作为目标列，记为Y"></a>首先去除特定的列得到属性列，记为X；选取特定的列作为目标列，记为Y</h4><h4 id="在得到的属性列中，用0填补缺失值"><a href="#在得到的属性列中，用0填补缺失值" class="headerlink" title="在得到的属性列中，用0填补缺失值"></a>在得到的属性列中，用0填补缺失值</h4><h4 id="在目标列中选取非空的行的index作为选取训练集的依据，空行的index作为测试集的依据"><a href="#在目标列中选取非空的行的index作为选取训练集的依据，空行的index作为测试集的依据" class="headerlink" title="在目标列中选取非空的行的index作为选取训练集的依据，空行的index作为测试集的依据"></a>在目标列中选取非空的行的index作为选取训练集的依据，空行的index作为测试集的依据</h4><h4 id="这样就可以在X-Y中得到训练集和测试集了"><a href="#这样就可以在X-Y中得到训练集和测试集了" class="headerlink" title="这样就可以在X,Y中得到训练集和测试集了"></a>这样就可以在X,Y中得到训练集和测试集了</h4><h4 id="有了训练集就把它们丢到随机森林训练，然后用训练好的模型预测测试集中的数据得到预测值"><a href="#有了训练集就把它们丢到随机森林训练，然后用训练好的模型预测测试集中的数据得到预测值" class="headerlink" title="有了训练集就把它们丢到随机森林训练，然后用训练好的模型预测测试集中的数据得到预测值"></a>有了训练集就把它们丢到随机森林训练，然后用训练好的模型预测测试集中的数据得到预测值</h4><h4 id="最后将预测值填到相应的位置中"><a href="#最后将预测值填到相应的位置中" class="headerlink" title="最后将预测值填到相应的位置中"></a>最后将预测值填到相应的位置中</h4><pre><code>for name in [&quot;engine&quot;, &quot;mileage&quot;, &quot;seats&quot;]:    X = data.drop(columns=f&quot;&#123;name&#125;&quot;)    Y = data.loc[:, f&quot;&#123;name&#125;&quot;]    X_0 = SimpleImputer(missing_values=np.nan, strategy=&quot;constant&quot;).fit_transform(X)    y_train = Y[Y.notnull()]    y_test = Y[Y.isnull()]    x_train = X_0[y_train.index, :]    x_test = X_0[y_test.index, :]    rfc = RandomForestRegressor(n_estimators=100)    rfc = rfc.fit(x_train, y_train)    y_predict = rfc.predict(x_test)    data.loc[Y.isnull(), f&quot;&#123;name&#125;&quot;] = y_predict</code></pre><h2 id="查看填充后的数据"><a href="#查看填充后的数据" class="headerlink" title="查看填充后的数据"></a>查看填充后的数据</h2><pre><code>data.info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 99 entries, 0 to 98Data columns (total 17 columns): #   Column                  Non-Null Count  Dtype  ---  ------                  --------------  -----   0   year                    99 non-null     int64   1   selling_price           99 non-null     int64   2   km_driven               99 non-null     int64   3   mileage                 99 non-null     float64 4   engine                  99 non-null     float64 5   seats                   99 non-null     float64 6   fuel_CNG                99 non-null     uint8   7   fuel_Diesel             99 non-null     uint8   8   fuel_LPG                99 non-null     uint8   9   fuel_Petrol             99 non-null     uint8   10  seller_type_Dealer      99 non-null     uint8   11  seller_type_Individual  99 non-null     uint8   12  transmission_Automatic  99 non-null     uint8   13  transmission_Manual     99 non-null     uint8   14  owner_First Owner       99 non-null     uint8   15  owner_Second Owner      99 non-null     uint8   16  owner_Third Owner       99 non-null     uint8  dtypes: float64(3), int64(3), uint8(11)memory usage: 5.8 KB</code></pre><h3 id="可以发现原有的缺失值已经被填好了"><a href="#可以发现原有的缺失值已经被填好了" class="headerlink" title="可以发现原有的缺失值已经被填好了"></a>可以发现原有的缺失值已经被填好了</h3><h2 id="最后把结果导出为excel文件"><a href="#最后把结果导出为excel文件" class="headerlink" title="最后把结果导出为excel文件"></a>最后把结果导出为excel文件</h2><pre><code>data.to_excel(&quot;test-2(填补后).xlsx&quot;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随机森林 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RFM客户分类模型</title>
      <link href="/2021/12/27/RFM%E5%AE%A2%E6%88%B7%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2021/12/27/RFM%E5%AE%A2%E6%88%B7%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h5 id="今天为大家分析一个英国在线零售商的交易数据集"><a href="#今天为大家分析一个英国在线零售商的交易数据集" class="headerlink" title="今天为大家分析一个英国在线零售商的交易数据集"></a>今天为大家分析一个英国在线零售商的交易数据集</h5><p><img src="http://files.mdnice.com/user/21987/d5d9c81c-1b03-478e-b526-8274e397f31f.jpg" alt="img"></p><h5 id="着重为大家介绍一下如何运用RFM模型对客户进行分类"><a href="#着重为大家介绍一下如何运用RFM模型对客户进行分类" class="headerlink" title="着重为大家介绍一下如何运用RFM模型对客户进行分类"></a>着重为大家介绍一下如何运用RFM模型对客户进行分类</h5><p><img src="http://files.mdnice.com/user/21987/a56f5497-2a07-4201-8d09-8abba3ae3d9b.jpg" alt="img"> <img src="http://files.mdnice.com/user/21987/67da1fdd-5003-4999-b4b7-758958dd4ddc.jpg" alt="img"></p><h5 id="并且解决如下问题："><a href="#并且解决如下问题：" class="headerlink" title="并且解决如下问题："></a>并且解决如下问题：</h5><h5 id="1-以星期为单位，周几的销售额最高？请可视化显示。"><a href="#1-以星期为单位，周几的销售额最高？请可视化显示。" class="headerlink" title="1.以星期为单位，周几的销售额最高？请可视化显示。"></a>1.以星期为单位，周几的销售额最高？请可视化显示。</h5><h5 id="2-利用-RFM-模型，对-United-Kingdom-的用户进行分类，分为如下所述的六类。"><a href="#2-利用-RFM-模型，对-United-Kingdom-的用户进行分类，分为如下所述的六类。" class="headerlink" title="2.利用 RFM 模型，对 United Kingdom 的用户进行分类，分为如下所述的六类。"></a>2.利用 RFM 模型，对 United Kingdom 的用户进行分类，分为如下所述的六类。</h5><h5 id="a-选用合适的图形，可视化展示每一类用户数占总数的比例；可视化展示每一类用户消费额占总消费额的比例。"><a href="#a-选用合适的图形，可视化展示每一类用户数占总数的比例；可视化展示每一类用户消费额占总消费额的比例。" class="headerlink" title="a) 选用合适的图形，可视化展示每一类用户数占总数的比例；可视化展示每一类用户消费额占总消费额的比例。"></a>a) 选用合适的图形，可视化展示每一类用户数占总数的比例；可视化展示每一类用户消费额占总消费额的比例。</h5><h5 id="b-对每一类用户，显示其每个月的消费总额的变化情况。（显示在一个-figure-中"><a href="#b-对每一类用户，显示其每个月的消费总额的变化情况。（显示在一个-figure-中" class="headerlink" title="b) 对每一类用户，显示其每个月的消费总额的变化情况。（显示在一个 figure 中"></a>b) 对每一类用户，显示其每个月的消费总额的变化情况。（显示在一个 figure 中</h5><h4 id="以下是具体的代码"><a href="#以下是具体的代码" class="headerlink" title="以下是具体的代码"></a>以下是具体的代码</h4><pre><code>import pandas as pd</code></pre><h3 id="读取数据，并且对数据格式作出一些调整，方便后续分析"><a href="#读取数据，并且对数据格式作出一些调整，方便后续分析" class="headerlink" title="读取数据，并且对数据格式作出一些调整，方便后续分析"></a>读取数据，并且对数据格式作出一些调整，方便后续分析</h3><pre><code># 读取并且查看数据df = pd.read_csv(&quot;Online Retail(1).csv&quot;)df.head()</code></pre><table><thead><tr><th align="left"></th><th align="left">InvoiceNo</th><th align="left">StockCode</th><th align="left">Description</th><th align="left">Quantity</th><th align="left">InvoiceDate</th><th align="left">UnitPrice</th><th align="left">CustomerID</th><th align="left">Country</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">536365</td><td align="left">85123A</td><td align="left">WHITE HANGING HEART T-LIGHT HOLDER</td><td align="left">6</td><td align="left">2010&#x2F;12&#x2F;1 8:26</td><td align="left">2.55</td><td align="left">17850.0</td><td align="left">United Kingdom</td></tr><tr><td align="left">1</td><td align="left">536365</td><td align="left">71053</td><td align="left">WHITE METAL LANTERN</td><td align="left">6</td><td align="left">2010&#x2F;12&#x2F;1 8:26</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td></tr><tr><td align="left">2</td><td align="left">536365</td><td align="left">84406B</td><td align="left">CREAM CUPID HEARTS COAT HANGER</td><td align="left">8</td><td align="left">2010&#x2F;12&#x2F;1 8:26</td><td align="left">2.75</td><td align="left">17850.0</td><td align="left">United Kingdom</td></tr><tr><td align="left">3</td><td align="left">536365</td><td align="left">84029G</td><td align="left">KNITTED UNION FLAG HOT WATER BOTTLE</td><td align="left">6</td><td align="left">2010&#x2F;12&#x2F;1 8:26</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td></tr><tr><td align="left">4</td><td align="left">536365</td><td align="left">84029E</td><td align="left">RED WOOLLY HOTTIE WHITE HEART.</td><td align="left">6</td><td align="left">2010&#x2F;12&#x2F;1 8:26</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td></tr></tbody></table><pre><code># 将时间数据设置为pandas的数据格式df[&quot;InvoiceDate&quot;] = pd.to_datetime(df[&quot;InvoiceDate&quot;])# 注：由于最近一次购物(Recency)是针对某个时间点计算的，而最后订货日期是 2011-12-09，因此# 我们把 2011-12-10 当作今天，来计算 Recency。df[&quot;target_time&quot;] = pd.to_datetime(&quot;2011-12-10&quot;)# 计算 Recencydf[&quot;Recency&quot;] = pd.to_datetime(df[&quot;target_time&quot;]) - pd.to_datetime(df[&quot;InvoiceDate&quot;])# 计算每个订单的总金额df[&quot;total&quot;] = df[&quot;Quantity&quot;] * df[&quot;UnitPrice&quot;]df.head()</code></pre><table><thead><tr><th align="left"></th><th align="left">InvoiceNo</th><th align="left">StockCode</th><th align="left">Description</th><th align="left">Quantity</th><th align="left">InvoiceDate</th><th align="left">UnitPrice</th><th align="left">CustomerID</th><th align="left">Country</th><th align="left">target_time</th><th align="left">Recency</th><th align="left">total</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">536365</td><td align="left">85123A</td><td align="left">WHITE HANGING HEART T-LIGHT HOLDER</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">2.55</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">15.30</td></tr><tr><td align="left">1</td><td align="left">536365</td><td align="left">71053</td><td align="left">WHITE METAL LANTERN</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">20.34</td></tr><tr><td align="left">2</td><td align="left">536365</td><td align="left">84406B</td><td align="left">CREAM CUPID HEARTS COAT HANGER</td><td align="left">8</td><td align="left">2010-12-01 08:26:00</td><td align="left">2.75</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">22.00</td></tr><tr><td align="left">3</td><td align="left">536365</td><td align="left">84029G</td><td align="left">KNITTED UNION FLAG HOT WATER BOTTLE</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">20.34</td></tr><tr><td align="left">4</td><td align="left">536365</td><td align="left">84029E</td><td align="left">RED WOOLLY HOTTIE WHITE HEART.</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">20.34</td></tr></tbody></table><h4 id="设置分析所需的星期、月份等属性，方便按此聚合"><a href="#设置分析所需的星期、月份等属性，方便按此聚合" class="headerlink" title="设置分析所需的星期、月份等属性，方便按此聚合"></a>设置分析所需的星期、月份等属性，方便按此聚合</h4><pre><code>df[&quot;星期&quot;] = df[&quot;InvoiceDate&quot;].dt.dayofweek+1df[&quot;月份&quot;] = df[&quot;InvoiceDate&quot;].dt.monthdf.head()</code></pre><table><thead><tr><th align="left"></th><th align="left">InvoiceNo</th><th align="left">StockCode</th><th align="left">Description</th><th align="left">Quantity</th><th align="left">InvoiceDate</th><th align="left">UnitPrice</th><th align="left">CustomerID</th><th align="left">Country</th><th align="left">target_time</th><th align="left">Recency</th><th align="left">total</th><th align="left">星期</th><th align="left">月份</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">536365</td><td align="left">85123A</td><td align="left">WHITE HANGING HEART T-LIGHT HOLDER</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">2.55</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">15.30</td><td align="left">3</td><td align="left">12</td></tr><tr><td align="left">1</td><td align="left">536365</td><td align="left">71053</td><td align="left">WHITE METAL LANTERN</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">20.34</td><td align="left">3</td><td align="left">12</td></tr><tr><td align="left">2</td><td align="left">536365</td><td align="left">84406B</td><td align="left">CREAM CUPID HEARTS COAT HANGER</td><td align="left">8</td><td align="left">2010-12-01 08:26:00</td><td align="left">2.75</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">22.00</td><td align="left">3</td><td align="left">12</td></tr><tr><td align="left">3</td><td align="left">536365</td><td align="left">84029G</td><td align="left">KNITTED UNION FLAG HOT WATER BOTTLE</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">20.34</td><td align="left">3</td><td align="left">12</td></tr><tr><td align="left">4</td><td align="left">536365</td><td align="left">84029E</td><td align="left">RED WOOLLY HOTTIE WHITE HEART.</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">20.34</td><td align="left">3</td><td align="left">12</td></tr></tbody></table><h3 id="按星期聚合总销售额"><a href="#按星期聚合总销售额" class="headerlink" title="按星期聚合总销售额"></a>按星期聚合总销售额</h3><pre><code>df_by_w = df.groupby(&quot;星期&quot;).agg(&#123;&quot;total&quot;:&quot;sum&quot;&#125;).reset_index()df_by_w</code></pre><table><thead><tr><th align="left"></th><th align="left">星期</th><th align="left">total</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">1</td><td align="left">1588609.431</td></tr><tr><td align="left">1</td><td align="left">2</td><td align="left">1966182.791</td></tr><tr><td align="left">2</td><td align="left">3</td><td align="left">1734147.010</td></tr><tr><td align="left">3</td><td align="left">4</td><td align="left">2112519.000</td></tr><tr><td align="left">4</td><td align="left">5</td><td align="left">1540610.811</td></tr><tr><td align="left">5</td><td align="left">7</td><td align="left">805678.891</td></tr></tbody></table><pre><code>from pyecharts import options as optsfrom pyecharts.charts import Barbar = (    Bar()    .add_xaxis(df_by_w[&quot;星期&quot;].to_list())    .add_yaxis(&quot;销售额&quot;, df_by_w[&quot;total&quot;].to_list())    .set_global_opts(title_opts=opts.TitleOpts(title=&quot;销售额对比图&quot;)))</code></pre><h4 id="以星期为单位，周几的销售额最高？-答案是周四"><a href="#以星期为单位，周几的销售额最高？-答案是周四" class="headerlink" title="以星期为单位，周几的销售额最高？ 答案是周四"></a>以星期为单位，周几的销售额最高？ 答案是周四</h4><pre><code>bar.render_notebook()</code></pre><p><img src="http://files.mdnice.com/user/21987/0c7b93bd-3c36-4def-aff2-01690f791513.jpg" alt="img"></p><h4 id="利用-RFM-模型，对-United-Kingdom-的用户进行分类"><a href="#利用-RFM-模型，对-United-Kingdom-的用户进行分类" class="headerlink" title="利用 RFM 模型，对 United Kingdom 的用户进行分类"></a>利用 RFM 模型，对 United Kingdom 的用户进行分类</h4><h4 id="按客户ID聚合，对发票订单号进行计数，对每个发票的销售额进行汇总，对recency取最小值（作为该顾客的recency值）"><a href="#按客户ID聚合，对发票订单号进行计数，对每个发票的销售额进行汇总，对recency取最小值（作为该顾客的recency值）" class="headerlink" title="按客户ID聚合，对发票订单号进行计数，对每个发票的销售额进行汇总，对recency取最小值（作为该顾客的recency值）"></a>按客户ID聚合，对发票订单号进行计数，对每个发票的销售额进行汇总，对recency取最小值（作为该顾客的recency值）</h4><pre><code>df_by_c = df.groupby(&quot;CustomerID&quot;).agg(&#123;&quot;InvoiceNo&quot;:&quot;count&quot;, &quot;total&quot;:&quot;sum&quot;, &quot;Recency&quot;:&quot;min&quot;&#125;).reset_index()df_by_c.columns = [&quot;CustomerID&quot;,&quot;Frequency&quot;,&quot;Monetary&quot;,&quot;Recency&quot;]df_by_c.head()</code></pre><table><thead><tr><th align="left"></th><th align="left">CustomerID</th><th align="left">Frequency</th><th align="left">Monetary</th><th align="left">Recency</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">12346.0</td><td align="left">2</td><td align="left">0.00</td><td align="left">325 days 13:43:00</td></tr><tr><td align="left">1</td><td align="left">12347.0</td><td align="left">182</td><td align="left">4310.00</td><td align="left">2 days 08:08:00</td></tr><tr><td align="left">2</td><td align="left">12348.0</td><td align="left">31</td><td align="left">1797.24</td><td align="left">75 days 10:47:00</td></tr><tr><td align="left">3</td><td align="left">12349.0</td><td align="left">73</td><td align="left">1757.55</td><td align="left">18 days 14:09:00</td></tr><tr><td align="left">4</td><td align="left">12350.0</td><td align="left">17</td><td align="left">334.40</td><td align="left">310 days 07:59:00</td></tr></tbody></table><h3 id="求出各列的四分位数"><a href="#求出各列的四分位数" class="headerlink" title="求出各列的四分位数"></a>求出各列的四分位数</h3><pre><code>df_by_c[&quot;Frequency&quot;].quantile([0.25,0.5,0.75])0.25     17.00.50     42.00.75    102.0Name: Frequency, dtype: float64df_by_c[&quot;Monetary&quot;].quantile([0.25,0.5,0.75])0.25     293.36250.50     648.07500.75    1611.7250Name: Monetary, dtype: float64</code></pre><h5 id="将Recency这列的数据格式转为整数（取day这个属性就可以了）"><a href="#将Recency这列的数据格式转为整数（取day这个属性就可以了）" class="headerlink" title="将Recency这列的数据格式转为整数（取day这个属性就可以了）"></a>将Recency这列的数据格式转为整数（取day这个属性就可以了）</h5><pre><code>df_by_c[&quot;Recency&quot;] = df_by_c[&quot;Recency&quot;].dt.daysdf_by_c[&quot;Recency&quot;].quantile([0.25,0.5,0.75])0.25     16.00.50     50.00.75    143.0Name: Recency, dtype: float64</code></pre><h4 id="按照刚才求得的各项指标的四分位数，制定相应的函数，计算RFM的值"><a href="#按照刚才求得的各项指标的四分位数，制定相应的函数，计算RFM的值" class="headerlink" title="按照刚才求得的各项指标的四分位数，制定相应的函数，计算RFM的值"></a>按照刚才求得的各项指标的四分位数，制定相应的函数，计算RFM的值</h4><pre><code>def decide_F(score):    if score&gt;102:        return 1    elif score&gt;42:        return 2    elif score&gt;17:        return 3    else:        return 4    def decide_M(score):    if score&gt;1611.725:        return 1    elif score&gt;648.075:        return 2    elif score&gt;293.3625:        return 3    else:        return 4    def decide_R(score):    if score&lt;16:        return 1    elif score&lt;50:        return 2    elif score&lt;143:        return 3    else:        return 4df_by_c[&quot;F&quot;] = df_by_c.apply(lambda x: decide_F(x.Frequency), axis = 1)df_by_c[&quot;M&quot;] = df_by_c.apply(lambda x: decide_F(x.Monetary), axis = 1)df_by_c[&quot;R&quot;] = df_by_c.apply(lambda x: decide_R(x.Recency), axis = 1)</code></pre><h4 id="看一下求得的值"><a href="#看一下求得的值" class="headerlink" title="看一下求得的值"></a>看一下求得的值</h4><pre><code>df_by_c.head()</code></pre><table><thead><tr><th align="left"></th><th align="left">CustomerID</th><th align="left">Frequency</th><th align="left">Monetary</th><th align="left">Recency</th><th align="left">F</th><th align="left">M</th><th align="left">R</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">12346.0</td><td align="left">2</td><td align="left">0.00</td><td align="left">325</td><td align="left">4</td><td align="left">4</td><td align="left">4</td></tr><tr><td align="left">1</td><td align="left">12347.0</td><td align="left">182</td><td align="left">4310.00</td><td align="left">2</td><td align="left">1</td><td align="left">1</td><td align="left">1</td></tr><tr><td align="left">2</td><td align="left">12348.0</td><td align="left">31</td><td align="left">1797.24</td><td align="left">75</td><td align="left">3</td><td align="left">1</td><td align="left">3</td></tr><tr><td align="left">3</td><td align="left">12349.0</td><td align="left">73</td><td align="left">1757.55</td><td align="left">18</td><td align="left">2</td><td align="left">1</td><td align="left">2</td></tr><tr><td align="left">4</td><td align="left">12350.0</td><td align="left">17</td><td align="left">334.40</td><td align="left">310</td><td align="left">4</td><td align="left">1</td><td align="left">4</td></tr></tbody></table><h3 id="根据定义，制定对客户进行类型划分的函数"><a href="#根据定义，制定对客户进行类型划分的函数" class="headerlink" title="根据定义，制定对客户进行类型划分的函数"></a>根据定义，制定对客户进行类型划分的函数</h3><pre><code>def decide_type(f, m, r):    if f == 1 and m == 1 and r == 1:        return &quot;最佳客户&quot;    if f == 1 and m == 1 and r == 3:        return &quot;近流失客户&quot;    if f == 1 and m == 1 and r == 4:        return &quot;流失客户&quot;    if f == 4 and m == 4 and r == 4:        return &quot;流失廉价客户&quot;    if f == 1:        return &quot;忠诚客户&quot;    if m == 1:        return &quot;大金主&quot;            df_by_c[&quot;客户类型&quot;] =  df_by_c.apply(lambda x: decide_type(x.F, x.M ,x.R), axis = 1)df_by_c.head()</code></pre><table><thead><tr><th align="left"></th><th align="left">CustomerID</th><th align="left">Frequency</th><th align="left">Monetary</th><th align="left">Recency</th><th align="left">F</th><th align="left">M</th><th align="left">R</th><th align="left">客户类型</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">12346.0</td><td align="left">2</td><td align="left">0.00</td><td align="left">325</td><td align="left">4</td><td align="left">4</td><td align="left">4</td><td align="left">流失廉价客户</td></tr><tr><td align="left">1</td><td align="left">12347.0</td><td align="left">182</td><td align="left">4310.00</td><td align="left">2</td><td align="left">1</td><td align="left">1</td><td align="left">1</td><td align="left">最佳客户</td></tr><tr><td align="left">2</td><td align="left">12348.0</td><td align="left">31</td><td align="left">1797.24</td><td align="left">75</td><td align="left">3</td><td align="left">1</td><td align="left">3</td><td align="left">大金主</td></tr><tr><td align="left">3</td><td align="left">12349.0</td><td align="left">73</td><td align="left">1757.55</td><td align="left">18</td><td align="left">2</td><td align="left">1</td><td align="left">2</td><td align="left">大金主</td></tr><tr><td align="left">4</td><td align="left">12350.0</td><td align="left">17</td><td align="left">334.40</td><td align="left">310</td><td align="left">4</td><td align="left">1</td><td align="left">4</td><td align="left">大金主</td></tr></tbody></table><pre><code>df_by_c_count = df_by_c.groupby(&quot;客户类型&quot;).size().sort_values(ascending=False)df_by_c_count客户类型大金主       3064最佳客户       533忠诚客户       347近流失客户      154流失客户        47流失廉价客户      39dtype: int64datas = list(zip(df_by_c_count.index.to_list(), df_by_c_count.to_list()))datas[(&#39;大金主&#39;, 3064), (&#39;最佳客户&#39;, 533), (&#39;忠诚客户&#39;, 347), (&#39;近流失客户&#39;, 154), (&#39;流失客户&#39;, 47), (&#39;流失廉价客户&#39;, 39)]from pyecharts import options as optsfrom pyecharts.charts import Piedef create_pie(datas, title) -&gt; Pie:    &quot;&quot;&quot; 创建饼图对象    文档地址：https://pyecharts.org/#/zh-cn/basic_charts?id=pie%ef%bc%9a%e9%a5%bc%e5%9b%be    @param datas: 数据，形式为[(&#39;类型1&#39;, 数据1), (&#39;类型2&#39;, 数据2), (&#39;类型3&#39;, 数据3)]    @param title: 图表的标题    &quot;&quot;&quot;    pie = Pie()    pie.add(&quot;&quot;, datas)    pie.set_global_opts(        title_opts=opts.TitleOpts(title=title),        legend_opts=opts.LegendOpts(pos_right=&quot;right&quot;)    )    pie.set_series_opts(label_opts=opts.LabelOpts(formatter=&quot;&#123;b&#125;: &#123;c&#125;: &#123;d&#125;%&quot;))    return pie</code></pre><h3 id="a）可视化展示每一类用户数占总数的比例"><a href="#a）可视化展示每一类用户数占总数的比例" class="headerlink" title="a）可视化展示每一类用户数占总数的比例"></a>a）可视化展示每一类用户数占总数的比例</h3><pre><code>pie = create_pie(datas, &quot;饼图-客户类型对比&quot;)pie.render_notebook()</code></pre><p><img src="http://files.mdnice.com/user/21987/5d0a189e-0d19-4817-8d1d-9ed9c1b471d9.jpg" alt="img"></p><h4 id="取出”CustomerID”-“客户类型”这两列，准备与df合并"><a href="#取出”CustomerID”-“客户类型”这两列，准备与df合并" class="headerlink" title="取出”CustomerID”,  “客户类型”这两列，准备与df合并"></a>取出”CustomerID”,  “客户类型”这两列，准备与df合并</h4><pre><code>df1 = df_by_c[[&quot;CustomerID&quot;,  &quot;客户类型&quot;]]new = pd.merge(df, df1, on=&quot;CustomerID&quot;)new.head()</code></pre><table><thead><tr><th align="left"></th><th align="left">InvoiceNo</th><th align="left">StockCode</th><th align="left">Description</th><th align="left">Quantity</th><th align="left">InvoiceDate</th><th align="left">UnitPrice</th><th align="left">CustomerID</th><th align="left">Country</th><th align="left">target_time</th><th align="left">Recency</th><th align="left">total</th><th align="left">星期</th><th align="left">月份</th><th align="left">客户类型</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">536365</td><td align="left">85123A</td><td align="left">WHITE HANGING HEART T-LIGHT HOLDER</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">2.55</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">15.30</td><td align="left">3</td><td align="left">12</td><td align="left">流失客户</td></tr><tr><td align="left">1</td><td align="left">536365</td><td align="left">71053</td><td align="left">WHITE METAL LANTERN</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">20.34</td><td align="left">3</td><td align="left">12</td><td align="left">流失客户</td></tr><tr><td align="left">2</td><td align="left">536365</td><td align="left">84406B</td><td align="left">CREAM CUPID HEARTS COAT HANGER</td><td align="left">8</td><td align="left">2010-12-01 08:26:00</td><td align="left">2.75</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">22.00</td><td align="left">3</td><td align="left">12</td><td align="left">流失客户</td></tr><tr><td align="left">3</td><td align="left">536365</td><td align="left">84029G</td><td align="left">KNITTED UNION FLAG HOT WATER BOTTLE</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">20.34</td><td align="left">3</td><td align="left">12</td><td align="left">流失客户</td></tr><tr><td align="left">4</td><td align="left">536365</td><td align="left">84029E</td><td align="left">RED WOOLLY HOTTIE WHITE HEART.</td><td align="left">6</td><td align="left">2010-12-01 08:26:00</td><td align="left">3.39</td><td align="left">17850.0</td><td align="left">United Kingdom</td><td align="left">2011-12-10</td><td align="left">373 days 15:34:00</td><td align="left">20.34</td><td align="left">3</td><td align="left">12</td><td align="left">流失客户</td></tr></tbody></table><pre><code>by_c_t = new.groupby(&quot;客户类型&quot;)[&quot;total&quot;].sum().sort_values(ascending=False)by_c_t客户类型最佳客户      4106703.970大金主       2472418.943忠诚客户      1270195.150近流失客户      364926.870流失客户        86803.711流失廉价客户     -11011.550Name: total, dtype: float64datas = list(zip(by_c_t.index.to_list(), by_c_t.to_list()))datas[(&#39;最佳客户&#39;, 4106703.970000173), (&#39;大金主&#39;, 2472418.9429999804), (&#39;忠诚客户&#39;, 1270195.1499999189), (&#39;近流失客户&#39;, 364926.870000011), (&#39;流失客户&#39;, 86803.71099999893), (&#39;流失廉价客户&#39;, -11011.549999999996)]</code></pre><h3 id="b-可视化展示每一类用户消费额占总消费额的比例"><a href="#b-可视化展示每一类用户消费额占总消费额的比例" class="headerlink" title="b) 可视化展示每一类用户消费额占总消费额的比例"></a>b) 可视化展示每一类用户消费额占总消费额的比例</h3><pre><code>pie = create_pie(datas, &quot;饼图-客户销售额对比&quot;)pie.render_notebook()</code></pre><p><img src="http://files.mdnice.com/user/21987/81c67dfb-da61-4f6f-b911-0f540e76f2d2.jpg" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 数据分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用pyecharts构建仪表板显示多图</title>
      <link href="/2021/12/01/%E7%94%A8pyecharts%E6%9E%84%E5%BB%BA%E4%BB%AA%E8%A1%A8%E6%9D%BF%E6%98%BE%E7%A4%BA%E5%A4%9A%E5%9B%BE/"/>
      <url>/2021/12/01/%E7%94%A8pyecharts%E6%9E%84%E5%BB%BA%E4%BB%AA%E8%A1%A8%E6%9D%BF%E6%98%BE%E7%A4%BA%E5%A4%9A%E5%9B%BE/</url>
      
        <content type="html"><![CDATA[<h5 id="学习完蚂蚁老师的pyecharts数据可视化课程后，受益匪浅！"><a href="#学习完蚂蚁老师的pyecharts数据可视化课程后，受益匪浅！" class="headerlink" title="学习完蚂蚁老师的pyecharts数据可视化课程后，受益匪浅！"></a>学习完蚂蚁老师的pyecharts数据可视化课程后，受益匪浅！</h5><h6 id="但是如何让pyecharts的图以自己想要的排版方式显示在网页上呢？"><a href="#但是如何让pyecharts的图以自己想要的排版方式显示在网页上呢？" class="headerlink" title="但是如何让pyecharts的图以自己想要的排版方式显示在网页上呢？"></a>但是如何让pyecharts的图以自己想要的排版方式显示在网页上呢？</h6><h5 id="就像这样"><a href="#就像这样" class="headerlink" title="就像这样"></a>就像这样<img src="http://files.mdnice.com/user/21987/2d7ffa58-0a3c-42e2-ae40-9cbb11026ec1.png" alt="img"></h5><h5 id="看完这篇文章你就能学会！"><a href="#看完这篇文章你就能学会！" class="headerlink" title="看完这篇文章你就能学会！"></a>看完这篇文章你就能学会！</h5><h4 id="一：将你的生成pyecharts可视化图像的相关代码包装成一个函数。"><a href="#一：将你的生成pyecharts可视化图像的相关代码包装成一个函数。" class="headerlink" title="一：将你的生成pyecharts可视化图像的相关代码包装成一个函数。"></a>一：将你的生成pyecharts可视化图像的相关代码包装成一个函数。</h4><p>类似这样的结构</p><pre><code>def func() -&gt; 图的对象名字  构造对象  return 对象</code></pre><p>下面的代码是生成一个WordCloud词云的对象，并且返回这个对象。</p><pre><code>def word_cloud() -&gt; WordCloud: c = (  WordCloud()   .add(&quot;&quot;, word, word_size_range=[20, 100], shape=SymbolType.DIAMOND)   .set_global_opts(title_opts=opts.TitleOpts(title=&quot;快时尚行业销售排行&quot;)) ) return c</code></pre><h4 id="二：从-pyecharts-charts-导入-Page"><a href="#二：从-pyecharts-charts-导入-Page" class="headerlink" title="二：从 pyecharts.charts 导入 Page"></a>二：从 pyecharts.charts 导入 Page</h4><pre><code>from pyecharts.charts import Page</code></pre><p>写完画图的函数之后，加上这几行代码</p><pre><code>page = Page(layout=Page.DraggablePageLayout, page_title=&quot;商户视角——行业分析&quot;)page.add( word_cloud(), bar_tool(), scat(), uVSz(),)page.render(&#39;商户视角——行业分析.html&#39;)</code></pre><h5 id="page-title的值你可以随便改"><a href="#page-title的值你可以随便改" class="headerlink" title="page_title的值你可以随便改"></a>page_title的值你可以随便改</h5><h5 id="page-add-里面填的就是你自己写的画图函数，按上面的格式添加进去"><a href="#page-add-里面填的就是你自己写的画图函数，按上面的格式添加进去" class="headerlink" title="page.add()里面填的就是你自己写的画图函数，按上面的格式添加进去"></a>page.add()里面填的就是你自己写的画图函数，按上面的格式添加进去</h5><h5 id="最后-page-render-‘XXXX-html’-这行代码之后过后会生成一个名叫XXXX的html文件"><a href="#最后-page-render-‘XXXX-html’-这行代码之后过后会生成一个名叫XXXX的html文件" class="headerlink" title="最后 page.render(‘XXXX.html’)  这行代码之后过后会生成一个名叫XXXX的html文件"></a>最后 page.render(‘XXXX.html’)  这行代码之后过后会生成一个名叫XXXX的html文件</h5><p>打开这个文件，里面就是仪表板的初始效果了 <img src="http://files.mdnice.com/user/21987/4a385972-4450-48f6-a63b-1030987eaf2d.png" alt="img"></p><h4 id="三：根据你的意愿，对图片进行排版"><a href="#三：根据你的意愿，对图片进行排版" class="headerlink" title="三：根据你的意愿，对图片进行排版"></a>三：根据你的意愿，对图片进行排版</h4><p>你可以用鼠标点击图片拖拽，可以用滚轮缩放图像比例等等</p><h4 id="四：保存你排版好的html文件"><a href="#四：保存你排版好的html文件" class="headerlink" title="四：保存你排版好的html文件"></a>四：保存你排版好的html文件</h4><p>点击<img src="http://files.mdnice.com/user/21987/ae0b4a93-4e99-4baf-ac69-65e607ee2259.jpg" alt="img"> 它就会自动生成一个json文件 <img src="https://files.mdnice.com/user/21987/9a6ab7d0-c613-423d-b3b4-05eae314d860.jpg" alt="img"> 把这个json文件放到你代码的目录下</p><h4 id="五：形成自定义布局"><a href="#五：形成自定义布局" class="headerlink" title="五：形成自定义布局"></a>五：形成自定义布局</h4><pre><code>Page.save_resize_html(&quot;源.html&quot;,  cfg_file=&quot;chart_config.json&quot;,  dest=&quot;自定义布局.html&quot;)</code></pre><p>执行上述代码，三个参数分别是，原先生成的html文件，保存布局产生的json文件以及你想要生成的自定义布局的html文件</p><p>最后打开dest参数指定的html文件，你就会发现做好了一张由你布局的仪表板啦！</p>]]></content>
      
      
      <categories>
          
          <category> 数据可视化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pyecharts </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>不会前端也能开发网页啦</title>
      <link href="/2021/11/30/%E4%B8%8D%E4%BC%9A%E5%89%8D%E7%AB%AF%E4%B9%9F%E8%83%BD%E5%BC%80%E5%8F%91%E7%BD%91%E9%A1%B5%E5%95%A6/"/>
      <url>/2021/11/30/%E4%B8%8D%E4%BC%9A%E5%89%8D%E7%AB%AF%E4%B9%9F%E8%83%BD%E5%BC%80%E5%8F%91%E7%BD%91%E9%A1%B5%E5%95%A6/</url>
      
        <content type="html"><![CDATA[<h4 id="提起网页制作，可能很多人就想到了繁杂的html、css、JavaScript等内容，但是如今用python也可实现网页制作，今天就为大家介绍这个神器——PyWebIO。"><a href="#提起网页制作，可能很多人就想到了繁杂的html、css、JavaScript等内容，但是如今用python也可实现网页制作，今天就为大家介绍这个神器——PyWebIO。" class="headerlink" title="提起网页制作，可能很多人就想到了繁杂的html、css、JavaScript等内容，但是如今用python也可实现网页制作，今天就为大家介绍这个神器——PyWebIO。"></a>提起网页制作，可能很多人就想到了繁杂的html、css、JavaScript等内容，但是如今用python也可实现网页制作，今天就为大家介绍这个神器——PyWebIO。</h4><h4 id="首先我们要安装它：pip3-install-U-pywebio"><a href="#首先我们要安装它：pip3-install-U-pywebio" class="headerlink" title="首先我们要安装它：pip3 install -U pywebio"></a>首先我们要安装它：pip3 install -U pywebio</h4><h4 id="让我们看看它的官方文档"><a href="#让我们看看它的官方文档" class="headerlink" title="让我们看看它的官方文档"></a>让我们看看它的官方文档</h4><p><a href="https://pywebio.readthedocs.io/zh_CN/latest/">https://pywebio.readthedocs.io/zh_CN/latest/</a> <img src="http://files.mdnice.com/user/21987/72b61bfb-cb7b-4639-ace2-e7fa864cb277.jpg" alt="img"></p><h5 id="用这个helloworld的代码看一下它的效果吧"><a href="#用这个helloworld的代码看一下它的效果吧" class="headerlink" title="用这个helloworld的代码看一下它的效果吧~"></a>用这个helloworld的代码看一下它的效果吧~</h5><pre><code>from pywebio.input import input, FLOATfrom pywebio.output import put_textdef bmi():    height = input(&quot;请输入你的身高(cm)：&quot;, type=FLOAT)    weight = input(&quot;请输入你的体重(kg)：&quot;, type=FLOAT)    BMI = weight / (height / 100) ** 2    top_status = [(14.9, &#39;极瘦&#39;), (18.4, &#39;偏瘦&#39;),                  (22.9, &#39;正常&#39;), (27.5, &#39;过重&#39;),                  (40.0, &#39;肥胖&#39;), (float(&#39;inf&#39;), &#39;非常肥胖&#39;)]    for top, status in top_status:        if BMI &lt;= top:            put_text(&#39;你的 BMI 值: %.1f，身体状态：%s&#39; % (BMI, status))            breakif __name__ == &#39;__main__&#39;:    bmi()</code></pre><p><img src="http://files.mdnice.com/user/21987/d8bff3c0-6644-47ca-bb3b-31ea404aceb2.jpg" alt="img"> <img src="http://files.mdnice.com/user/21987/a49db040-e1cb-42b3-bb55-2c244f7b2084.jpg" alt="img"> <img src="https://files.mdnice.com/user/21987/f8e75003-e407-4f66-ab02-1829bc5d7a27.jpg" alt="img"></p><h3 id="可以发现，它随机打开了一个端口运行这个程序，身高和体重的输入框是分步显示的，比较麻烦"><a href="#可以发现，它随机打开了一个端口运行这个程序，身高和体重的输入框是分步显示的，比较麻烦" class="headerlink" title="可以发现，它随机打开了一个端口运行这个程序，身高和体重的输入框是分步显示的，比较麻烦"></a>可以发现，它随机打开了一个端口运行这个程序，身高和体重的输入框是分步显示的，比较麻烦</h3><h4 id="接下来我们通过start-server函数让程序运行在指定的端口号上，并且通过input-group组件让身高和体重的输入框能够同时显示"><a href="#接下来我们通过start-server函数让程序运行在指定的端口号上，并且通过input-group组件让身高和体重的输入框能够同时显示" class="headerlink" title="接下来我们通过start_server函数让程序运行在指定的端口号上，并且通过input_group组件让身高和体重的输入框能够同时显示"></a>接下来我们通过start_server函数让程序运行在指定的端口号上，并且通过input_group组件让身高和体重的输入框能够同时显示</h4><h4 id="把前面的示例改造为如下代码"><a href="#把前面的示例改造为如下代码" class="headerlink" title="把前面的示例改造为如下代码"></a>把前面的示例改造为如下代码</h4><pre><code>from pywebio.input import input, FLOAT, input_groupfrom pywebio.output import put_textfrom pywebio import start_serverdef bmi():    inputs = input_group(        label=&quot;请输入您的身高和体重&quot;,        inputs=[            input(&quot;请输入你的身高(cm)：&quot;, type=FLOAT, name=&quot;height&quot;),            input(&quot;请输入你的体重(kg)：&quot;, type=FLOAT, name=&quot;weight&quot;),        ]    )    height = inputs[&quot;身高&quot;]    weight = inputs[&quot;体重&quot;]    BMI = weight / (height / 100) ** 2    top_status = [(14.9, &#39;极瘦&#39;), (18.4, &#39;偏瘦&#39;),                  (22.9, &#39;正常&#39;), (27.5, &#39;过重&#39;),                  (40.0, &#39;肥胖&#39;), (float(&#39;inf&#39;), &#39;非常肥胖&#39;)]    for top, status in top_status:        if BMI &lt;= top:            put_text(&#39;你的 BMI 值: %.1f，身体状态：%s&#39; % (BMI, status))            breakif __name__ == &#39;__main__&#39;:    start_server(bmi, 8000)</code></pre><p><img src="http://files.mdnice.com/user/21987/92450830-c482-49b4-a156-572f4f749f38.jpg" alt="img"></p><h4 id="可以发现，达到了我们的目的"><a href="#可以发现，达到了我们的目的" class="headerlink" title="可以发现，达到了我们的目的"></a>可以发现，达到了我们的目的</h4><h4 id="除了官方文档的例子，我们还可以用它做更有趣的东西，比如结合pyecharts进行在线图表的展示！代码如下："><a href="#除了官方文档的例子，我们还可以用它做更有趣的东西，比如结合pyecharts进行在线图表的展示！代码如下：" class="headerlink" title="除了官方文档的例子，我们还可以用它做更有趣的东西，比如结合pyecharts进行在线图表的展示！代码如下："></a>除了官方文档的例子，我们还可以用它做更有趣的东西，比如结合pyecharts进行在线图表的展示！代码如下：</h4><pre><code>from pywebio.input import input, FLOAT, input_groupfrom pywebio.output import put_htmlfrom pywebio import start_serverfrom pyecharts.charts import Barfrom pyecharts import options as optsdef draw_bar(inputs): bar = (  Bar()   .add_xaxis([key for key, _ in inputs.items()])   .add_yaxis(&quot;商家&quot;, [value for _, value in inputs.items()])   .set_global_opts(title_opts=opts.TitleOpts(title=&quot;商品销量对比图&quot;)) ) return bar.render_notebook()def demo(): inputs = input_group(  label=&quot;请输入商家的各项数据&quot;,  inputs=[   input(&quot;裤子：&quot;, type=FLOAT, name=&quot;k&quot;),   input(&quot;高跟鞋：&quot;, type=FLOAT, name=&quot;g&quot;),   input(&quot;袜子：&quot;, type=FLOAT, name=&quot;w&quot;),  ] ) put_html(draw_bar(inputs))if __name__ == &#39;__main__&#39;: start_server(demo, 8000)</code></pre><h4 id="现在我们就可以通过输入框输入自定义的数据结合pyecharts来画图啦，效果如下"><a href="#现在我们就可以通过输入框输入自定义的数据结合pyecharts来画图啦，效果如下" class="headerlink" title="现在我们就可以通过输入框输入自定义的数据结合pyecharts来画图啦，效果如下"></a>现在我们就可以通过输入框输入自定义的数据结合pyecharts来画图啦，效果如下</h4><p><img src="http://files.mdnice.com/user/21987/5678d42f-b410-413b-8a73-b590b0514030.jpg" alt="img"> <img src="https://files.mdnice.com/user/21987/5b8b8149-4d15-4562-8923-d55e93aee536.jpg" alt="img"></p><h4 id="除了从输入框中输入数据，还可以通过文件上传的方式上传数据"><a href="#除了从输入框中输入数据，还可以通过文件上传的方式上传数据" class="headerlink" title="除了从输入框中输入数据，还可以通过文件上传的方式上传数据"></a>除了从输入框中输入数据，还可以通过文件上传的方式上传数据</h4><h4 id="下面我们这个案例就是上传文件、读取数据并画图-数据来自于蚂蚁老师的可视化课程"><a href="#下面我们这个案例就是上传文件、读取数据并画图-数据来自于蚂蚁老师的可视化课程" class="headerlink" title="下面我们这个案例就是上传文件、读取数据并画图(数据来自于蚂蚁老师的可视化课程)"></a>下面我们这个案例就是上传文件、读取数据并画图(数据来自于蚂蚁老师的可视化课程)</h4><pre><code>from pywebio.input import input_group, file_uploadfrom pywebio.output import put_htmlfrom pywebio import start_serverfrom pyecharts.charts import Funnelimport pandas as pddef demo(): inputs = input_group(  label=&quot;请各项上传数据&quot;,  inputs=[   file_upload(name=&quot;home&quot;),   file_upload(name=&quot;search&quot;),   file_upload(name=&quot;user&quot;),   file_upload(name=&quot;pay&quot;),   file_upload(name=&quot;confirm&quot;),  ] ) df_home_page = pd.read_csv(inputs[&quot;home&quot;][&#39;filename&#39;]) df_search_page = pd.read_csv(inputs[&quot;search&quot;][&#39;filename&#39;]) df_user_table = pd.read_csv(inputs[&quot;user&quot;][&#39;filename&#39;]) df_payment_page = pd.read_csv(inputs[&quot;pay&quot;][&#39;filename&#39;]) df_payment_confirmation_page = pd.read_csv(inputs[&quot;confirm&quot;][&#39;filename&#39;]) df_merge = df_user_table for df_inter in [df_home_page, df_search_page, df_payment_page, df_payment_confirmation_page]:  # 每次循环都会往df_merge中添加新列  df_merge = pd.merge(   left=df_merge,   right=df_inter,   left_on=&quot;user_id&quot;,   right_on=&quot;user_id&quot;,   how=&quot;left&quot;  ) df_merge.columns = [  &quot;user_id&quot;, &quot;date&quot;, &quot;device&quot;, &quot;sex&quot;,  &quot;home_page&quot;, &quot;search_page&quot;, &quot;payment_page&quot;, &quot;confirmation_page&quot;] datas = [] for column in [&quot;home_page&quot;, &quot;search_page&quot;, &quot;payment_page&quot;, &quot;confirmation_page&quot;]:  user_count = df_merge[column].dropna().size  datas.append((column, user_count)) # 方便查看对比，进行归一化 max_count = datas[0][1] datas_norm = [  (x, round(y * 100 / max_count, 2))  for x, y in datas ] funnel = Funnel() funnel.add(&quot;用户比例&quot;, datas_norm) put_html(funnel.render_notebook())if __name__ == &#39;__main__&#39;: start_server(demo, 8000)</code></pre><h4 id="效果如下"><a href="#效果如下" class="headerlink" title="效果如下"></a>效果如下</h4><p><img src="http://files.mdnice.com/user/21987/9466bc81-c020-48b3-aace-8aff45913bac.jpg" alt="img"> <img src="http://files.mdnice.com/user/21987/9180dfe4-b102-41a6-9671-67908bda7735.jpg" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Web开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pywebio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>金融时间序列数据处理</title>
      <link href="/2021/11/10/%E9%87%91%E8%9E%8D%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
      <url>/2021/11/10/%E9%87%91%E8%9E%8D%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h5 id="今天遇到一个关于金融背景的时间序列数据分析计算的需求，其中的分析处理方案非常具有代表性，所以分享给大家。"><a href="#今天遇到一个关于金融背景的时间序列数据分析计算的需求，其中的分析处理方案非常具有代表性，所以分享给大家。" class="headerlink" title="今天遇到一个关于金融背景的时间序列数据分析计算的需求，其中的分析处理方案非常具有代表性，所以分享给大家。"></a>今天遇到一个关于金融背景的时间序列数据分析计算的需求，其中的分析处理方案非常具有代表性，所以分享给大家。</h5><h5 id="数据大概长这个样子，其中Unnamed-0这一列代表时间"><a href="#数据大概长这个样子，其中Unnamed-0这一列代表时间" class="headerlink" title="数据大概长这个样子，其中Unnamed: 0这一列代表时间"></a>数据大概长这个样子，其中Unnamed: 0这一列代表时间</h5><pre><code class="python">import numpy as npimport pandas as pddata = pd.read_excel(&quot;a1.xlsx&quot;)data.head(3)</code></pre><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Unnamed: 0</th>      <th>000016.SH</th>      <th>CBA02501.CS</th>      <th>000832.CSI</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>2016-01-04</td>      <td>2270.4609</td>      <td>171.8353</td>      <td>312.9428</td>    </tr>    <tr>      <th>1</th>      <td>2016-01-05</td>      <td>2288.1127</td>      <td>171.5916</td>      <td>309.5110</td>    </tr>    <tr>      <th>2</th>      <td>2016-01-06</td>      <td>2317.6465</td>      <td>171.6981</td>      <td>310.2719</td>    </tr>  </tbody></table><h5 id="客户需要按日、周、月、季度计算各列数据百分比变化率的协方差以及期望值。"><a href="#客户需要按日、周、月、季度计算各列数据百分比变化率的协方差以及期望值。" class="headerlink" title="客户需要按日、周、月、季度计算各列数据百分比变化率的协方差以及期望值。"></a>客户需要按日、周、月、季度计算各列数据百分比变化率的协方差以及期望值。</h5><h5 id="例如：按季度计算数据百分比变化率-x3D-（本季度最后一天－上一个季度的最后一天）-x2F-上一个季度的最后一天。"><a href="#例如：按季度计算数据百分比变化率-x3D-（本季度最后一天－上一个季度的最后一天）-x2F-上一个季度的最后一天。" class="headerlink" title="例如：按季度计算数据百分比变化率&#x3D;（本季度最后一天－上一个季度的最后一天）&#x2F;上一个季度的最后一天。"></a>例如：按季度计算数据百分比变化率&#x3D;（本季度最后一天－上一个季度的最后一天）&#x2F;上一个季度的最后一天。</h5><h5 id="核心问题就在于客户给的数据的时间列并不是完整的，需要先判断出哪一天是（周、月、季度）的最后一天，并且取出对应的数据。"><a href="#核心问题就在于客户给的数据的时间列并不是完整的，需要先判断出哪一天是（周、月、季度）的最后一天，并且取出对应的数据。" class="headerlink" title="核心问题就在于客户给的数据的时间列并不是完整的，需要先判断出哪一天是（周、月、季度）的最后一天，并且取出对应的数据。"></a>核心问题就在于客户给的数据的时间列并不是完整的，需要先判断出哪一天是（周、月、季度）的最后一天，并且取出对应的数据。</h5><h5 id="接下来就分享我的解决方案。"><a href="#接下来就分享我的解决方案。" class="headerlink" title="接下来就分享我的解决方案。"></a>接下来就分享我的解决方案。</h5><h5 id="一：通过Unnamed-0这一列计算出某一日期的年份以及当年是第几个季度、第几个月、第几个周。"><a href="#一：通过Unnamed-0这一列计算出某一日期的年份以及当年是第几个季度、第几个月、第几个周。" class="headerlink" title="一：通过Unnamed: 0这一列计算出某一日期的年份以及当年是第几个季度、第几个月、第几个周。"></a>一：通过Unnamed: 0这一列计算出某一日期的年份以及当年是第几个季度、第几个月、第几个周。</h5><pre><code class="python">data[&quot;year&quot;] = data[&#39;Unnamed: 0&#39;].apply(lambda x: x.year)data[&#39;month&#39;] = data[&#39;Unnamed: 0&#39;].apply(lambda x: x.month)data[&#39;week&#39;] = data[&#39;Unnamed: 0&#39;].apply(lambda x: x.week)data[&#39;quarter&#39;] = data[&#39;Unnamed: 0&#39;].apply(lambda x: x.quarter)</code></pre><h5 id="二：通过上面计算得到的数据，按year、month分组聚合，得到按每年每月分组聚合的数据。同理得到每年每季度、每年每周的数据。"><a href="#二：通过上面计算得到的数据，按year、month分组聚合，得到按每年每月分组聚合的数据。同理得到每年每季度、每年每周的数据。" class="headerlink" title="二：通过上面计算得到的数据，按year、month分组聚合，得到按每年每月分组聚合的数据。同理得到每年每季度、每年每周的数据。"></a>二：通过上面计算得到的数据，按year、month分组聚合，得到按每年每月分组聚合的数据。同理得到每年每季度、每年每周的数据。</h5><pre><code class="python">month_grouped = data[&#39;Unnamed: 0&#39;].groupby([data[&#39;year&#39;], data[&#39;month&#39;]])week_grouped = data[&#39;Unnamed: 0&#39;].groupby([data[&#39;year&#39;], data[&#39;week&#39;]])quarter_grouped = data[&#39;Unnamed: 0&#39;].groupby([data[&#39;year&#39;], data[&#39;quarter&#39;]])</code></pre><h5 id="三：通过上面的分组聚合数据，用max-函数取出最后一天数据"><a href="#三：通过上面的分组聚合数据，用max-函数取出最后一天数据" class="headerlink" title="三：通过上面的分组聚合数据，用max()函数取出最后一天数据"></a>三：通过上面的分组聚合数据，用max()函数取出最后一天数据</h5><pre><code class="python">month_last_day = pd.to_datetime(month_grouped.max().to_list())week_last_day = pd.to_datetime(week_grouped.max().to_list())quarter_last_day = pd.to_datetime(quarter_grouped.max().to_list())</code></pre><h5 id="把时间列设为index"><a href="#把时间列设为index" class="headerlink" title="把时间列设为index"></a>把时间列设为index</h5><pre><code class="python">data.index = pd.to_datetime(data[&quot;Unnamed: 0&quot;])data.head(3)</code></pre><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Unnamed: 0</th>      <th>000016.SH</th>      <th>CBA02501.CS</th>      <th>000832.CSI</th>      <th>year</th>      <th>month</th>      <th>week</th>      <th>quarter</th>    </tr>    <tr>      <th>Unnamed: 0</th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>2016-01-04</th>      <td>2016-01-04</td>      <td>2270.4609</td>      <td>171.8353</td>      <td>312.9428</td>      <td>2016</td>      <td>1</td>      <td>1</td>      <td>1</td>    </tr>    <tr>      <th>2016-01-05</th>      <td>2016-01-05</td>      <td>2288.1127</td>      <td>171.5916</td>      <td>309.5110</td>      <td>2016</td>      <td>1</td>      <td>1</td>      <td>1</td>    </tr>    <tr>      <th>2016-01-06</th>      <td>2016-01-06</td>      <td>2317.6465</td>      <td>171.6981</td>      <td>310.2719</td>      <td>2016</td>      <td>1</td>      <td>1</td>      <td>1</td>    </tr>  </tbody></table><h5 id="删掉因计算中间数据而增加的列"><a href="#删掉因计算中间数据而增加的列" class="headerlink" title="删掉因计算中间数据而增加的列"></a>删掉因计算中间数据而增加的列</h5><pre><code class="python">data.drop([&#39;year&#39;, &#39;month&#39;, &quot;quarter&quot;, &quot;week&quot;, &quot;Unnamed: 0&quot;], axis=1, inplace=True)</code></pre><h5 id="四：用计算得到的每季的最后一天数据为索引，生成一个全0的series"><a href="#四：用计算得到的每季的最后一天数据为索引，生成一个全0的series" class="headerlink" title="四：用计算得到的每季的最后一天数据为索引，生成一个全0的series"></a>四：用计算得到的每季的最后一天数据为索引，生成一个全0的series</h5><pre><code class="python">quarter_last_day_series = pd.Series(    [0 for _ in range(len(quarter_last_day))],     index=quarter_last_day)</code></pre><h5 id="这个series的作用就是跟data进行内连接，从而达到了取出相应日期数据的目的。"><a href="#这个series的作用就是跟data进行内连接，从而达到了取出相应日期数据的目的。" class="headerlink" title="这个series的作用就是跟data进行内连接，从而达到了取出相应日期数据的目的。"></a>这个series的作用就是跟data进行内连接，从而达到了取出相应日期数据的目的。</h5><pre><code class="python">selected_by_quarter = pd.concat(    [quarter_last_day_series, data],     axis=1, join=&#39;inner&#39;).drop(0, axis=1)selected_by_quarter.head(3)</code></pre><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>000016.SH</th>      <th>CBA02501.CS</th>      <th>000832.CSI</th>    </tr>  </thead>  <tbody>    <tr>      <th>2016-03-31</th>      <td>2156.5410</td>      <td>172.8271</td>      <td>295.1833</td>    </tr>    <tr>      <th>2016-06-30</th>      <td>2122.6330</td>      <td>174.1138</td>      <td>282.8972</td>    </tr>    <tr>      <th>2016-09-30</th>      <td>2177.3524</td>      <td>176.9656</td>      <td>294.4042</td>    </tr>  </tbody></table><h5 id="同理就可以取出其它日期的数据"><a href="#同理就可以取出其它日期的数据" class="headerlink" title="同理就可以取出其它日期的数据"></a>同理就可以取出其它日期的数据</h5><h5 id="计算平均值"><a href="#计算平均值" class="headerlink" title="计算平均值"></a>计算平均值</h5><pre><code class="python">np.array(    selected_by_quarter.mean()).reshape(3, -1)array([[2805.834556],       [ 190.929684],       [ 328.764932]])</code></pre><h5 id="五：用pct-change-计算出百分比变化率"><a href="#五：用pct-change-计算出百分比变化率" class="headerlink" title="五：用pct_change()计算出百分比变化率"></a>五：用pct_change()计算出百分比变化率</h5><pre><code class="python">selected_by_quarter_pct = selected_by_quarter.pct_change().dropna()selected_by_quarter_pct.head(3)</code></pre><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>000016.SH</th>      <th>CBA02501.CS</th>      <th>000832.CSI</th>    </tr>  </thead>  <tbody>    <tr>      <th>2016-06-30</th>      <td>-0.015723</td>      <td>0.007445</td>      <td>-0.041622</td>    </tr>    <tr>      <th>2016-09-30</th>      <td>0.025779</td>      <td>0.016379</td>      <td>0.040676</td>    </tr>    <tr>      <th>2016-12-30</th>      <td>0.050312</td>      <td>-0.018757</td>      <td>-0.037569</td>    </tr>  </tbody></table><pre><code class="python">selected_by_quarter_pct.cov()</code></pre><table><thead><tr><th align="left"></th><th align="left">000016.SH</th><th align="left">CBA02501.CS</th><th align="left">000832.CSI</th></tr></thead><tbody><tr><td align="left">000016.SH</td><td align="left">0.006911</td><td align="left">-0.000556</td><td align="left">0.001930</td></tr><tr><td align="left">CBA02501.CS</td><td align="left">-0.000556</td><td align="left">0.000156</td><td align="left">0.000132</td></tr><tr><td align="left">000832.CSI</td><td align="left">0.001930</td><td align="left">0.000132</td><td align="left">0.002710</td></tr></tbody></table><h5 id="用cov-计算协方差-并且用-values-转化为numpy的array格式"><a href="#用cov-计算协方差-并且用-values-转化为numpy的array格式" class="headerlink" title="用cov()计算协方差,并且用.values 转化为numpy的array格式"></a>用cov()计算协方差,并且用.values 转化为numpy的array格式</h5><pre><code class="python">selected_by_quarter_pct.cov().valuesarray([[ 0.00691068, -0.00055623,  0.00193013],       [-0.00055623,  0.00015612,  0.00013195],       [ 0.00193013,  0.00013195,  0.00271026]])selected_by_day_pct = data.pct_change().dropna()selected_by_day_pct.cov().valuesarray([[ 1.45767216e-04, -1.84259700e-06,  5.16153366e-05],       [-1.84259700e-06,  9.07220432e-07, -1.19196633e-07],       [ 5.16153366e-05, -1.19196633e-07,  4.17346061e-05]])</code></pre><h5 id="六：用面向对象的方式重构代码"><a href="#六：用面向对象的方式重构代码" class="headerlink" title="六：用面向对象的方式重构代码"></a>六：用面向对象的方式重构代码</h5><h5 id="了解了全流程之后就用类封装数据和计算逻辑"><a href="#了解了全流程之后就用类封装数据和计算逻辑" class="headerlink" title="了解了全流程之后就用类封装数据和计算逻辑"></a>了解了全流程之后就用类封装数据和计算逻辑</h5><pre><code class="python">import numpy as npimport pandas as pdclass RiskParity: def __init__(self):  self.data = pd.read_excel(&quot;a1.xlsx&quot;)  self.data[&quot;year&quot;] = self.data[&#39;Unnamed: 0&#39;].apply(lambda x: x.year)  self.data[&#39;month&#39;] = self.data[&#39;Unnamed: 0&#39;].apply(lambda x: x.month)  self.data[&#39;week&#39;] = self.data[&#39;Unnamed: 0&#39;].apply(lambda x: x.week)  self.data[&#39;quarter&#39;] = self.data[&#39;Unnamed: 0&#39;].apply(lambda x: x.quarter)  self.data.index = pd.to_datetime(self.data[&quot;Unnamed: 0&quot;]) def __grouped(self, freq):  return self.data[&#39;Unnamed: 0&#39;].groupby([self.data[&#39;year&#39;], self.data[f&quot;&#123;freq&#125;&quot;]]) def __last_day(self, freq):  return pd.to_datetime(self.__grouped(freq).max().to_list()) def __calculate(self, columns, freq):  # 按给定频率计算协方差和期望  if freq == &quot;day&quot;:   selected_data_pct = self.data[columns].pct_change().dropna()   cov = selected_data_pct.cov().values   mean = np.array(selected_data_pct.mean()).reshape(len(columns), -1)  else:   last_day_series = pd.Series([0 for _ in range(len(self.__last_day(freq)))], index=self.__last_day(freq))   selected_data = pd.concat([last_day_series, self.data[columns]], axis=1, join=&#39;inner&#39;).drop(0, axis=1)   selected_data_pct = selected_data.pct_change().dropna()   cov = selected_data_pct.cov().values   mean = np.array(selected_data.mean()).reshape(len(columns), -1)  return cov, mean def cov_and_mean(self, columns: list, freq=&quot;day&quot;) -&gt; np.array:  &quot;&quot;&quot;   columns : 你所需要计算显示的列名，例如 [&quot;000016.SH&quot;, &quot;CBA02501.CS&quot;, &quot;000832.CSI&quot;]   freq : 你所需要计算的频率，按天: day   按月: month   按周: week   按季: quarter  &quot;&quot;&quot;  freq_excepted = (&quot;day&quot;, &quot;month&quot;, &quot;week&quot;, &quot;quarter&quot;)  if freq not in freq_excepted:   print(&quot;请输入正确的freq&quot;)   return None  else:   return self.__calculate(columns, freq)if __name__ == &quot;__main__&quot;: rp = RiskParity() v, r = rp.cov_and_mean([&quot;000016.SH&quot;, &quot;CBA02501.CS&quot;, &quot;000832.CSI&quot;], freq=&quot;quarter&quot;) print(v) print(r)</code></pre><h5 id="七：总结"><a href="#七：总结" class="headerlink" title="七：总结"></a>七：总结</h5><h5 id="这个需求的难点主要有两个方面，其一是如何从不完整的日期中挑选出每周、每月、每季度的最后一天；其二则是在给定时间序列的情况下，在原数据中取出相应日期的数据。"><a href="#这个需求的难点主要有两个方面，其一是如何从不完整的日期中挑选出每周、每月、每季度的最后一天；其二则是在给定时间序列的情况下，在原数据中取出相应日期的数据。" class="headerlink" title="这个需求的难点主要有两个方面，其一是如何从不完整的日期中挑选出每周、每月、每季度的最后一天；其二则是在给定时间序列的情况下，在原数据中取出相应日期的数据。"></a>这个需求的难点主要有两个方面，其一是如何从不完整的日期中挑选出每周、每月、每季度的最后一天；其二则是在给定时间序列的情况下，在原数据中取出相应日期的数据。</h5><h5 id="第一个问题的解决思路是通过（年、月）、（年、季度）这种双重的分组聚合，将每月、每季度的数据进行分割，之后取出每个分组的最后一条数据。"><a href="#第一个问题的解决思路是通过（年、月）、（年、季度）这种双重的分组聚合，将每月、每季度的数据进行分割，之后取出每个分组的最后一条数据。" class="headerlink" title="第一个问题的解决思路是通过（年、月）、（年、季度）这种双重的分组聚合，将每月、每季度的数据进行分割，之后取出每个分组的最后一条数据。"></a>第一个问题的解决思路是通过（年、月）、（年、季度）这种双重的分组聚合，将每月、每季度的数据进行分割，之后取出每个分组的最后一条数据。</h5><h5 id="第二个问题的解决思路是通过先生成以给定时间序列为index的任意一个series，然后用这个series进行内连接，以此就起到了筛选数据的效果"><a href="#第二个问题的解决思路是通过先生成以给定时间序列为index的任意一个series，然后用这个series进行内连接，以此就起到了筛选数据的效果" class="headerlink" title="第二个问题的解决思路是通过先生成以给定时间序列为index的任意一个series，然后用这个series进行内连接，以此就起到了筛选数据的效果"></a>第二个问题的解决思路是通过先生成以给定时间序列为index的任意一个series，然后用这个series进行内连接，以此就起到了筛选数据的效果</h5><h5 id="最后一点就是用面向对象的方式重构代码时，要明确需要为客户暴露哪些方法，哪些数据，并且选取相同逻辑的部分进行方法封装。"><a href="#最后一点就是用面向对象的方式重构代码时，要明确需要为客户暴露哪些方法，哪些数据，并且选取相同逻辑的部分进行方法封装。" class="headerlink" title="最后一点就是用面向对象的方式重构代码时，要明确需要为客户暴露哪些方法，哪些数据，并且选取相同逻辑的部分进行方法封装。"></a>最后一点就是用面向对象的方式重构代码时，要明确需要为客户暴露哪些方法，哪些数据，并且选取相同逻辑的部分进行方法封装。</h5>]]></content>
      
      
      <categories>
          
          <category> 数据分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>破解ajax动态网页——爬取篮球比赛数据</title>
      <link href="/2021/10/10/%E7%A0%B4%E8%A7%A3ajax%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E2%80%94%E2%80%94%E7%88%AC%E5%8F%96%E7%AF%AE%E7%90%83%E6%AF%94%E8%B5%9B%E6%95%B0%E6%8D%AE/"/>
      <url>/2021/10/10/%E7%A0%B4%E8%A7%A3ajax%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E2%80%94%E2%80%94%E7%88%AC%E5%8F%96%E7%AF%AE%E7%90%83%E6%AF%94%E8%B5%9B%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>今天遇到了群友的一个需求：</p><p>爬取 <a href="https://www.sporttery.cn/jc/lqsgkj/">https://www.sporttery.cn/jc/lqsgkj/</a> 全年的篮球赛果开奖数据，如下所示 <img src="http://files.mdnice.com/user/21987/51f00a1a-7c94-44f0-be37-18a335b32b0d.jpg" alt="img"></p><h1 id="一、网站分析"><a href="#一、网站分析" class="headerlink" title="一、网站分析"></a>一、网站分析</h1><p>这个网站的数据不是静态的，是动态加载的（选取相应日期，点击”开始查询“后，数据从服务端加载到浏览器）</p><p><img src="http://files.mdnice.com/user/21987/069d660d-2950-4800-9a89-6758df9147c2.jpg" alt="img"></p><p>看一下网页的低端，发现数据并不是一次性加载的，而是通过分页的形式完成 <img src="http://files.mdnice.com/user/21987/5f1b09e0-50c8-4978-8ba9-f7d70d358c73.jpg" alt="img"> 进一步研究还发现，每个月份的分页数并不是一致的，最多有8页 <img src="http://files.mdnice.com/user/21987/4f02e5ae-6da7-442d-9a91-5d495f6987be.jpg" alt="img"></p><p>对日期的输入框进行分析，发现它是可以直接通过键盘输入日期的 <img src="http://files.mdnice.com/user/21987/9dbc9c3a-e9a5-4147-b221-f9a3c2178bd1.jpg" alt="img"></p><h1 id="二：确定爬取方案"><a href="#二：确定爬取方案" class="headerlink" title="二：确定爬取方案"></a>二：确定爬取方案</h1><p>对于这种动态的网页，直接通过request库不太好爬取，而模拟人类操作浏览器的selenium对这种场景具有天然优势，所以我采用后者。对网页元素的定位则使用xpath。再此推荐蚂蚁老师的爬虫课程，简洁清晰，新手入门必备。</p><p>大概的思路就是，先构建每月的开始月份、结束月份</p><pre><code class="python">date = [(&quot;2021-01-01&quot;, &quot;2021-01-31&quot;), (&quot;2021-02-01&quot;, &quot;2021-02-28&quot;), (&quot;2021-03-01&quot;, &quot;2021-03-31&quot;),  (&quot;2021-04-01&quot;, &quot;2021-04-30&quot;), (&quot;2021-05-01&quot;, &quot;2021-05-31&quot;), (&quot;2021-06-01&quot;, &quot;2021-06-30&quot;),  (&quot;2021-07-01&quot;, &quot;2021-07-31&quot;),  (&quot;2021-08-01&quot;, &quot;2021-08-30&quot;), (&quot;2021-09-01&quot;, &quot;2021-09-31&quot;), (&quot;2021-10-01&quot;, &quot;2021-10-30&quot;),  (&quot;2021-11-01&quot;, &quot;2021-11-31&quot;),  (&quot;2021-12-01&quot;, &quot;2021-12-24&quot;)]</code></pre><p>再用这些数据分别输入到开始框和结束框内 <img src="http://files.mdnice.com/user/21987/dc0a7699-4476-45fa-8da1-75eb6cedaf3a.jpg" alt="img"> <img src="http://files.mdnice.com/user/21987/21d53b8e-bfca-4832-b684-e2235c740e96.jpg" alt="img"></p><p>点击查询 <img src="http://files.mdnice.com/user/21987/ddfb4718-c27c-48e6-8b98-7c4064933fe6.jpg" alt="img"></p><p>点击分页 <img src="http://files.mdnice.com/user/21987/f3fb0ecd-b870-4882-b15a-e21b56c8ef61.jpg" alt="img"> 特别注意到分页的xpath路径是有规律的，因而我们可以构造xpath路径，方便接下来的爬取</p><pre><code class="python">Xpath = [f&#39;//*[@id=&quot;matchList&quot;]/div/div/ul/li[&#123;i&#125;]&#39; for i in range(2, 10)]</code></pre><p>爬取内容 <img src="http://files.mdnice.com/user/21987/61104325-5c2c-4c72-aafe-9d5e84294634.jpg" alt="img"></p><h1 id="三：编码实现"><a href="#三：编码实现" class="headerlink" title="三：编码实现"></a>三：编码实现</h1><pre><code class="python">from selenium.webdriver import Edgefrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitimport timedriver = Edge(executable_path=&quot;C:/WebDriver/bin/msedgedriver.exe&quot;)date = [(&quot;2021-01-01&quot;, &quot;2021-01-31&quot;), (&quot;2021-02-01&quot;, &quot;2021-02-28&quot;), (&quot;2021-03-01&quot;, &quot;2021-03-31&quot;),  (&quot;2021-04-01&quot;, &quot;2021-04-30&quot;), (&quot;2021-05-01&quot;, &quot;2021-05-31&quot;), (&quot;2021-06-01&quot;, &quot;2021-06-30&quot;),  (&quot;2021-07-01&quot;, &quot;2021-07-31&quot;),  (&quot;2021-08-01&quot;, &quot;2021-08-30&quot;), (&quot;2021-09-01&quot;, &quot;2021-09-31&quot;), (&quot;2021-10-01&quot;, &quot;2021-10-30&quot;),  (&quot;2021-11-01&quot;, &quot;2021-11-31&quot;),  (&quot;2021-12-01&quot;, &quot;2021-12-24&quot;)]Xpath = [f&#39;//*[@id=&quot;matchList&quot;]/div/div/ul/li[&#123;i&#125;]&#39; for i in range(2, 10)]# 判断xpath存不存在的函数def NodeExists(xpath): try:  driver.find_element_by_xpath(xpath)  return True except:  return Falsedef crawler(): for item in date:  # 打开网页  driver.get(&quot;https://www.sporttery.cn/jc/lqsgkj/&quot;)  # 等待加载  WebDriverWait(driver, 10).until(lambda d: &quot;篮球赛果开奖&quot; in d.title)  # 等待一秒后 先清空再输入开始日期  time.sleep(1)  driver.find_element(By.XPATH, &#39;//*[@id=&quot;start_date&quot;]&#39;).clear()  driver.find_element(By.XPATH, &#39;//*[@id=&quot;start_date&quot;]&#39;).send_keys(item[0])  # 再等一秒 先清空再输入结束日期  time.sleep(1)  driver.find_element(By.XPATH, &#39;//*[@id=&quot;end_date&quot;]&#39;).clear()  driver.find_element(By.XPATH, &#39;//*[@id=&quot;end_date&quot;]&#39;).send_keys(item[1])  # 点击查询  driver.find_element(By.XPATH, &#39;//*[@id=&quot;headerTr&quot;]/div[1]/div[1]/div/a&#39;).click()  print(f&quot;开始爬取&#123;item[0]&#125;&quot;)  with open(f&quot;&#123;item[0]&#125;.csv&quot;, &quot;w&quot;, encoding=&#39;ANSI&#39;) as fin:   # 点击1-8页内容   page = 1   for this in Xpath:    time.sleep(1)    if NodeExists(this):     driver.find_element(By.XPATH, this).click()    else:     print(f&quot;没有&#123;this&#125;这个xpath&quot;)     break    time.sleep(1)    content = driver.find_element(By.XPATH, &#39;//*[@id=&quot;matchList&quot;]/table&#39;).text    fin.write(&quot;\n&quot; + content)    print(f&quot;完成第&#123;page&#125;页的爬取,此时的xpath是:&#123;this&#125;&quot;)    page += 1  print(f&quot;结束爬取&#123;item[0]&#125;&quot;)if __name__ == &quot;__main__&quot;: crawler()</code></pre>]]></content>
      
      
      <categories>
          
          <category> 网络爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> selenium </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬取博客园文章信息</title>
      <link href="/2021/09/10/%E7%88%AC%E5%8F%96%E5%8D%9A%E5%AE%A2%E5%9B%AD%E6%96%87%E7%AB%A0%E4%BF%A1%E6%81%AF/"/>
      <url>/2021/09/10/%E7%88%AC%E5%8F%96%E5%8D%9A%E5%AE%A2%E5%9B%AD%E6%96%87%E7%AB%A0%E4%BF%A1%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本次的案例源自于蚂蚁老师的爬虫课程。但由于博客园网站作了升级，所以蚂蚁老师的示例代码失效了，我根据蚂蚁老师教授的知识结合网上搜索来的资料，重新编写了一份代码，复现了蚂蚁老师在课程中的演示效果。</p><h1 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h1><p>下面我们来明确一下需求 爬取博客园前20页的文章标题、对应的文章链接以及对应的点赞数 如图所示 <img src="http://files.mdnice.com/user/21987/f140402f-a4fb-4e45-80f6-9d6465fc089b.jpg" alt="img"> 网站分页设置 <img src="http://files.mdnice.com/user/21987/ac0b815f-2d09-4090-a7c1-4af24cd5fa36.jpg" alt="img"></p><h1 id="查看网站的html代码"><a href="#查看网站的html代码" class="headerlink" title="查看网站的html代码"></a>查看网站的html代码</h1><h3 id="整体分析"><a href="#整体分析" class="headerlink" title="整体分析"></a>整体分析</h3><p>通过浏览器内置的查看网络部分，我们发现了点击分页时，实际请求的url并不是浏览器输入框中的信息，而是：<a href="https://www.cnblogs.com/AggSite/AggSitePostList">https://www.cnblogs.com/AggSite/AggSitePostList</a> <img src="http://files.mdnice.com/user/21987/da4597ab-de1a-48b0-a4c8-af67b6a67052.jpg" alt="img"></p><p>找到对应的请求部分 <img src="https://files.mdnice.com/user/21987/9f8d58bd-2388-4b0a-a7d7-8dd05faa160a.jpg" alt="img"> 可以发现，对应的content-type信息，以及user-agent信息 我们将这俩信息对应放到代码的header部分，作为一个请求头附入request里面，模拟浏览器，防止网站的反爬虫措施干扰。 另外，为了将爬虫伪装得更加像是一个浏览器，可以将请求负载也加到request中 <img src="http://files.mdnice.com/user/21987/f8e04c20-e4fe-4836-9ac6-8b916fad1a06.jpg" alt="img"></p><h3 id="局部分析"><a href="#局部分析" class="headerlink" title="局部分析"></a>局部分析</h3><p>下面我们来找具体的信息藏在哪里 鼠标右键对应的内容，查看html代码，就可以发现如下内容</p><p>文章标题和url链接 <img src="http://files.mdnice.com/user/21987/91bb6fff-0057-4659-8c77-d2f09028c8ae.jpg" alt="img"> 点赞数 <img src="http://files.mdnice.com/user/21987/f2098cf5-25fd-49e2-9da5-4a20347feebe.jpg" alt="img"></p><p>现在我们就可以根据前面的分析写代码了</p><h1 id="代码编写"><a href="#代码编写" class="headerlink" title="代码编写"></a>代码编写</h1><p>为了演示，我的代码只爬取了前3页的内容，如果你想要爬200页，可自行修改代码</p><pre><code>&quot;&quot;&quot;爬取前20页的文章标题、链接、点赞数&quot;&quot;&quot;import requestsfrom bs4 import BeautifulSoupimport reimport jsonfor idx in range(1, 3): print(&quot;#&quot; * 30, idx) url = &quot;https://www.cnblogs.com/AggSite/AggSitePostList&quot; headers = &#123;&quot;content-type&quot;: &quot;application/json&quot;, &quot;user-agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36 Edg/95.0.1020.30&quot;&#125; request_payload = &#123;&quot;CategoryType&quot;: &quot;SiteHome&quot;, &quot;ParentCategoryId&quot;: 0, &quot;CategoryId&quot;: 808, &quot;PageIndex&quot;: idx, &quot;TotalPostCount&quot;: 4000, &quot;ItemListActionName&quot;: &quot;AggSitePostList&quot;&#125; r = requests.post(url, data=json.dumps(request_payload), headers=headers) if r.status_code != 200:  print(r.status_code)  raise Exception() soup = BeautifulSoup(r.text, &quot;html.parser&quot;) section_list = soup.find_all(&quot;section&quot;, class_=&quot;post-item-body&quot;) for section in section_list:  a_tag = section.find(&quot;a&quot;, class_=&quot;post-item-title&quot;)  passage_link = a_tag.get(&quot;href&quot;)  passage_name = a_tag.get_text()  span_tag = section.find(&quot;span&quot;, id=re.compile(r&quot;^digg&quot;))  numbers = span_tag.get_text()  print(passage_link, passage_name, numbers)</code></pre><h1 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h1><p><img src="http://files.mdnice.com/user/21987/fb04eb0d-b847-4a19-a2be-9b1ddc4b6e3f.jpg" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 网络爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BeautifulSoup </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
